[
  {
    "type": "commit",
    "url": "https://github.com/tkhoa2711/terraform-digitalocean/commit/a86d89369aaf5a20c1e4d8415a8a771aa7de7d10",
    "content": {
      "message": "provision a droplet with cheapest price"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/blinkist/terraform-aws-airship-ecs-cluster/commit/d7aa659971bee1be873d3dda92e30443556f52df",
    "content": {
      "message": "Removed the default use of detailed monitoring. (#17)\n\n* Reduces CloudWatch costs for metrics by 80%"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/stealthHat/k8s-terraform/commit/681a3f8b4942be495b3f2528fb9ee40d7a4eb08a",
    "content": {
      "message": "nat gateway is verry expensive"
    },
    "codes": [
      "networking",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/stealthHat/k8s-terraform/commit/4193db798227c6538c61d55a906ed9ac997563f7",
    "content": {
      "message": "move to us for better prices -  I manually added this one while looking through the repo. It looks promissing as they basically change the servers to US because the prices are better"
    },
    "codes": [
      "saving",
      "area"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/JamesWoolfenden/terraform-aws-codebuild-container/commit/4a00ffcbf9576d7e5febdbdf94a31d4735fc8035",
    "content": {
      "message": "costs"
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/dannysievers/gcp-project/commit/88303c62ab59b1e7a538289112cf19354a8ed05f",
    "content": {
      "message": "adjusting billing account to reflect the id that needs to be passed"
    },
    "codes": [
      "saving",
      "billing_mode"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/thomastodon/jabujabu/commit/02210a3d3ba4a770c29623825b7f54f3ff33f3c7",
    "content": {
      "message": "Make the concourse cluster cheaper\n\n- no longer uses a load balancer\n- no longer uses more expensive VMs"
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/tooxie/terraform-workshop/commit/002bcce28e46728714fa1e0d20bec6f2559caba2",
    "content": {
      "message": "Add prod var\n\nWhat if we want to use less (or cheaper) infrastructure for non-prod systems?"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/deptno/terraform-aws-modules/commit/49f447bdbb3cf23499e8194e78f852ea1e256d3a",
    "content": {
      "message": "fix: dynamodb billing mode"
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/beaulabs/terraform_aws_ec2_instance/commit/d6df68da5ae58fb5c650c6be15d9d8e676a129db",
    "content": {
      "message": "a more cheaper instance"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/aws-observability/aws-otel-test-framework/commit/c928fe0a05d1e9b8f1ecb1a7dacffecc2800e038",
    "content": {
      "message": "Fix dynamo DB options and change go X-ray sample app to public (#525)\n\n* fix billing mode write capacity and change xray receiver to public\n\n* fix billing mode write capacity and change xray receiver to public\n\n* fix format for dynamo db"
    },
    "codes": [
      "saving",
      "billing_mode"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ken-matsui/poac-infrastructure/commit/02c710b8259f493c475021fc9eac23b871305ae6",
    "content": {
      "message": "cost reduction\nhttps://aws.amazon.com/about-aws/whats-new/2017/06/amazon-rds-enables-encryption-at-rest-for-additional-t2-instance-types"
    },
    "codes": [
      "saving",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/chad-russell-git/terraform-oci-cis-landing-zone/commit/6a696dfa2dd5716a65b10fc3277cd9e994b8b30a",
    "content": {
      "message": "cost tracking tag set to true"
    },
    "codes": [
      "awareness",
      "policy"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/chad-russell-git/terraform-oci-cis-landing-zone/commit/7247909ecd98c2d511316392f22cb3877f05250b",
    "content": {
      "message": "added policies for cost management"
    },
    "codes": [
      "awareness",
      "policy"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ministryofjustice/modernisation-platform/commit/bc346a5604045d29b4b427c926f3e70d69440698",
    "content": {
      "message": "enable PITR to squash CKV_AWS_28\n\nEnables continuous backups. In this case the cost is negligeable due to low volumes of data being stored in this data base. https://aws.amazon.com/dynamodb/pricing/on-demand/"
    },
    "codes": [
      "awareness",
      "increase",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/guilhermerenew/infra-cost/commit/ba858d94e29d03e3e81533df8cd8bc85b9f176f1",
    "content": {
      "message": "mega-fix for adjustments in costing"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/UriKatsirPrivate/gcp-landing-zone/commit/0160ab1f89489abec4120ef4102bf5964b0af2fe",
    "content": {
      "message": "Use billing account with variable"
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/AJarombek/global-aws-infrastructure/commit/4a89f4b8235961275fa0e6aaf20848f2b8b7e733",
    "content": {
      "message": "budget alarms for high costs, budget & cloudtrail tests"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/AJarombek/global-aws-infrastructure/commit/61e07012be3f140daf18d33b3be19c80147f12f2",
    "content": {
      "message": "daily cost lambda infrastructure"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/gudlyf/TerraformOpenVPN/commit/be1245d8634025277ba79a4155ee88d7eaffcdfb",
    "content": {
      "message": "- Updated Ubuntu to 16.04\n- Instance type is not t3.nano (cheaper)\n- terraform fmt\n- Modifications to userdata.sh to adhere to new Ubuntu version, DNS\nchanges, and handling of apt upgrade without CLI."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/gudlyf/TerraformOpenVPN/commit/4bc861c153b65a2d7c0d5f3fac30ab72b0fc6942",
    "content": {
      "message": "Updated VM size to B2s to reduce cost\n\nUpdated from A0 to B2s to reduce cost"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cloudchacho/terraform-google-hedwig-subscription/commit/ab8808de2e078c572e9d88fdc57ff91b8e4acc36",
    "content": {
      "message": "Remove dataflow since that pattern is prohibitively expensive (#3)"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ohoareau/terraform-modules/commit/d066b8dfa3b03df200afa9a521126a83884cdfb3",
    "content": {
      "message": "dynamodb: read/write capacities to 0 if pay per request billing mode"
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/structurefall/jamulus-builder/commit/7190744187e0aed2df8ce84f2a944294d6d4fc5b",
    "content": {
      "message": "Redid networking ton one subnet to eliminate costly NAT, added SSH in"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-aws/commit/6488091456429ed61264b9cad841eeb6bf871e32",
    "content": {
      "message": "Use all of available glacier time for backups\n\nGlacier charges an [early delete\nfee](https://aws.amazon.com/premiumsupport/knowledge-center/glacier-early-delete-fees/)\nfor deleting objects which are stored for less than 90 days. This\nappears to be calculated by charging you for 90 days of storage\nregardless of the length of time things are stored in glacier, and\ncalling this an early deletion fee. This is a footnote on the [pricing\npage](https://aws.amazon.com/glacier/pricing/) which reads\n\n> Glacier archives have a minimum 90 days of storage, and archives\n> deleted before 90 days incur a pro-rated charge equal to the storage\n> charge for the remaining days\n\nWe may as well make use of this\nadditional time as we are already paying for it through the early\ndeletion fee."
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-aws/commit/6cfda6ada5137b232ff442ae9f2aedc8520ee1b4",
    "content": {
      "message": "Move from m4.large to m5.large\n\nThe new gen have more CPU and are cheaper.\n\n           ECPU\nm4.large   6.5  8  EBS Only  $0.111 per Hour\nm5.large   10   8  EBS Only  $0.107 per Hour"
    },
    "codes": [
      "saving",
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-aws/commit/aeb3bfbe393cdfc02e62b812843ed75cf5f245e4",
    "content": {
      "message": "Move from m4.xlarge to m5.xlarge\n\nThe new gen have more CPU and are cheaper."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-aws/commit/5fa5da9756f12559b490217dd5b173db48e7f2a9",
    "content": {
      "message": "Resize graphite machine type\n\nUpdate machine type to m5.xlarge. It should be cheaper, we tried to\nresize it before but it didn't work because of disk labels. Trying again\nafter the 'Device' tag was added to the EBS volume."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-aws/commit/2ee1ff690416016dfed50dab6f516559f2270bf9",
    "content": {
      "message": "Make our monitoring instances m5.large instead of t2\n\nThe monitoring instance that runs Icinga and Smokey in Integration has\nbegun running out of CPU credits. This is making it difficult to\ndiagnose problems elsewhere in the stack. Making it an m5.large costs\n~$77pcm instead of ~$36pcm, and doubles the RAM."
    },
    "codes": [
      "saving",
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-aws/commit/19d187e4a29147cbcf1cfae456cfcbfa8ad52b45",
    "content": {
      "message": "Increase PostgreSQL primary DB to next instance class\n\nThis commit increases the PostgreSQL primary DB instances to the next instance class to provide more CPUs and RAM. This is due to CKAN now using this database, resuting in increased load. The instances are changed from M4 to M5 since they have improved performance and are cheaper for the equivalent instances than M4.\n\nThe changes will be made during the next maintenance window, which is Monday 4-6am, since `apply_immediately` is not set and defaults to `false`."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-aws/commit/5d30d7d7ca658bed21677b1aa56c72a0e0cd737b",
    "content": {
      "message": "Upgrade data-science-2 instance type tp p3.8xlarge\n\nThis commit upgrades the `data-science-2` EC2 instance type from `p3.2xlarge` to `p3.8xlarge`.\n\nThis instance is being used to evaulate the implementation of a GraphSAGE algorithm which will be used to generate related links for content pages across GOV.UK. To perform this evaluation, it is necessary to run the algorithm against a set of sample data.\n\nCurrently on the `p3.2xlarge` instance, it takes approximately 20 minutes to evaluate one page of the sample data (one content item) using the algorithm. By switching the instance type to `p3.8xlarge`, we roughly estimate that this time can be brought down to 5 minutes per page of the sample data.\n\nBy speeding up the evaluation process, we will be able to determine the suitability of the algorithm for the purpose of generating related links and will then productionise the process to be able to evaluate ~200K content items for the actual A/B test.\n\nIn terms of cost (at the time of writing), the `p3.8xlarge` is priced at $12.24/hr; we anticipate that we will need up to 8 hours of compute time, which would result in a total cost of $97.92. In comparison, the `p3.2xlarge` currently costs $3.06/hr - this would result in the same total cost of $97.92 as we anticipate it would take up to four times as long to completely evaluate our sample data."
    },
    "codes": [
      "saving",
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-aws/commit/fbd513a3c74bf20ecdb67a4243d30356e8b86a01",
    "content": {
      "message": "Bump the search instance size from t2 to c5\n\n- We have been seeing search-api instances run out of CPU credits. At\nthe moment they are manually set to be \"unlimited\".\n- Using c5 removes CPU credits. As we seem to be using burst mode 30-40\npercent of the time, this change should not increase cost.\n\nsolo: @schmie"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-aws/commit/74fe1ac392549eb3aad67d239310b92cc9f0dd10",
    "content": {
      "message": "Update app-elasticsearch5 for Training environment\n\nAdd backend to build app-elasticsearch5 in the Training environment.\n\nAdd parameters to select which domain to use with the DNS records (Training\ndoes not use the stack domain).\n\nTraining elasticsearch runs with with fewer instances and t2.smalls to save\non costs."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-aws/commit/e91c7912b25beb51199ccfff2414668bcc69186e",
    "content": {
      "message": "Change instance type to r4.2xlarge\n\nThe Knowledge Graph is running out of memory on start-up when it generates the data it needs. Changing the instance type from `m5.2xlarge` to `r4.2xlarge` doubles the amount of RAM available (from 32GB to 64GB), whilst minimally increasing cost."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-aws/commit/ef79a5aec29133d4c479652863c61f943fbdb42b",
    "content": {
      "message": "postgresql: Bump production database instance size to db.m5.8xlarge\n\n- We're seeing spiky CPU usage, going up to 99%, when scheduled\n  publishing occurs at 0930, and emails are sent out at various points\n  during the day.\n- Tuning the apps would take longer and maybe not even be feasible as\n  scheduled publishing has to happen, and so does sending urgent emails.\n- We could investigate splitting the databases out into\n  `publishing-api-postgresql` and `email-alert-api-postgresql`, but we\n  attempted that before and [resolved to not do\n  it](https://github.com/alphagov/govuk-aws/commit/c51e8bbf879dd867b73bec4f71ba7271703e1909)\n  (also for\n  [publishing-api](https://github.com/alphagov/govuk-aws/pull/1086/commits/3f7cc071b4361e5df15ac1dfb2a1e3c565bb8678)).\n- According to\n  https://www.ec2instances.info/rds/?region=eu-west-1&cost_duration=monthly,\n  base cost for the old instance type is ~$1150/month. The bigger\n  instance type specified here costs ~$2300/month. That's an increase of\n  $1150 / month, or  $13,800 / year."
    },
    "codes": [
      "saving",
      "instance",
      "awareness",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-aws/commit/6283dd7b80fd26ff09d242555a6e56c2eb75471b",
    "content": {
      "message": "postgres: Increase instance size to db.m5.12xlarge\n\n- We're using too old a Postgres version (9.6.old-patch) for Amazon to\n  let us use `db.m5.8xlarge`:\n\n```\n* module.postgresql-primary_rds_instance.aws_db_instance.db_instance: 1 error occurred:\n* aws_db_instance.db_instance: Error modifying DB Instance blue-postgresql-primary: InvalidParameterCombination: RDS does not support creating a DB instance with the following combination: DBInstanceClass=db.m5.8xlarge, Engine=postgres, EngineVersion=9.6.11, LicenseModel=postgresql-license. For supported combinations of instance class and database engine version, see the documentation.\n```\n\n- Instead, we're upgrading to the next unrestricted database instance.\n  This costs an extra $2254/month compared to what we have _now_\n  (`db.m5.4xlarge`)"
    },
    "codes": [
      "saving",
      "instance",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-aws/commit/b4094f63c5d4a216ed7ade1a51ea3653222cf222",
    "content": {
      "message": "Configure memcached_instance_type\n\nWe probably want to use a smaller memcached instance in integration (and\nlower environments), as the default costs about $2k / year.\n\nWe'll raise a govuk-aws-data PR to set this to cache.t3.small in\nintegration."
    },
    "codes": [
      "saving",
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-aws/commit/f844cd8e254b161bebef04101f8ce177bcd0840c",
    "content": {
      "message": "Add lifecycle rules within govuk-data-infrastructure-integration bucket\n\nThis commit adds lifecycle rules within the `govuk-data-infrastructure-integration` bucket to keep only the last 30 days of data. This is necessary as we currently hold more data which is never used, so by only keeping the most recent month's worth of data we can reduce costs and remove noise."
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-aws/commit/806b1a2a47f2f4e580e524b2cf8cc5928749d972",
    "content": {
      "message": "Make Prometheus storage volume configurable and upgrade to gp3.\n\nThis enables us to add more timeseries storage space in prod, which was\nrunning out.\n\ngp3 is cheaper and faster, so no reason not to use it."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-aws/commit/8d7d2ebe0dbe9ebf8009572d1d710c4700cf245e",
    "content": {
      "message": "Use cheaper and faster gp3 storage for CI agent EBS volumes.\n\ngp3 is 20% cheaper than gp2 and includes 3000 IOPS at any size instead\nof 3 IOPS/GB. This should speed up builds at least a bit.\n\nWhile we're there, define the values once instead of copy-pasting."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/joshuaspence/infrastructure/commit/d8e1979ea7954076f64ab4d3337b95f14a06fc31",
    "content": {
      "message": "Use a single NAT gateway\n\nNAT gateways are expensive ($0.059/hour) so let's use less of them."
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/joshuaspence/infrastructure/commit/b9b9465314e3b5ada78340d06b90703136cdf3dc",
    "content": {
      "message": "Remove VPC NAT gateway\n\nThis is a temporary measure to cut down on costs."
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/joshuaspence/infrastructure/commit/0cea1cf84859055ff5107fc8d8514475e7036bee",
    "content": {
      "message": "Remove EKS cluster\n\nUse Minikube instead of EKS because EKS is expensive."
    },
    "codes": [
      "saving",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/openinfrastructure/terraform-google-gitlab-runner/commit/8429375df72b04cc6fedc1ebb5f2c2e4ba18b9f2",
    "content": {
      "message": "Add preemptible feature flag, default to true\n\nCreate preemptible instances by default to reduce cost.  The managed\ninstance group will recreate preempted instances.\n\nSee: https://cloud.google.com/compute/docs/instances/preemptible"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/dexterchan/Terraform_Webserver/commit/af5af0b8e6a59a9c5879fde7eaaa86d694c2bfa2",
    "content": {
      "message": "reduce cost by reducing vpc endpoint deployment to one subnet only"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ONSdigital/eq-terraform-dynamodb/commit/40eb651a50d0dfd5cf047ef62c8a6259c1c66e02",
    "content": {
      "message": "Set billing_mode to PAY_PER_REQUEST"
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/DoSomething/infrastructure/commit/a2dbfe25699f574b5ad4540063e4208ea8b04b4e",
    "content": {
      "message": "Removing provisioned IOPS from testing to get back to cheaper storage options."
    },
    "codes": [
      "saving",
      "feature",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/hashicorp/terraform-azurerm-terraform-enterprise/commit/b2fe861f1949f5f9f5bad0b4ca4ee6b6cb65485f",
    "content": {
      "message": "Scale down default Redis size\n\nThe P3 size has 26GB of cache and costs $1485.17 per month. The P1 has 6GB of cache and costs $370.96 per month. The 6GB size is more than enough for intense workloads."
    },
    "codes": [
      "saving",
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/MartinFeineis/terraform/commit/359ba426393c78b78695797f9bdd6a08c0455720",
    "content": {
      "message": "added billing"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/stuartellis/stuartellis-org-tf-modules/commit/39a9cabac6765c75591ba258fef0d10ba7ae0f9e",
    "content": {
      "message": "Add module for billing alerts"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/greenbrian/musical-spork/commit/24c07bfd5c31438fff6374e9ba3d577e6402d777",
    "content": {
      "message": "f-centos-7 - Support for cheaper CentOS 7/t3 instances, visibility fixes (#23)\n\n* Adding support for CentOS 7(cheaper), moved to t3 instances(cheaper) for non t2.micro(free-tier), hostnaming/display name visibility improvements\n\n* Fixing ami name change typo, fixing docker consul dns configuration"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/fahdr/asz-dbz/commit/f5b50d8dfbec87dba0b824369021dc6b22058840",
    "content": {
      "message": "Choosing only availabilty zone for cost optimaization"
    },
    "codes": [
      "saving",
      "area"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/kmalkin/tf-aws-pi-hole/commit/066049972b995fb019bf29be7f0f307064a2f00e",
    "content": {
      "message": "placeholder, too long..."
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/poseidon/terraform-google-kubernetes/commit/189d96ce0f96c107ebf62dbaf73734e6fd20883a",
    "content": {
      "message": "Use global HTTP/TCP proxy load balancing for Ingress on GCP\n\n* Switch Ingress from regional network load balancers to global\nHTTP/TCP Proxy load balancing\n* Reduce cost by ~$19/month per cluster. Google bills the first 5\nglobal and regional forwarding rules separately. Typhoon clusters now\nuse 3 global and 0 regional forwarding rules.\n* Worker pools no longer include an extraneous load balancer. Remove\nworker module's `ingress_static_ip` output.\n* Add `ingress_static_ipv4` output variable\n* Add `worker_instance_group` output to allow custom global load\nbalancing\n* Deprecate `controllers_ipv4_public` module output\n* Deprecate `ingress_static_ip` module output. Use `ingress_static_ipv4`"
    },
    "codes": [
      "saving",
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/austin1237/clip-stitcher/commit/4eed76f9bfd4f93181660178ea689d98cd6d66d5",
    "content": {
      "message": "removed private subnet/nat to reduce cost"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/giantswarm/giantnetes-terraform/commit/53ed24b573947c73ea9f0f4f8b477c44b7de2d54",
    "content": {
      "message": "Use cheaper resources by default (#97)\n\n* Use cheaper resources by defalt\n\n* Fix worker vm size"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/kathputli/terraform-aws/commit/321b1aee88f7d15dafe46aede2b86ced70061025",
    "content": {
      "message": "Add support for spot-fleet bootstrap\n\nThis allows using a spot fleet to create the bootstrap instance.  When instance\nprices are low, this can result in savings over the on-demand cost of a\nt2.small instance (amazing, to be honest), and the bootstrap server is probably\nOK to handle the possible termination and re-creation that can occur with spot\ninstances.\n\nA new set of variables are added to control if we should use this, and what\nprice to set.  We also need a variable to control the expiration time, since we\ncan't compute it ourselves (at least, not easily).\n\nBUG: Setting \"associate_public_ip_address\", while not having any real effect in\nour config (because we set that previously at the subnet level) causes\nTerraform to generate a config that fails to run.  This needs to be reported.\n\nTODO: Make multiple launch configurations for different subnet/instance-type\ncombinations."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/public-transport/infrastructure/commit/64bfc57cef180a90c4dfe91801e7e1a4c66c3151",
    "content": {
      "message": "add second node, switch to monthly billing"
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/IncredibleHolg/infra-aws-code/commit/70904707a36ff8e5167e695de3529d8318911ba4",
    "content": {
      "message": "cost update"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/IoT-Data-Marketplace/mp-infrastructure/commit/5afcf39a85fc972eb9bb3486e5dc8aeeba77d3ee",
    "content": {
      "message": "removing the vpn since it is too expensive and enabling cluster api public access"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/mdcurran/terraform-infrastructure/commit/d7f24bafaa9f80e9d3e834dc34b8fbe950a6c437",
    "content": {
      "message": "Stop hosting in GCP as it's very expensive"
    },
    "codes": [
      "saving",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ministryofjustice/cloud-platform-infrastructure/commit/e5dd13d33c1e927f932971d067d8f70e9041b5f3",
    "content": {
      "message": "Reduce live-1 worker node instance size\n\nThis changes the worker nodes from r5.2xlarge to\nr5.xlarge\n\nThis change should still leave plenty of extra\ncapacity to accomodate some growth, while saving\nthe MoJ almost $5,000/month in AWS hosting costs.\n\nNB: After merging this change, we need to do a\nkops update and rolling update of the cluster,\nbefore the change takes effect."
    },
    "codes": [
      "saving",
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ministryofjustice/cloud-platform-infrastructure/commit/fdd3a7dcb787a6a24e34b93a6aa124c94e93cb0e",
    "content": {
      "message": "Cheaper Prometheus storage for test clusters. (#1452)"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ministryofjustice/cloud-platform-terraform-elasticache-cluster/commit/ddcc1a8abc1d29e8e32ed3c2e92ed9755dd17014",
    "content": {
      "message": "Add \"namespace\" tag to created Elasticache cluster\n\nFor cost allocation."
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/dotancohen81/Rancher/commit/90944271b4e8bd46e3d42ac64bc4964a33a8fdc3",
    "content": {
      "message": "Updated server from t2 to t3\n\nDue to better performance and cost savings updating from t2 to t3."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cisagov/cyhy_amis/commit/4e67a501bb3f5187a3e9af523921ac62b8a88469",
    "content": {
      "message": "Change code to use the cheaper r4.xlarge instances type.\n\nI watched the memory consumption while running BOD scanning, and it\nappears to consume about 13GB.  I could probably get by with 16GB of\nRAM, but this is an economical option that has almost 32GB of RAM."
    },
    "codes": [
      "saving",
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cisagov/cyhy_amis/commit/7b8d9247a679295e0e1791b13d6c437c473e44b8",
    "content": {
      "message": "Upgrade volumes from gp2 to gp3\n\ngp3 is 20% cheaper, and the baseline configuration offers better\nperformance than gp2 for volumes smaller than 2TB.  It also allows the\nvolume size and IOPS to be configured separately, whereas the two are\nintertwined with gp2.\n\nAlso remove delete_on_termination lines from root volume configs.\ndelete_on_termination = true is the default, so there is no need to\nspecify it."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/olliefr/aws-terraform-cloud1/commit/bf753832a519b0649f8d58d93aa643afe3f94fc7",
    "content": {
      "message": "Change DynamoDB billing to \"on demand\""
    },
    "codes": [
      "saving",
      "billing_mode"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/InvictrixRom/website-infrastructure/commit/09e400452c1bde25fe393dd56c2fd608b84a18ac",
    "content": {
      "message": "Update infrastructure to be more cost-effective"
    },
    "codes": [
      "saving",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/InvictrixRom/website-infrastructure/commit/d51caa0c810f6d5ad6f0846a317ec2432835aad7",
    "content": {
      "message": "Revert \"Update infrastructure to be more cost-effective\"\n\nThis reverts commit 09e400452c1bde25fe393dd56c2fd608b84a18ac."
    },
    "codes": [
      "saving",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/InvictrixRom/website-infrastructure/commit/44d66328ea4467d05d7fb8092631aff5afbd8b26",
    "content": {
      "message": "Cost saving: Against my best practices\n\nSecurity groups are more than enough to secure the instance, but best practice says you should subnet it and keep the load balancer in the public subnet.\n\nSadly this requires a NAT gateway but it costs nearly 15 bucks a month on it's own.\n\nThis nearly halfs monthly charges"
    },
    "codes": [
      "networking",
      "saving",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/tjpotenza/kubernetes-modules/commit/d750e25615e7e37914119cb257fbd9c7fad3a52a",
    "content": {
      "message": "Begin to decouple the workers from requiring a static DNS address, so I can run the control plane in an ASG without paying for an NLB."
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Harmelodic/automation-infrastructure/commit/8b563a7d3c7d90ddd2359500d9781547347cc56a",
    "content": {
      "message": "Removing infra because I'm not using it and it costs"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/garylb2/terraform-example-patterns/commit/6de6d83d930bd9459e1cf8c311fa7b45c3f90987",
    "content": {
      "message": "Removed extraneous global secondary indices that just cost money"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/gstimac/terralab/commit/7696353632379532ab7006f9eda39fbf14e25532",
    "content": {
      "message": "fix: cheap AKS with basic sku lb"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/naciriii/terraform-ec2-gitlab-runner/commit/f8af6bc22bd3d827566e7e65deb63c13cdaf6031",
    "content": {
      "message": "Use aws spot instances\n\n- Reduce cost and use persistent spot instances requests to get cheaper instances/hour\n- Expose module output to root output"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/JamesWoolfenden/terraform-aws-codepipeline/commit/cf842d48df037476a754b221f7b4115bb4f5ddf1",
    "content": {
      "message": "costs"
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Arkoprabho/TerraformTutorial/commit/ba317d7e402f014589e230fad8c7384016211ba2",
    "content": {
      "message": "MODIFY: Change billing_mode to PAY_PER_REQUEST"
    },
    "codes": [
      "saving",
      "billing_mode"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Kalmalyzer/UE-Jenkins-BuildSystem/commit/636097557e403eb1d6b6211b09e30c47e7f39466",
    "content": {
      "message": "Reduce build nodes to 16-vCPU, delays during bootups are costly"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Kalmalyzer/UE-Jenkins-BuildSystem/commit/ee8942b2c5d59546dd3b3be5f2cb88500d0fe1be",
    "content": {
      "message": "kalms env uses standard hdds (minimize cost)"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/forgotpw/forgotpw-infrastructure/commit/f4363ad27d366385f2388d073ce8af796e035406",
    "content": {
      "message": "added billing cloudwatch alarms"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/dshmelev/aws_kube_tc/commit/853298ac74250964aa2d2ea921daa5905528b3a9",
    "content": {
      "message": "Cost saving"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/sworisbreathing/splunk-refactor-poc/commit/5be96c3735b14c16dbe87f4c544f5ab1c75ab93a",
    "content": {
      "message": "update auto stop to 1hr. we get billed a minimum of 1h anyway"
    },
    "codes": [
      "saving",
      "billing_mode"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/generation-org/tech-foundations-labs/commit/88f50c92b92f5c3ab8259902a56932f295fecce7",
    "content": {
      "message": "Moving to pay_per_request"
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/pvandervelde/infrastructure.azure.core.network.hub/commit/0ecf0a154918bd9bdc0f53557bc1f80920da6b14",
    "content": {
      "message": "Remove the firewall because it's expensive!! And we're not going to have a public IP at the moment that\nis not a VPN anyway."
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/yardbirdsax/elasticsearch-the-hard-way/commit/521bae59a4002a616eac44c1681ca5066bbd00c8",
    "content": {
      "message": "Updated instance size to reduce cost of lab."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/geraldwuhoo/homelab-iac/commit/3228ee1c5bfdf89e57e898659b086364122d58c1",
    "content": {
      "message": "Move to Rocky Linux instead of RHEL no-cost license"
    },
    "codes": [
      "saving",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/firehawkvfx/firehawk-main/commit/aef45215a4fe18d7fd03b7556fa3070815c57a22",
    "content": {
      "message": "init kms seperately to save cost"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/jkstenzel95/jks.gameservers/commit/411ab992ba07e698cb08b56eb4cfc9d6e001d43f",
    "content": {
      "message": "Change dynamodb kv store billing mode"
    },
    "codes": [
      "saving",
      "billing_mode",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Eximchain/terraform-aws-quorum-cluster/commit/6a56f400f7de3f4d5cef646d92e3f848608031c1",
    "content": {
      "message": "Make CloudWatch metrics optional\n\nThis adds a terraform variable to choose whether to enable cloudwatch\nmetrics. This is for cost-savings, since put-metric-data calls are\ncharged per call and dashboards (coming soon) are multiple USD per month."
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/bretmullinix/terraform-for-beginners/commit/3a3e9f6d19da0730f2b077b0c160a467102f5666",
    "content": {
      "message": "updated the image id to use, the other one was costing too much."
    },
    "codes": [
      "saving",
      "area"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Accurate0/infrastructure/commit/06889e08148d258f329118d43734f8c8dcff994e",
    "content": {
      "message": "I DONT WANT TO PAY FOR LOGS"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/JamesWoolfenden/terraform-azurerm-sql/commit/4fe0be131b0f6b5f073391227b9fe54372506b42",
    "content": {
      "message": "costs"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Midas-Protocol/webtwo-infra/commit/25ed0319bf8099f0cc79eceba9104c73d9507e0d",
    "content": {
      "message": "remove unneded configs and (expensive) resources"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Midas-Protocol/webtwo-infra/commit/2809f4e2d78ff33b48e9c1742b58a256ebce986d",
    "content": {
      "message": "remove unneded configs and (expensive) resources"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ImminentDomain/immdom-terraform/commit/a49aaeed1776f0991256221e61ebc868ae5b60cb",
    "content": {
      "message": "fix: update billing alarm source"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/GBergeret/tf-vpc-module/commit/34d80ece7d0ef598414baffceb074c6580dd819b",
    "content": {
      "message": "test: Update test instance to be smaller\n\nUpdate test instance from t2.micro to t3.nano (reduce cost)."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/GBergeret/tf-vpc-module/commit/5e63c8390cb1001daf4ad74bb2926cc060c0de08",
    "content": {
      "message": "feat: Create a private tier with private subnets\n\nCreate a private tier with private subnets and use an ssh bastion to run\nkitchen-terraform on private subnets.\n\n* Update README.md\n* Create private subnets, nat gateways and route tables.\n* Implement var.price_saving_enabled to have AWS NAT gateway or EC2 instance\n(to save on cost).\n* Auto determine subnets size (size(private) = 4 * size(public)).\n* Update kitchen-terraform tests to run from private subnets.\n* Update kitchen-terraform to test each case (with NAT GW or EC2\ninstances)"
    },
    "codes": [
      "networking",
      "saving",
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/chaspy/terraform-alibaba-isucon8/commit/53588dad5dd4c13903a6c582f74e1afe2671d33e",
    "content": {
      "message": "Minimamize spec to save the cost"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Cinegy/terraform-cinegy-modules/commit/09636724e63599b1f589065c92074575471ac234",
    "content": {
      "message": "-add 'secondary_az' var for cheaper VPCs\n-cleanup"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/SUSE/caps-automation/commit/5675965b57d05932ae8b806bad2cd219d15fe516",
    "content": {
      "message": "Use Aurora DB as external database\n\nThat is slightly cheaper, and does the job as well."
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/akshay3030/dockerized-jenkins-master/commit/4356e9ebf89921898e9459f85f1173d44f3a3a6f",
    "content": {
      "message": "cost optimized ebs first checking"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/00inboxtest/cloud-foundation-fabric/commit/3a8a040ff3efcec38c423d5249625ed2d87ab261",
    "content": {
      "message": "Billing budget module"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/laurijssen/tform_azure/commit/6f802267c2d43803eebbdb383da9e3815c1c68bd",
    "content": {
      "message": "northeurope center mo cheapo"
    },
    "codes": [
      "saving",
      "area"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/AwakeningSV/live-infrastructure/commit/43bba3db97631d8730160950395fde42793adbc2",
    "content": {
      "message": "Migrate infrastructure to Microsoft Azure.\n\nMicrosoft offers $5,000/year in credits to non-profit organizations,\nincluding churches. This allows for us to eliminate all costs with\nrunning our live video system a few hours a week.\n\nhttps://www.microsoft.com/en-us/philanthropies/product-donations/products/azure\n\nDeployments take about 2-3 minutes, which is considerably worse than\nDigitalOcean which only took about 20-30 seconds. Teardowns are also\nworse: 10+ minutes instead of under 1 minute. The verbose specification\ncompared to DigitalOcean is also a bit more complex to understand.\nHowever, the price is right. Thanks Microsoft!"
    },
    "codes": [
      "saving",
      "awareness",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/plxis/eng-ci/commit/b795561bc0909fc942aa847cc952dad2091a6ae2",
    "content": {
      "message": "Add toxic host into standard eng-ci; removed internal ssh load balance for proxy to reduce monthly expenses"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-terraform-provisioning/commit/ed67711a54ca65473c9c7b788161d680914ef1c7",
    "content": {
      "message": "sets up S3 bucket for transition logs\n\nWe want the lowest-cost storage class for these logs as they will\nseldomly be accessed, such as in a DR scenario. Unfortunately\nterraform does not currently support lifecycle configuration, so\nwe will need to manage this aspect manually."
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-terraform-provisioning/commit/ac105ab0a5ae38fbf69167e072f8970a4a61c3e8",
    "content": {
      "message": "Enable lifecycles on MongoDB backup buckets\n\nWe have two different buckets:\n\n - The bucket which contains full backups taken every 15 minutes expires\n   after 7 days\n - The bucket which contains daily backups moves objects into cheaper\n   storage after 30 then 60 days, and then expires after 90 days"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/JamesWoolfenden/terraform-aws-glue-crawler/commit/484b85587d32f2f2a6d54ffe16d5c759841961f8",
    "content": {
      "message": "costs"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/GBergeret/micro-service-as-code/commit/46f76d50b8569f450ce909e04f3c5fa81b97737a",
    "content": {
      "message": "feat: Move from ELB to ALB\n\nUpdate project to use ALB in order to save money as ALB are cheaper than\nELB. Also Classic Load Balancers are deprecated."
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/aaaaasam/azure/commit/c7bc0ce6f3fcaffcbbe7753f1a9d8437809bc167",
    "content": {
      "message": "using az account to get subscription_id and changing to use a smaller VM size to save cost."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Hapag-Lloyd/terraform-aws-bastion-host-ssm/commit/516075e2987bdd1063f22768d451c1c1eb647175",
    "content": {
      "message": "feat: add spot instances to save costs (#41)"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Hapag-Lloyd/terraform-aws-bastion-host-ssm/commit/840ed0938bea8867caba921446b8a77a2f1c9e7d",
    "content": {
      "message": "docs: add cost estimation based on 1.9.1 (#43)"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ExpediaGroup/apiary-data-lake/commit/47e62f2fc73a96611606cd619c084d1ded9d844d",
    "content": {
      "message": "Feature/storage cost optimization (#106)\n\n* Adding lifecycle policy for s3 data buckets\n\nMaking No of days in Lifecycle policy configurable\n\nMaking storage classes configurable at schema level\n\n* PR comments\n\n* PR comments\n\n* PR Comments\n\n* PR Comments\n\n* PR Comments\n\n* fix to run terraform when apiary_managed_schemas is empty"
    },
    "codes": [
      "saving",
      "feature",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/nmaupu/auto-gcp/commit/ef1df1044df317c78987f7f8398b4e45f5a3ee0a",
    "content": {
      "message": "GKE master are now on only one zone (billing)"
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/jenkins-infra/aws/commit/4a9e2c05092acca34f0608394638776c75496a99",
    "content": {
      "message": "feat: increase cik8s worker nodes sizing to m5a.4xlarge\n\n- Goal: comply with https://github.com/jenkins-infra/jenkins-infra/pull/1872\n- m5a.4xlarge\tonly costs 2x more,  for 16 vCPUs (AMD EPYC),\t64 GiB\n- That would allow ~3-4 pods per machine (let's consider 3)\n- Max of 50 machines: that should increase containers cap. to 150\n\nSigned-off-by: Damien Duportal <damien.duportal@gmail.com>"
    },
    "codes": [
      "saving",
      "awareness",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/rbabyuk/terra/commit/beae899804779adf914c08f290c5d71b542c9ed1",
    "content": {
      "message": "sets 'micro' VM profile to minimize costs"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ministryofjustice/hmpps-delius-core-terraform/commit/6e25b2cfb1cb80fd45ab6627a04e52735bb2fb5e",
    "content": {
      "message": "ALS-2957 Set default placement strategy to AZ-balanced bin-pack\nto save on EC2 costs while maintaining service reliability/resilience."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/wellcomecollection/goobi-infrastructure/commit/271bf02f91bfe5e8fc7ea9f94bfe56fd49a8d8e0",
    "content": {
      "message": "Add a VPC endpoint for S3\n\nThe lack of any VPC endpoints is a non-trivial cost \u2013 about $400 a month\nin managed NAT Gateway, and that was before anybody started using\nArchivematica for serious work.\n\nThis change adds an S3 VPC endpoint to the workflow and workflow-stage\nVPC, which should dramatically cut those costs.\n\nI haven't set up other endpoints yet because I'm not sure what other\nservices we're using; I'm guessing S3 is probably the bulk of the load."
    },
    "codes": [
      "networking",
      "saving",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Leonard-Ta/Sample-Security-service-Terraform/commit/c16481a84d5823b65ce96bd811a265222385b43b",
    "content": {
      "message": "Reduced preset variables for better cost optimization"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/jjffggpp/jjffggpp/commit/0e69a9978eae9e3432de8f16c2ca1b38033c23a7",
    "content": {
      "message": "Add cost optimized variables to libvirt"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/jjffggpp/jjffggpp/commit/948727c5b1e87b970a7a9dee4cd67bb84d19abf8",
    "content": {
      "message": "Add cost optimized variables to the cloud providers"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/jjffggpp/jjffggpp/commit/93ee12adde6ac773c76b590fe89c24df372f326b",
    "content": {
      "message": "change default azure vm size and disk type to be cheaper"
    },
    "codes": [
      "saving",
      "instance",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cisagov/vulnerable-instances/commit/f70410061b8c6b9249895571e05dfb7a142efb18",
    "content": {
      "message": "Upgrade the root volume type to gp3\n\ngp3 is 20% cheaper, and the baseline configuration offers better\nperformance than gp2 for volumes smaller than 2TB.  It also allows the\nvolume size and IOPS to be configured separately, whereas the two are\nintertwined with gp2.\n\nAlso remove the delete_on_termination line, since\ndelete_on_termination = true is the default, so there is no need to\nspecify it."
    },
    "codes": [
      "saving",
      "instance",
      "awareness",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/lwilliams1990/deepfence-threatmapper-lab/commit/cbd8ce761041eac3afbdfbf74e001895895682f2",
    "content": {
      "message": "Explicitly size instance to m5.large, below minimum spec on https://github.com/deepfence/ThreatMapper#pre-requisites but will try at this size to lower cost of the lab"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/adeelbarki/aws-with-terraform-and-ansible/commit/742f35a1fdf66b24ddb1f6a514b2af5a1f2b4282",
    "content": {
      "message": "corrected resource parameters for low cost and changed version"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/FXinnovation/fx-terraform-module-aws-virtual-machine/commit/e58135aece58a6fa930b585ad2dc74ad089cf739",
    "content": {
      "message": "test: uses m5a instance because less expensive"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/FXinnovation/fx-terraform-module-aws-virtual-machine/commit/73524718199222025197ed7e96bf332ea193dc3e",
    "content": {
      "message": "feat (BREAKING): default instance type is changes to t3.small to cheaper t3.nano"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/EOjeah/route53-ps/commit/1c619d5df3f2e7f49dd9a3a47150aa41b0de497f",
    "content": {
      "message": "separate delegation set ID\n\nseparating delegation set ID ensures that the same name servers are not destroyed when trying to destroy all resources to save cost\n\nrunning terraform destroy main.tf will not destroy the delgation set id\n\nhttps://stackoverflow.com/questions/60823339/deterministic-name-server-addresses"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/SamTowne/BasketballDrillBot/commit/4ec6d54e4d36ab02b0a7daf042e727717371eaec",
    "content": {
      "message": "Await Athena Query Completion\n\nBucket notifications are used to continue app flow after Athena Query\nresults arrive. The reason for designing the application like this is to\nlimit Lambda Function runtime. This is done by invoking each function asynchronously.\n\n1. The user submits the google form.\n2. The API Gateway receives the post and triggers the collection Lambda\nFunction.\n3. The collection Lambda Function stores the post data and triggers the\nsetup_processing Lambda Function.\n4. The setup_processing Lambda Function starts execution of an Athena Query.\n5. The Athena Query results file creation invokes the processing Lambda Function (via bucket notification).\n6. The processing Lambda Function starts execution of an Athena Query.\n7. The Athena Query results file creation invokes the response Lambda Function (via bucket notification).\n8. The response Lambda Function formats the data into an email, sends\nit, and invokes the cleanup Lambda Function.\n9. The cleanup Lambda Function runs to remove temp files.\n10. The user receives the Shooting Insights email.\n\n- Athena query completion times can vary greatly and bucket\nnotifications are used to trigger lambda functions that require the\nquery results\n- New Lambda function setup_processing ensures that the Athena table is\ncreated before any queries are run against it\n- Add a processing s3 bucket\n- Add a temp bucket\n  - This bucket is used for sharing data between the lambda functions (primarilly the new event data\n  and athena invocation IDs)\n- Add bucket lifecycles to expire old data, incomplete uploads, and to\ntransition data to cheaper storage tiers\n  - The intelligent archiving option is being used\n  - It may be better to explicitly specify a storage type transition\n  after x number of days instead of using intelligent archiving\n- Bucket notifications\n  1. Processing\n    - Notification attached to processing s3 bucket\n    - Invokes processing lambda\n    - Awaits create table athena query completion\n  2. Response\n    - Notification attached to athena s3 bucket\n    - Invokes response lambda\n    - Awaits select query completion"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ecsworkshop2018/expertalk-2018-ecs-workshop/commit/034908d914982eacea51b0ac61f2781069387412",
    "content": {
      "message": "Added ability to save costs if no private subnets required"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/amezousan/serverless-blog-in-aws/commit/c130862a29878f0e483551bc115d2ca5bdc4bf5c",
    "content": {
      "message": "Disable unneeded methods for saving costs"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/prometheus-aws-configuration-beta/commit/0edcb1dbd7337eab4155be2f2a7683a77cc3cc6d",
    "content": {
      "message": "Remove multi nat-gateways in dev to save on cost\n\nThe dev enviroment currently creates three of every network component  based\non the configuration in the vpc module. I have added a small change which will\nprevent the dev enviroment from creating a nat gateway for each AZ. If the\ninstance count is 3 in dev the single nat gateway will still be enabled. This\nis fine since the dev enviroment is not HA enviroment. There will be extra routes\ncreated to route traffic through the AZ which has all the active nat-gateway.\n\nThe current production enviroment currently supports an gateway per AZ. This is for HA\npurposes."
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/hmcts/cnp-module-api-mgmt/commit/038d43dc5f17870bc6681e1d8699e0a0951a859e",
    "content": {
      "message": "Uses the Premium tier when in production\n\nPlease note Premium tier is very expensive but is the only non\nDevelopment Tier which supports VNETs"
    },
    "codes": [
      "saving",
      "billing_mode"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/dwp/dataworks-aws-data-egress/commit/14f065e5161fee14c286c34df7db9f5516ef9bb6",
    "content": {
      "message": "Use GP3 which is much cheaper and shrink the size except for prod (#22)\n\n* Use GP3 which is much cheaper and shrink the size except for prod\n\n* Volume tags"
    },
    "codes": [
      "saving",
      "billing_mode"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/UCDenver-ccp/Translator-TM-Provider-Infrastructure-Modules/commit/ddc29467494ee63c323d0eb6493de6e8440c113e",
    "content": {
      "message": "consolidated oger node pools\n\nTo save cost when idle, the individual oger node pools (one for each ontology) have been replaced with a single node pool."
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/emyasa/hashicorp-consul-setup/commit/0c38902905b96c598715a08d5c3880ad01ce5fea",
    "content": {
      "message": "changes:\n* setup fake service as api and web\n* make use of bastion as client-web (to minimize aws costs)\n* attach necessary security group and user data to bastion instance to act as a consul client\n* removes unnecessary security group on client"
    },
    "codes": [
      "saving",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/circleci/enterprise-setup/commit/26cc5295c2bb9d8756e450712e0f5f75af440c4a",
    "content": {
      "message": "Update default dev env sizes\n\nPick slightly less expensive instance sizes"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/linuxlsr/azureLearning/commit/163a9de084161e21cb62c19033b4bff7d3a5e649",
    "content": {
      "message": "add outputs, monitoring, tear down vm resources that cost"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/linuxlsr/azureLearning/commit/e2a2ef243deb0374ccfe3f37e0d1395d4338969c",
    "content": {
      "message": "teardown resources that cost"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/bh1m2rn/gitlab-environment-toolkit/commit/b9750f0bb88bc22256085b6bc8f060055e90a8c4",
    "content": {
      "message": "Update AWS disk type to gp3\n\ngp2 is bursty and can lead to performance issues.\ngp3 is also cheaper."
    },
    "codes": [
      "saving",
      "instance",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/dankline/waapdemo/commit/087f2caeff6ecaa2a18ecc2f20b7313a70d2d191",
    "content": {
      "message": "Change the SKU to a cheaper one"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Katesagay/terraform-repo/commit/9aacfbe12e3e7c1e726b9a3d834211aae01f419c",
    "content": {
      "message": "billing alert in terraform"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/wellcomecollection/loris-infrastructure/commit/77c58fa499829785825e45aa90e1bb6c7058c5a2",
    "content": {
      "message": "Migrating main loris cluster to c4.xlarge with EBS IO1 volume. Switching second loris cluster to EBS GP2 to see if we can reduce costs"
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/sastels/learn-AWS-terraform/commit/0816f145a8b709df6af66a7dad229a4ff31619ba",
    "content": {
      "message": "7.10: Reduce Storage Costs with EFS (#7)"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/schramm-famm/bespin/commit/cff3747c70722eba940317dec58dc9c22b3b4939",
    "content": {
      "message": "Add Terraform config for VPC and autoscaling group\n\nThe VPC has a NAT that allows the EC2 instances in the private subnets\nto be able to connect to the ECS cluster. I think the NAT gateway will\ncost some small amount of money per hour.\n\nThe autoscaling group spawns the EC2 instance(s) that will connect to\nthe ECS cluster and will eventually run the actual services/tasks."
    },
    "codes": [
      "networking",
      "awareness",
      "increase"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/SaltyDev42/OSXP-Hackathon-SX/commit/4be7890dc968fbbfa90dfce095feca6e9be24316",
    "content": {
      "message": "Changed from t2 to t3a, cheaper (maybe?) and faster"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cmbrad/cy-aws/commit/9b954fd2e8e8a51285154388dc2390484fe93c31",
    "content": {
      "message": "Back grafana using EFS for cheap persistence, promtail tweaks"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/zoitech/terraform-aws-account/commit/2eb42357d986ad2c3d8693907df99895cee25c63",
    "content": {
      "message": "Added new policies to prevent users causing huge costs; created  changelog.md"
    },
    "codes": [
      "saving",
      "policy"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/gigaSproule/terraform/commit/3a7df234e8abd47c37245b5b66fcd85d40620dc3",
    "content": {
      "message": "Remove AWS vm as it costs :("
    },
    "codes": [
      "saving",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cbChgit/testrepo/commit/803640f6e77857520f69700ccc27c772ee7564e4",
    "content": {
      "message": "Minecraft (#18)\n\nMinecraft recipe\n\nFeatures\n\nRuns itzg/minecraft-server Docker image\nPreemtible VM (cheapest), shuts down automatically within 24h if you forget to stop the VM\nReserves a stable public IP, so the minecraft clients do not need to be reconfigured\nReserves the disk, so game data is remembered across sessions\nRestricted service account, VM has no ability to consume GCP resources beyond its instance and disk\n2$ per month\nReserved IP address costs: $1.46 per month\nReserved 10Gb disk costs: $0.40\nVM cost: $0.01 per hour, max session cost $0.24"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/EngineerBetter/kf-infra/commit/fa5f7fb35b0b44020fb81dd5c4e3b9ceaca1f967",
    "content": {
      "message": "Use cheaper nodes"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/travis-ci/terraform-config/commit/4f641b162fa877aef842481631906d5bfe874781",
    "content": {
      "message": "Drop down some staging instance sizes for great cost savings glory\n\n- 2 * c3.4xlarge + 2 * t2.medium =~ $1295/mo.\n+ 2 * c4.large + 2 * t2.micro =~ $164/mo."
    },
    "codes": [
      "saving",
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/travis-ci/terraform-config/commit/0e2fc3ca535ca4a1fe3097b336fb145e6c73cde6",
    "content": {
      "message": "Use a custom machine type for NAT hosts\n\nper recommendation from GCE, thereby reducing monthly costs by ~$21/host\nin production."
    },
    "codes": [
      "networking",
      "saving",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/bhavikkumar/terraform-master/commit/8a71bbe9588b78351fe231a4341d07a5e2101ebb",
    "content": {
      "message": "Add billing policy to admin group"
    },
    "codes": [
      "awareness",
      "policy"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/KoutaroNohira/hashicat/commit/81dc1d3f98034672d5f62f440f2cc3abc58ce2a2",
    "content": {
      "message": "fixed cost"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/robinbryce/iona/commit/e89519dc59655ddbd3ecf56a4d680d7d01128152",
    "content": {
      "message": "* Disable redis provider for traefik (using CRD's exclusively)\n* Disable google memorystore - its too expensive and its not being used*"
    },
    "codes": [
      "saving",
      "awareness",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/techservicesillinois/aws-enterprise-vpc/commit/0d21bea79e1936e2bdaee58bd6e328dd08e59b30",
    "content": {
      "message": "bootstrap: change DynamoDB billing_mode to \"PAY_PER_REQUEST\"\n\nAs of this writing, minimal PROVISIONED mode is only costing us $0.56/month, but it would take ~373k (each) read and write requests to spend that much with on-demand pricing"
    },
    "codes": [
      "saving",
      "awareness",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/wallnerryan/terraform-scaleio/commit/605e74facfa2ff519ba5cda6e57474666901bd8c",
    "content": {
      "message": "Allow for cheaper instances and choosing instance types"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cloudposse/terraform-aws-alb/commit/c3c8dd8bebf14f91518253cadadc6d8a88b5ea09",
    "content": {
      "message": "Add enabled variable (#56)\n\n## what\nThis allows the ALB to be created selectively.\n\n## why\nAs a cost saving measure, I want to be able to create an ALB per AWS account for a given application and then create multiple target groups.  \n\n## references\n* This addresses https://github.com/cloudposse/terraform-aws-alb/issues/55.\n* Closes #55"
    },
    "codes": [
      "saving",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/byu-oit/terraform-aws-rds/commit/86a0795540edb426c3226775d73fcd4ce807d36a",
    "content": {
      "message": "Change default to t3 family\n\nBecause t3 instances are cheaper and are sometimes more performant, the BYU cloud office is considering preventing t2 hardware from being launched to encourage the use of t3 instances. Regardless of the decision the cloud office makes, t3 family instances seem like a better default for cost and performance reasons."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/akaron/kubeadm_aws/commit/2e2092ec94b27a4c3f0b9f4ee4d46a1983a72518",
    "content": {
      "message": "minor updates: use 1 master to save cost; volume_size as variable"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/commit/c50fd3b3f25f87a6326684ad44d507d187e24874",
    "content": {
      "message": "feat(vm-series): SystemAssigned identity a default\n\nBy default use a managed identity instead of none. It's costless:\nhttps://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/services-support-managed-identities"
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/commit/d9b4b9dcb95b96bdfb09f9631acd91ab34aa7dcf",
    "content": {
      "message": "feat(vm-series): add Application Insights metrics\n\nVerified that the defaults here do not currently cost anything and do\nnot require extra permissions. Users are billed only when they actually\nmanually paste the metrics_instrumentation_key by GB of metrics sent.\n\nAn opinionated setup is thus to furnish a single Application Insights\nfor every module instantiation.\n\nSeemingly, no RBAC changes are required for this to work, it doesn't\nmatter if firewall runs on a SystemAssigned or a UserAssigned or even\nNo Identity."
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/commit/72e7126b0baf6e039f70c12262d11e7ea7562189",
    "content": {
      "message": "feat(examples/common): clearly show \"bundle1\"\n\nThe \"bundle1\" is great for a newcomer, but financially expensive for the\ndevs. Just show visibly, so that devs could catch on and change to byol."
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/UrbanOS-Examples/common/commit/206394bcc75866953f64cbf3bd6214e4e6f96e91",
    "content": {
      "message": "Right size Kong instance\n\nKong is serverely under-utilized in every environment,\nincluding prod.\nBring the instance down to a less expensive option."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/darogina/terragrunt-aws-modules/commit/9c84d03bde131e7f349dcd493ba5b7e55bf8ae2c",
    "content": {
      "message": "Replace CloudWatch billing alarm with AWS Budget"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cloudspout/Gefjun/commit/665692a86bb65ddfa6c001f296c76c17288e2944",
    "content": {
      "message": "Lower costs by deactivating verbose logging & long archive period"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/vx-labs/infra/commit/302b243d09b58375efbff31c9e2dfc60c3d7bb7b",
    "content": {
      "message": "terraform: put all the nomad-agents behind a proxy to reduce costs"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/kitchen/personal-terraform/commit/b0660183c95f4b938dbf33c5607381b8e4e2bd68",
    "content": {
      "message": "add a shared vpc\n\nI only want to create one cloud NAT instance as I think it costs me\nmoney? Not entirely certain. shared VPC seems to be the way to do it"
    },
    "codes": [
      "networking",
      "awareness",
      "increase"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/kitchen/personal-terraform/commit/e3d337caa558830f8e337d24edaab577ee9d43a2",
    "content": {
      "message": "set up a load balancer for vault.kitchen.horse\n\nI'm not sure I like this model.\nI like the forwarding rule -> http/https proxy -> backends (by rules)\nthing, but I think the mapping of proxy -> rule -> backend should be an\nexternal resource so I can make one load balancer for everything in\ngcp-central and just attach things to it. This would save me from having\nto have a bunch of external IPs that I have to pay for, or from having\nto centralize all of my load balancer configuration"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/kitchen/personal-terraform/commit/bc22c89eabce912e3bb5ced85348d37df351b06d",
    "content": {
      "message": "turn off kitchen.horse public web services\n\nsadly the forwarding rule and the global IP address are freaking EXPENSIVE.\nMaybe I'll find another alternative at some point, but for now, it's wayyyyy too\nmuch. $18/mo for this crap.\n\nNot as bad as the ripoff NAT gateway fees, but close;."
    },
    "codes": [
      "domain",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/kitchen/personal-terraform/commit/fe1f2665b198308a438ec3d46b24843089df1a2a",
    "content": {
      "message": "turn off the NAT instance because suuuuuper expensive. Meh."
    },
    "codes": [
      "networking",
      "awareness",
      "increase"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/glenngillen/terraform-demo-cost-estimation-azure/commit/f29cf1138d21a2eba8aaccec3185e461cb8675bd",
    "content": {
      "message": "So expensive."
    },
    "codes": [
      "increase"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/glenngillen/terraform-demo-cost-estimation-azure/commit/dfc9a7ea4a62b6e6c14e09b2f08c7fb5c63ca711",
    "content": {
      "message": "Very expensive!"
    },
    "codes": [
      "increase"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/acmucsd/tech-stack/commit/e78aad174927ca59a43a622d7e82f664e3005790",
    "content": {
      "message": "Add Minecraft AWS instance provisioning config\n\nUpdate cost table to reflect instance changes.\n\nUltimately, we've decided on hosting Minecraft on AWS simply because\nwe might be saving costs in the long term."
    },
    "codes": [
      "awareness",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alstard/terraform/commit/17463f89d0801c30bbca8ad5f0c4ef7f6f15e49f",
    "content": {
      "message": "Switched to Amazon Linux - cheaper and CentOS like"
    },
    "codes": [
      "saving",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Terraform-Projects/lead-terraform-Dashboard/commit/71a0250ad3aeed1da2b11734ac22c21831f3595d",
    "content": {
      "message": "updates for cost optimization - essential vs preemptive ASGs"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/mambotangu/mamz-cloud-foundation/commit/ccd76152996d46c2f4feafcf2abec29384446f8f",
    "content": {
      "message": "Added billing alertd to shared tier."
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Crown-Commercial-Service/digitalmarketplace-aws/commit/9928d58017829d85789bdf45f467b6d32c6354b2",
    "content": {
      "message": "Raise log streaming lambda function timeout to 10s\n\nWe've seen some occasional (a few per day) timeouts on the log\nstreaming lambda functions likely caused by slow Elasticsearch\nresponses.\n\nRaising the timeout value from 5s to 10s might help reduce the number\nof failed runs. We don't want a very high timeout value since if\nElasticsearch at any point starts to respond to requests very slowly\nour functions will spend more time waiting which can very quickly\nraise the AWS Lambda costs."
    },
    "codes": [
      "awareness",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/vuvuzella/jt-aws-serverless-app/commit/53d023dc22daa7b4181b8ea51ce72c6b857aa553",
    "content": {
      "message": "removed codebuild because it's hella expensive\nadded .circleci to use circleci for ci instead"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/utilitywarehouse/tf_telecom/commit/17007456991c1a8faa26b1f4ac993883f577d124",
    "content": {
      "message": "feat: Cold Storage for S3 (#15)\n\nadds functionality to move old s3 objects into\ncold storage for cost saving"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/danielpodwysocki/infra/commit/d8cb608a3c38868c99d4e7868488162b81f8fb06",
    "content": {
      "message": "feat: remove kubernetes nodes, downgrade Xardas hardware to save costs"
    },
    "codes": [
      "saving",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cds-snc/terraform-modules/commit/75fc899c58848b290a31ff92bab92c27b3f6bf53",
    "content": {
      "message": "feat: S3 delete marker replication status (#110)\n\nUpdate to allow the S3 bucket's replication rule to specify if delete\nmarkers are replicated to the destination bucket.\n\nThis is being done to support the Cloud Based Sensor so that\nsatellite buckets can have a shorter expiration time and save\nstorage costs."
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/rribeiro1/terraform-aws-budget-alarms/commit/cd81824b92fb7205071ca00359ce4bd86412af10",
    "content": {
      "message": "Billing alert changes (#9)\n\n* Add chatbot integration"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/willfarrell/terraform-logs-module/commit/545b0bb763a4dbaab6b262c4b9642767e2434d07",
    "content": {
      "message": "docs: costing"
    },
    "codes": [
      "awareness",
      "policy"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/falldamagestudio/UE4-GHA-BuildSystem/commit/e58083adbf91e7daa8ddb5db6c3b2e5c8c0a906c",
    "content": {
      "message": "Change pd-ssd to pd-balanced\n\nTests have shown pd-balanced to have performance reasonably close to that of pd-ssd, but at half the cost"
    },
    "codes": [
      "saving",
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cookpad/terraform-aws-eks/commit/59c40286757e1fa5cb5391421c5380e5ad506387",
    "content": {
      "message": "Run GPU node group tests with on_demand instances (#236)\n\nThis should fix the flakiness we have seen with this test recently.\n\nUsing g4dn.xlarge instance type as this is currently the lowest cost\nNvidia GPU node"
    },
    "codes": [
      "saving",
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/robertlupinek/rh-ex407/commit/0c679d7adfa5bf38d5c7846958f3508fc036b3e9",
    "content": {
      "message": "Adding set private IP addresses and allowing public IP addresses on non-jump hosts.  Too cheap for Nat instance. :)"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/JamesWoolfenden/terraform-aws-workspaces/commit/e4558e322cd6af610c43a840a59af8dad41f307f",
    "content": {
      "message": "cost"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ndebuhr/cloud-native-workstation/commit/301244e00d9f359f964242849462704f80e8db13",
    "content": {
      "message": "\ud83e\udde0 Parameterize Jupyter GPU provisioning, for more flexibility in AI/ML/GPU workload cost-performance decisions"
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/keithnoguchi/do-in-action/commit/2291a1ac7e47009d058a1a448760cd5559e5946e",
    "content": {
      "message": "vars.tf: Increase the number of droplets to 40 each\n\nTo hit the 40Gig total with cheapest droplets :)"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ayltai/hknews-infrastructure/commit/68171be117d3997b84253258f41fad6daedbc76a",
    "content": {
      "message": "Use a bigger but cheaper EC2 instance"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/skehlet/aws-batch-processing/commit/decdbce98d33cf2599aee554779ef5d8b5361d8f",
    "content": {
      "message": "updated lambda to use SQS as an event source; added NAT instance option (cheaper than NAT GW)"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/JamesWoolfenden/terraform-azurerm-mysql/commit/ab28ba331929475431fe695036d0863d329cdac2",
    "content": {
      "message": "costs"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ae-lexs/terraform_modules/commit/84628710c900b2d1db058aa0e339134f24e6d20c",
    "content": {
      "message": "billing_cloudwatch_alarm module documentation (#5)\n\n* Rename cloudwatch_alarm to billing_cloudwatch_alarm\n\n* Add documentation for the billing_cloudwatch_alarm module"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ae-lexs/terraform_modules/commit/8cd62acc20ce3418eaf4cdd9d8b472091101cdbd",
    "content": {
      "message": "Fix the issue with threshold variable of the billing_cloudwatch_alarm module (#6)"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/reschouw-homelab/terraform-homelab/commit/e19dd7497737376a07e62042ed9fb3e8ad9739a9",
    "content": {
      "message": "Moving eks project to archive\n\neks was cool but cost-prohibitive for a homelab"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/austin1237/gifbot/commit/c11dabf1ea02c0e044c62448986bb3f9abdf3967",
    "content": {
      "message": "reducing read and write capacity to 1 to reduce costs."
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/joelchrist/terraform/commit/bbf18d695bd7597977ea7a97d5434ca7f1a37d57",
    "content": {
      "message": "Downgrade machine type to save costs"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ToruMakabe/aks-anti-dry-iac/commit/4ba7a9dc3085ab701c85737a4f36dd57fcd7596f",
    "content": {
      "message": "improve: cost reduction (test env.)"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/servers-tf/infrastructure/commit/cc9e50a3864511f9fb9f871293e6a6e7e2719d2c",
    "content": {
      "message": "Make dynamodb cheap"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/chetanbothra/Terraform_AWS_Billing_Alert/commit/43b0d3b4cef0d3f57d4f5d4f1c7aeb9bfc3e362a",
    "content": {
      "message": "Terraform Script for AWS Billing Alerts"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/poseidon/terraform-azure-kubernetes/commit/633eb938742a43be09612b944c29aaaf70dac119",
    "content": {
      "message": "Change Azure default controller_type and worker_type\n\n* Change default controller_type to Standard_B2s. A B2s is cheaper\nby $17/month and provides 2 vCPU, 4GB RAM (vs 1 vCPU, 3.5GB RAM)\n* Change default worker_type to Standard_DS1_v2. F1 was the previous\ngeneration. The DS1_v2 is newer, similar cost, more memory, and still\nsupports Low Priority mode, if desired"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/mozilla-platform-ops/devservices-aws/commit/ebb4040f6ad47d3e41c5ddf2cdceac3b3a8abead",
    "content": {
      "message": "Add s3 bucket to store Mercurial backups\n\nWe have several gigabytes of old repos sitting on an NFS volume\nin SCL3. It's cheaper to store them in S3. Plus, we'll soon be\nmigrating a bunch of data and it is easier if we have less data to\nmigrate."
    },
    "codes": [
      "saving",
      "awareness",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/poseidon/terraform-aws-kubernetes/commit/12f8972a912d97427fe0ef1b84becdc8a2993022",
    "content": {
      "message": "Change kube-apiserver port from 443 to 6443\n\n* Adjust firewall rules, security groups, cloud load balancers,\nand generated kubeconfig's\n* Facilitates some future simplifications and cost reductions\n* Bare-Metal users who exposed kube-apiserver on a WAN via their\nrouter or load balancer will need to adjust its configuration.\nThis is uncommon, most apiserver are on LAN and/or behind VPN\nso no routing infrastructure is configured with the port number"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/poseidon/terraform-aws-kubernetes/commit/ef0372d2684ef920c6e54cf8b9f80d87db90e636",
    "content": {
      "message": "Combine NLBs to use one NLB per cluster\n\n* Simplify clusters to come with a single NLB\n* Listen for apiserver traffic on port 6443 and forward\nto controllers (with healthy apiserver)\n* Listen for ingress traffic on ports 80/443 and forward\nto workers (with healthy ingress controller)\n* Reduce cost of default clusters by 1 NLB ($18.14/month)\n* Keep using CNAME records to the `ingress_dns_name` NLB and\nthe nginx-ingress addon for Ingress (up to a few million RPS)\n* Users with heavy traffic (many million RPS) can create their\nown separate NLB(s) for Ingress and use the new output worker\ntarget groups\n* Fix issue where additional worker pools come with an\nextraneous network load balancer"
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/poseidon/terraform-aws-kubernetes/commit/e09126b45746f1c967d1990fa04ce781a0478c6d",
    "content": {
      "message": "Change AWS default type from t2.small to t3.small\n\n* T3 is the next generation general purpose burstable\ninstance type. Compared with t2.small, the t3.small is\ncheaper, has 2 vCPU (instead of 1) and provides 5 Gbps\nof pod-to-pod bandwidth (instead of 1 Gbps)"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/paperphyte/terraform-drone/commit/79f4b7c2cf3ad2d1a6d2646eaf27a08fd2611d07",
    "content": {
      "message": "feat: aws spot fleet for cheap build agents"
    },
    "codes": [
      "saving",
      "instance",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/paperphyte/terraform-drone/commit/f62bfebb54530c9466cfdb21336794f24bcd63a7",
    "content": {
      "message": "feat: optimize cost and upgrade to terraform12\n\n* feat: remove task load balancer\n* feat: update dns on task state change\n* feat: enable drone built-in autocert for https\n* fix: allow less with improved dns policy\n* fix: default port for build agent\n* feat: convert to terraform12 syntax and functions (#7)"
    },
    "codes": [
      "saving",
      "policy"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/eutambem/eutambem-serverless/commit/43182470f02a463a4b9ee5ed3ec899f5d7de9653",
    "content": {
      "message": "descrease max capacity of rds cluster for cost efficiency"
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/openfun/jitsi-meet-torture-rocket/commit/349c94aeffffb4eef513107d281d59ef6dd33962",
    "content": {
      "message": ":sparkles:(terraform) possibility to deploy stacks of instances\n\nWe will use small instances because they are cheaper, but we cannot\nsupport too many instances on the same conference. Thus, we have to\nseparate meet-torture instances in stacks."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/bculberson/btc2snowflake/commit/9f8227bf53ebc2b1bb0b99d0697f9f66eed7ab6d",
    "content": {
      "message": "optimize ebs costs"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/HumanCellAtlas/dcp-monitoring/commit/4fd89da7e37ce7f22d920e3584761f702383a8ba",
    "content": {
      "message": "Add alert description to alert payload.\n\nThis will enable more informative slack alerts."
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/NDPH-ARTS/mts-trial-deployment-config/commit/0cb5e776b8695da57892560c89010a8abd817afc",
    "content": {
      "message": "Include role-service (#39)\n\n* move to app service premuim\n\n* move the sql serverless to save costs\n\n* add role-service app\n\n* add a new trial\n\n* move to mssql\n\n* update sql\n\n* create a tfvars file\n\n* update ports\n\n* update\n\n* update\n\n* Update terraform/trial_rg/modules/kv/main.tf"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/picatz/terraform-google-nomad/commit/3c48151cd75447f7fbd3e6ce14c491cd28e2442c",
    "content": {
      "message": "Bump client machine type\n\nMight change back to the cheaper one by default, but having more power for clients out-of-the-box has been preferable in my situation."
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/terraform-google-modules/terraform-example-foundation/commit/8391f1bd4322fec04fda7509b537c5f66cddbbd9",
    "content": {
      "message": "fix: data_access_logs_enabled now enables read and write audit logs, defaults to false for cost savings (#630)\n\n* Comment enabling Read and Write\n\n* Fix code review issues\n\n* Fix code review issues\n\n* Changes default value to false\n\n* Fix build error\n\n* Fix dynamic code"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/filhodanuvem/from-dev-to-ops/commit/998be8119321e8812884075b078a1d5fb36cfa69",
    "content": {
      "message": "Use EC2 spot instances to reduce cost"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/sivaganesan23/EKS-Terraform/commit/c82a19d98673c1aeeb1fa26afb983acbfbea272f",
    "content": {
      "message": "Add ec2 cost optimination using tags"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/andreishappy/clouduru-infra/commit/991915ce492e074d8df658aa54c902b52f9c4cc5",
    "content": {
      "message": "Fargate cluster (commented out to avoid costs)"
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/andyfangdz/aws-crawler/commit/7f2ffcc3cbb8538f82cb14377edb08b49105baf7",
    "content": {
      "message": "did you know dynamo read capacity is really expensive"
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/mudathirlawal/azure-terraform-packer-iac/commit/587d3a91c12aae8f0dd4b34a46d03f919ea385c6",
    "content": {
      "message": "\"feat: Remove update domain options\"\n\nRemove fault domain count as well as\nupdate domain count options from scale\n(availability) set. This keeps both the\nfault domains and update domains at five\neach. This is necessary since we are try-\ning  to minimize cost."
    },
    "codes": [
      "domain",
      "saving",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ntk1000/aws-terraform-template/commit/d016b96d89370b8039817fabdfa055576cf6b4cc",
    "content": {
      "message": "billing alert"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/bishoybassem/aws-jenkins/commit/3f27fe4cb4b75bffb7c1cf754f3f22a61cad9bfd",
    "content": {
      "message": "Use the same AZ for the subnets to avoid regional data transfer costs"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/StratusGrid/terraform-aws-single-account-starter/commit/c291c0954c89e1bfbdb76d4c8990baf9db986343",
    "content": {
      "message": "Feature/billing alerts (#6)\n\n* Updated required_version as the previous one was way out of date.\n\n* Uncommented parts of code designed to be used for other regions.\n\n* Removed unnecessary variables in code.\n\n* Reverted the version as the new one broke stuff.\n\n* Added EventBridge rules, ran terraform FMT\n\n* Added Environment tag back into config rule and updated the name of the eventbridge rule.\n\n* Fixed a typo.\n\n* Updated terraform version.\n\n* Added billing alerts for billing anomalies"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/jg210/aws-experiments/commit/5ff37f12a421fdd902d8eb1e6d7491ee181fd179",
    "content": {
      "message": "t2.nano costs half as much as t2.micro."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/robertdebock/terraform-azurerm-container-group/commit/c0d6578f1ebbdcc9cab091017259e4d596bfe8c9",
    "content": {
      "message": "Cheaper, that's how I am..."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/nsbno/terraform-aws-cost-alarm/commit/7e135499d33f0a5c51602a506fefe258cac072c6",
    "content": {
      "message": "cost alarm v1"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/wushin/ttrpg-tools/commit/9604c14a781530129e47e7a7684a90db86a7f7a0",
    "content": {
      "message": "Add Mongo back as a t2.micro for cost"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/grimoire-ca/dns.tf/commit/2a362c145bd6220b2baf94608a866de8c9c8eafb",
    "content": {
      "message": "Revert changes to support multiple zones.\n\nIt turns out the zones I was going to add lapsed last year, and there's no point in paying for R53 resources for them."
    },
    "codes": [
      "domain",
      "instance",
      "awareness",
      "saving",
      "area"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/grimoire-ca/dns.tf/commit/de637ed8a80faa4fe40907dd2aaa92cc735653ea",
    "content": {
      "message": "Remove the Distant Shore zone.\n\nThis is costing us about 50\u00a2/mo, which isn't much, but we're getting nothing for that money because we're not using the domain."
    },
    "codes": [
      "domain",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/stephaneclavel/terraform/commit/74b4ba406b9ea761d27298165d0e0de45c9d8491",
    "content": {
      "message": "now using spot instance to reduce costs"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/64kramsystem/ultimate_aws_certified_cloud_practitioner_course_terraform_configuration/commit/2f36b8a5f2f818138da72d218c1f3c9666ed54aa",
    "content": {
      "message": "CloudWatch Billing alarm"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/michael-griehm/azure-data-streams/commit/68c7b29b7142a634034a41fde5896846d0b4c493",
    "content": {
      "message": "Reduced the amount of partitions in the event hub to reduce cost"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/michael-griehm/azure-data-streams/commit/6eb6aced2b9ffb5190bec10322b23cde07ac783f",
    "content": {
      "message": "Reduced the amount of partitions in the event hub to reduce cost"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/michael-griehm/azure-data-streams/commit/02a82877d37eaa41f6f4bbda32d6ed6536da61d7",
    "content": {
      "message": "Made the function always on so the trigger works as expected, also reduced the capture frequency and skipping empty archive to reduce data movement to reduce cost"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/michael-griehm/azure-data-streams/commit/9a8221f612d32a4b4fadca51835e05906f93e023",
    "content": {
      "message": "Reduced partition count to reduce cost"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/JamesWoolfenden/terraform-aws-lex-lambda/commit/a39ed520b0048d54886d16c10c988ca905f64c58",
    "content": {
      "message": "costs"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/kelledge/idkfa/commit/25cda0b77ff329a89551cc6f14fe8c62820fd424",
    "content": {
      "message": "Add billing alarms\n\nEase of configuration for thresholds will be important"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/scott45/vof-deployment-scripts/commit/c6b2c1bee4c1e53e87fd3d94fc8c07cf64342d7b",
    "content": {
      "message": "Updates machine type for jumpbox so it uses one with smaller specs to reduce the cost"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/jsoconno/aws-terraform-remote-state-infrastructure/commit/fed8be2748bc2286a6f9888d282d66763ba612ed",
    "content": {
      "message": "default dynamodb to pay per request"
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/digitalronin/digitalocean-build-machine/commit/edbb68b29f7e055a50c16f62a71ee7be8c019407",
    "content": {
      "message": "Use a larger VM size\n\nThis is the next size up, in terms of CPU power. The cost is $15/month,\nwhich is roughly $0.02/hour (I think)."
    },
    "codes": [
      "instance",
      "awareness",
      "increase"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/nisunisu/AWS_Blue_Green_Deployment/commit/d0741cddb32ed7970904693e3a697603fa21bbbb",
    "content": {
      "message": ":art: Start using terraform variables and Comment out Elastic-IP setting to prevent unexpected billings."
    },
    "codes": [
      "saving",
      "billing_mode"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/fdns/terraform-k8s/commit/f106917bb7b2d8d4428022bb119585bf9f35769c",
    "content": {
      "message": "Restore t3a.medium and adding a duplicaed cheap_compute2 instance with count"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/tale-toul/SingleNodeOpenshiftOnLibvirt/commit/638430604158044fcf123adaf8dfdcc91b1a873e",
    "content": {
      "message": "Creates an AWS spot instance instead of the normal one for cost reduction.\n\nThe spot instance cannot use an AMI with RHEL 8.5 at the moment, AWS returns an error; it can use a RHEL 8.4 though\n\nSigned-off-by: Jose Ignacio Jerez Rodriguez <jjerezro@redhat.com>"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/oke-py/aws-tf/commit/ec2982c8742cc7bc294f8a3cc07ae9ba5ffcaced",
    "content": {
      "message": "create budget to notify me if actual cost > $30"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/wellcomecollection/platform-infrastructure/commit/cc8cb3ab5e257109ea9f6c19f4d034b1ac7ca4ca",
    "content": {
      "message": "Snapshot bucket (#4132)\n\n* Adds an S3 bucket for Elastic Cloud snapshots\n\nWe currently rely on Elastic Cloud to hold our snapshots for the catalogue API ES cluster.\n\nIn order to make working with Catalogue API ES data easy to do locally and to do outside of Elastic Cloud for reduced time and expense, it is useful and more arguably more reliable to hold these in an S3 bucket we have direct access to.\n\nTo complete this change we will need to make some changes to the Elastic Cloud configuration.\n\nSee: https://www.elastic.co/guide/en/cloud/current/ec-aws-custom-repository.html\n\n* add some documentation for ES snapshot storage\n\n* moved to critical stack\n\n* principals needs to be a list"
    },
    "codes": [
      "instance",
      "increase"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/combinator-ml/terraform-google-kubernetes/commit/703ef359dbe7d4d2f2bbf9c0bbd31f5b4bc30e68",
    "content": {
      "message": "feat: initial cheap gke cluster"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/combinator-ml/terraform-google-kubernetes/commit/27deaade65fd58e978b0a604c6c3fcd6683306bb",
    "content": {
      "message": "feat: add cheap(est) cpu autoscaling example"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cob16/aws_static_website/commit/0d4fbd0a7b296a5c9377a835dff89d1499716082",
    "content": {
      "message": "add pagerduty alerting\n\nThis is to avoid this site becoming unexpectedly expensive based of high traffic"
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/deadlysyn/terraform-keycloak-aws/commit/1c982ac4120ae3ed5a88c38f2a4d568ad9a83d22",
    "content": {
      "message": "use cheaper graviton rds instance type"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/JaredStufftGD/grok-airflow/commit/7ac9544b0c651fd8193eb063079514d0aa41e290",
    "content": {
      "message": "adding fargate spot as the capacity provider for the worker node for cost optimization"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/rajesh-nitc/gcp-foundation/commit/fc054a7183757d1e51e371e7329c5cf848cc6dd8",
    "content": {
      "message": "feat:save costs for personal organization"
    },
    "codes": [
      "saving",
      "area"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/rajesh-nitc/gcp-foundation/commit/eaf81b5f5ca1dbdd8951d4179a631818139566a5",
    "content": {
      "message": "feat:destroy dns zones when not in use to save cost"
    },
    "codes": [
      "domain",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/rajesh-nitc/gcp-foundation/commit/0db462f5528100ece08d85366fc777cd59fad473",
    "content": {
      "message": "feat:create spoke dns zones on demand to save cost"
    },
    "codes": [
      "domain",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/datarootsio/terraform-module-azure-datalake/commit/4907fd7598d0083705494182d985c6b3b58db2c0",
    "content": {
      "message": "Provision databricks resources with built-in token management (#29)\n\n* change default region to eastus2 for cheaper demos and faster provisioning\n\nhttps://azureprice.net/Region\n\n* use built-in token mgmt for databricks cluster creation\n\nCo-authored-by: samueldumont <samuel@dumont.info>"
    },
    "codes": [
      "saving",
      "area"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/mintak21/terraform-old/commit/c10b476d869282ed6cf55def47445b9c703788fe",
    "content": {
      "message": "Add aws bill alert :sparkles: (#6)\n\n* [feat] add budget cost module :sparkles:\n\n* [feat] add budget notification alert :sparkles:\n\n* [feat] lambda notification go src ver1 :+1:\n\n* [feat] use logging library :orange_book:\n\n* [feat] add lambda module :sparkles:\n\n* [structure] rename dir :railway_car:\n\n* [fear] create Makefile :sparkles:\n\n* [feat] fix details :shirt:\n\n* [feat] put ssm_parameter by terraform :sparkles:\n\n* [fix] make terraform formt :orange_book:\n\n* [fix] fix method scope :shirt:\n\n* [fix] fix dependency :bug:\n\n* [fix] del unused data :shirt:\n\n* [feat] add cloudwatch logs module :sparkles:"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ministryofjustice/hmpps-env-configs/commit/0328838420ac0d3754cf772a7d2f5bb1612193ed",
    "content": {
      "message": "Added configuration for LDAP EFS + reduced specs for root EBS volume\n\nProvisioned IOPS is no longer required for the root volume, as the LDAP data is now hosted on EFS. This should result in an EBS cost reduction."
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ministryofjustice/hmpps-env-configs/commit/28e80387490ac5303782b4762f1d42cac394464c",
    "content": {
      "message": "ALS-1712 Set ActiveMQ EFS throughput to 16MiB/s\n\nFollowing performance test results, and risk/cost evaluation here: https://jira.engineering-dev.probation.hmpps.dsd.io/browse/ALS-1712"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ministryofjustice/hmpps-env-configs/commit/670c006bad288d0360c3811aa63b3c323753c385",
    "content": {
      "message": "ALS-3882 Set LDAP instance type to m5.8xlarge\nEven though m5.8xlarge ($1.776) has more memory, it costs less than the c5.9xlarge ($1.818) in the eu-west-2 region"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ministryofjustice/hmpps-env-configs/commit/954dda617d47007a8a1ff5780d3174e900e95be1",
    "content": {
      "message": "Reducde DFI Instances size for identified cost savings"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/nephosolutions/terraform-google-gcp-project/commit/908c3fd85a60090cdd06f6f4b46893ceeeaf99fb",
    "content": {
      "message": "enable cloudbilling.googleapis.com by default\n\nThe Cloud Billing API is required to attach a billing account to the\nproject."
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Tripstagger-AA/tripstagger-google-infrastructure/commit/35cb09210e5d7c95e3bce8d92911407d4366cda5",
    "content": {
      "message": "chore(Orchestration): Cleanup expensive kubernetes"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Tripstagger-AA/tripstagger-google-infrastructure/commit/d89eab7b799c356608b95264ee04beb52f99c242",
    "content": {
      "message": "feat(Cluster): Cheap gke configuration"
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/jackofallops/terraform-aws-mysql-cluster/commit/7b2a446b0915a3ad26093f8234f7493ff152138a",
    "content": {
      "message": "Update variables.tf\n\nReduced default values for cost purposes during development..."
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/JamesWoolfenden/terraform-aws-config/commit/9df6f6e72cd4fa33003fa5498c796bd129c99d0f",
    "content": {
      "message": "costs"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/tlc-pack/ci-terraform/commit/b93d225cc274dba55027a4a3db2475e78c77018f",
    "content": {
      "message": "Add gd4n.4xlarge instances\n\nWe have these reserved so we're paying for them regardless, we should use them in CI"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/tlc-pack/ci-terraform/commit/aae09ea0dc9f394bd83e640ae003ba336a28a2b5",
    "content": {
      "message": "Add CPU-SMALL instances\n\nThis adds a class of CPU instances that are far cheaper than the current `CPU` instances that we can use for certain tasks that don't require big instances, I'm thinking for linting and builds (since those are mostly cached anyways). We could also potentially move certain test steps to these instances such as the unit tests but reliability."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/tlc-pack/ci-terraform/commit/9a51a8ae697b9f9df1c05c8276a871b614be2da9",
    "content": {
      "message": "Bump minimum capacity to num reserved instances\n\nWe should set this as a minimum for the autoscaler since they're reserved and we pay for them either way"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ryanlg/ryhino-public/commit/e51b9583b2df3154b5c82da361d411d65ed23bab",
    "content": {
      "message": "Destroy private subnets cuz expensive"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/BradleyChatha/infrastructure/commit/418d0f2bcbb34d808f433d78306507e05c50c93e",
    "content": {
      "message": "If only EKS were cheaper ;("
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/masterpointio/terraform-aws-nuke-bomber/commit/33fbb76715ce6e35565b5f83f7ece0f9df37d282",
    "content": {
      "message": "Disables NAT Gateways\n\nI forgot how much these damn things cost."
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ONSdigital/eq-terraform/commit/a252266af60f5ab828ad9b81b17bb7b16a2fc52f",
    "content": {
      "message": "Developer environment optimisations\nOptimised developer_defaults.tf to reduce cost of running dev environments\nUpdated default values to match production\nRemoved unused variable\nUpdated readme to target git commit rather and tagged version of alexeymedvedchikov.rabbitmq"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ONSdigital/eq-terraform/commit/79845fe095cd87287346f40d2adce9b28a32ef35",
    "content": {
      "message": "User t3 instances for ECS\n - They are cheaper and faster\n - Removed the max limits on DynamoDB"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ONSdigital/eq-terraform/commit/6eaf697bf9f111214a6d74ee3094e5784a57d1bb",
    "content": {
      "message": "Switch to on-demand billing for DynamoDB"
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/travis-infrastructure/terraform-stuff/commit/1e208af4c83d093c900f4cccedbca6183142a07f",
    "content": {
      "message": "Add \"ubuntu-trusty-micro-playground\" instance\n\nfor easy/forever access to an Ubuntu Trusty machine in GCE, estimated monthly\ncost of ~$4.75"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/sdcote/cloudsql/commit/dfe44fcf8f5a477e1fbc354f1b1d87af28895c0f",
    "content": {
      "message": "Changed default disk type to cheaper HDD instead of SSD"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Wieschie/personal-infra/commit/bfa7b3ae90f1d64d957700976080f9b1921dba30",
    "content": {
      "message": "Disable ssm endpoint (it's an interface and costs money)"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Linaro/qa-reports.linaro.org/commit/76c8d1ee35046912b6da4f1cc23e8b1dcc12abe9",
    "content": {
      "message": "terraform: make staging a T3a instance type\n\nThis is slightly cheaper than regular T3 instances"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Linaro/qa-reports.linaro.org/commit/77116ea84b2e8d088c644723e787ad5d6d9677fd",
    "content": {
      "message": "terraform: s3: enable S3 Glacier backups\n\nThis will replicate any incoming data to S3 bucket into\na backup bucket with \"Glacier Deep Archive\" storage type [1].\n\nThis storage type costs $ 0.00099 / GB / month. It's dirt cheap\nand it's made for cases where rare access is required, like\nonce or twice a year.\n\nOnly drawback of this storage type is the retriving time. It might\ntake up to 12 hours to retrive a file storage in Deep Archive mode.\n\nStoring files in standard S3 storage already has a redundancy, so having\na backup is a fall back of a fall back if data ever gets lost at some\npoint.\n\n[1] https://aws.amazon.com/blogs/aws/new-amazon-s3-storage-class-glacier-deep-archive/"
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/kinvolk-archives/lokomotive-kubernetes/commit/0c4d59db87b67d7c7a0a0f54677961a01ed8fbe4",
    "content": {
      "message": "Use global HTTP/TCP proxy load balancing for Ingress on GCP\n\n* Switch Ingress from regional network load balancers to global\nHTTP/TCP Proxy load balancing\n* Reduce cost by ~$19/month per cluster. Google bills the first 5\nglobal and regional forwarding rules separately. Typhoon clusters now\nuse 3 global and 0 regional forwarding rules.\n* Worker pools no longer include an extraneous load balancer. Remove\nworker module's `ingress_static_ip` output.\n* Add `ingress_static_ipv4` output variable\n* Add `worker_instance_group` output to allow custom global load\nbalancing\n* Deprecate `controllers_ipv4_public` module output\n* Deprecate `ingress_static_ip` module output. Use `ingress_static_ipv4`"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/kinvolk-archives/lokomotive-kubernetes/commit/f2f4deb8bb44988eeb0b64b919e51fb556aef4fb",
    "content": {
      "message": "Change AWS default type from t2.small to t3.small\n\n* T3 is the next generation general purpose burstable\ninstance type. Compared with t2.small, the t3.small is\ncheaper, has 2 vCPU (instead of 1) and provides 5 Gbps\nof pod-to-pod bandwidth (instead of 1 Gbps)"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/dvdhrm/imagebuilder-terraform/commit/9e3285b91b2a95271c3b4d0c8777f5363bfdfd40",
    "content": {
      "message": "Remove external VPC endpoints\n\nInterface-style VPC endpoints are not needed for our external\ninfrastructure and they are causing our AWS bill to increase by a large\namount.\n\nSigned-off-by: Major Hayden <major@redhat.com>"
    },
    "codes": [
      "saving",
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/armand1m/terraform-gke-drone/commit/e5dd322eadc9e0a00fd3a603d5a9e9e07a812df6",
    "content": {
      "message": "feat: make it single node\n\nmake it single node so it gets cheaper"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/reifnir/iac/commit/baced6c85a92e5c39d6270397ab5471262748bf6",
    "content": {
      "message": "double nodes, this is super cheap"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/javier-lopez/terraforms/commit/afed4b83a174335ec2b371061c81bc92938ecda3",
    "content": {
      "message": "cheapest-aws-gpu-spot-instance.* => scripts"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/tdooner/flynn/commit/a9ea9d09727825f9a047e70d94caf73c99e6b2a8",
    "content": {
      "message": "Remove health check of disclosure-backend-static\n\nIt's costing me $2.75/mo. What a gigantic waste of money! /s"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/tomnz/rocketchat-tf/commit/5298263d68548372c064b9d9c76977325a7373c8",
    "content": {
      "message": "Upgrade instance size - costs seem reasonable"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ministryofjustice/cloud-platform-environments/commit/d7afb9018e20ef85f1fbee2300dbf5c5cdf3cb0e",
    "content": {
      "message": "Re-create live-1 MTP resources in AWS eu-west-2\n\u2026excluding prod RDS which is expensive and not yet needed"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ministryofjustice/cloud-platform-environments/commit/5199e517f5e08b8bed3f96334b102814b3e99200",
    "content": {
      "message": "Add namespace tag to replica database\n\nThe replica RDS instance does not have a value for \"namespace\", so its\ncosts are not being allocated correctly."
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ministryofjustice/cloud-platform-environments/commit/cca186a0f7165b361f372c87524011e91b28f41a",
    "content": {
      "message": "Delete the test SQL Server databases\n\nThey're not needed just yet and they're pretty expensive to leave just\nsitting there..."
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/CPWu/terraform_three_instances/commit/faaecdc2e201a146f76f4a94f51e9fd5c5a428f3",
    "content": {
      "message": "terraform destroy to save costs"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/josephakeni/terraform_postgres/commit/1cbe75a2b220921aea6e105ba5a732a1dae9061b",
    "content": {
      "message": "Suspended the NAT Gateway in nat.tf file because of billing"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govwifi-terraform/commit/348b52a2ae5d6d242c8802644f9e3a5e6460de1d",
    "content": {
      "message": "Add Autoscaling to Logging API (#97)\n\nThis adds 2 Cloudwatch alarms, one when the CPU is above 50% for 5\nminutes and another when the CPU is below 10% for 10 minutes.\n\nThese alarms will trigger an autoscaling action, adding or removing\nanother task.\n\nWe want this behaviour to accommodate traffic spikes and to scale down\nafterwards to save costs."
    },
    "codes": [
      "saving",
      "awareness",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govwifi-terraform/commit/38d0a67cf70d46c26675ce60a7a647eef0f11e52",
    "content": {
      "message": "Reduce memory for containers in our API cluster (#101)\n\nCurrently we have just under 2Gb (1900MB) memory reserved for each container.\nThe memory utilisation never goes above 5%.\nReduce this memory reservation by half to 950MB per container.\n\nThis will save on AWS costs as we need less EC2 instances.\nWe could reduce this further but would do that\nas a separate task after having seen this work in production for a\nwhile."
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govwifi-terraform/commit/d75eace6e505fe5645647593ab8a9bb018db64c2",
    "content": {
      "message": "Remove staging GovWifi devices cluster (#163)\n\nWe are able to broadcast GovWifi Devices on the same SSID, so currently\nthere is no need to have a standalone cluster to do this.\n\nIt will also save costs as we don't need to be running these unused EC2\ninstances."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/RagedUnicorn/tf-aws-rg-base/commit/df2bfd188197969796e4e74f038332e723001007",
    "content": {
      "message": "Add new module budget for tracking aws costs"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/kis9a/terraform/commit/f70b95654a30088143a29a5d37a658b8d31540e7",
    "content": {
      "message": "init alarm resources for billing"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/eladidan/speedyhead.xyz-terraform/commit/71f034f3e13e9118a2a954e1fc3c0d35153184f0",
    "content": {
      "message": "add billing budget and cerebro project (#1)\n\nadd billing budget and cerebro project"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/nikkiwritescode/flask-app-terraform-deployment/commit/af47bb6201f1dcc8264e60da429e4ff8d126e56c",
    "content": {
      "message": "Updated gunicorn to run 4 worker threads per instance; assigned public IPs to the instances so that they can update themselves on launch; updated DB to Pay Per Request instead of Provisioned; removed unneeded dependencies in the bash script; and added the EC2 Instance Profile to the second instance."
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/KieniL/terraform_setups/commit/37f66bc43f57b1b7a5a897c58cefe09900afd7ef",
    "content": {
      "message": "made default pool cost effective"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alhardy-net/terraform-core-aws-alhardynet-networking/commit/30be6aa1969e37d512153b558540fe714b635c4c",
    "content": {
      "message": "disable nat gateways in vpc to save cost for now"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alhardy-net/terraform-core-aws-alhardynet-networking/commit/f7b96f0b28008f8aed881cc211a5f3fdb3ae67ac",
    "content": {
      "message": "disable nat gateways in vpc to save cost for now"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alhardy-net/terraform-core-aws-alhardynet-networking/commit/b26b9e5d1b13602a4c192d9697a12df7770906b2",
    "content": {
      "message": "disable nat gateway save cost"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/kperson/terraform-modules/commit/53bd2d84776f9ed7ae287fc59ed42f87bd7bbc4b",
    "content": {
      "message": "allowed for pay per request"
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/digio/terraform-google-gitlab-runner/commit/07f8279fe65a35c0e595f3171f3d75791e49a9ae",
    "content": {
      "message": "change default runner machine type to f1-micro\n\nA previous change set the default runner type to e2-micro in the\nmistaken belief that it was cheaper.  It _is_ cheaper per hour but is\nmore expensive per month due to lack of sustained use discounts."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/jharley/azure-basic-demo/commit/7cd3d202d8723c565704f23c143cae3b1e1d6d2b",
    "content": {
      "message": "OPS-7237 - switch machine type to DS1\n\nCost savings project"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/juliandunn/terraform-chefserver/commit/3971011e4253cb5b98fdf77a558f5ef8f34d2653",
    "content": {
      "message": "Switch to Centos 7 for cheaper costs"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/NedimRenesalis/demblock-pipeline/commit/9edee1a04f6056f367ce5ef678775c1203d6a778",
    "content": {
      "message": "infra: switch to cheaper region"
    },
    "codes": [
      "saving",
      "area"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/pangeo-data/terraform-deploy/commit/f8163bd52bea3774e2f160cff0523c602e65d933",
    "content": {
      "message": "Use smaller notebook instances\n\nCosts less money, given we can't scale to 0 yet."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/pangeo-data/terraform-deploy/commit/7244eed07a1008675f45cc4349bf68141bb29edc",
    "content": {
      "message": "Use less expensive nodes for kubernetes by default\n\nWe are still testing, tearing things up and down - cheaper\nis better."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/fluent-labs/infrastructure/commit/c166c9356246b540be28f56c7240e69e45f5b973",
    "content": {
      "message": "Bring down the digital ocean K8s cluster (#104)\n\nIt's been great digitalocean, but I'd rather use free credits than pay for compute."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/colinjfw/stack/commit/e27f8951d4387d5a2f6e03fd7f5a4f3c6ee53c66",
    "content": {
      "message": "feat(vpc): Optionally use NAT instances instead of NAT gateways\n\nIF NAT instances are used, optional inputs are whether to use Elastic\nIPs, an SSH key name, and the EC2 instance type. The latest Amazon VPC\nNAT AMI is used. The use of NAT instances can be less expensive than NAT\ngateways for development VPCs. NAT instances can be stopped when not in\nuse."
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/sciety/infrastructure/commit/e9c6f0eec2380115182f3a964fd2283622734256",
    "content": {
      "message": "Turn on T2 unlimited (#15)\n\n`unstable` is experiencing high load and ~80% CPU steal which indicates\nCPU credits are consumed, but not replenished by periods of low activity:\n```\ntop - 10:14:50 up 68 days, 20:26,  1 user,  load average: 10.83, 11.57,\n9.00\nTasks: 165 total,   8 running, 122 sleeping,   0 stopped,   2 zombie\n%Cpu(s): 19.0 us,  7.9 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,\n73.1 st\n```\nThis is a [classic problem for T2 instances on\nAWS](https://aws.amazon.com/blogs/aws/new-t2-unlimited-going-beyond-the-burst-with-high-performance/), cheap but throttled often in this way.\n\nThis results in the T2 instance automatically being billed at an higher\nprice to allow full usage of the CPU."
    },
    "codes": [
      "instance",
      "awareness",
      "increase"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/sciety/infrastructure/commit/09732a4fcb6a9bb81c37741ed16261ae43aa6fea",
    "content": {
      "message": "Expand disk size as we are over the 85% mark (#22)\n\n* Expand disk size as we are over the 85% mark\n\n* Switch to cheapest SSD volume\n\nIn an attempt to support elastic expansion of the volume size.\n\n* Revert change to SSD\n\nDoesn't help with volume expansion\n\n* Actually force a magnetic volume rather than not specifying the type"
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/coremaker/terraform-google-nucleus/commit/11234f631f7370dd80ee5fbc5dd7bdbc12dcbf49",
    "content": {
      "message": "add per project billing alerts"
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/rjmco/rjmco-github-tests/commit/ea9c4efeb857958f5a31b75d22555c34e1181937",
    "content": {
      "message": "Move imgs from gcr.io to eu.gcr.io to avoid costs"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Kakaranish/eszop-infra/commit/cd97b1881cb0d1c145b8b6236a3702a970c671c2",
    "content": {
      "message": "\"+ added persistent_resources for \"cheap\" resources creation\""
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/eduardobaitello/terraform-eks/commit/c11fca6440a5000648f690e6282778fb4ec73309",
    "content": {
      "message": "Small volume size to reduce costs per hour"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/maxjahn/terraform-oci-azure-connect/commit/fa109033da045468d612fe8eae1a2f903e30e128",
    "content": {
      "message": "Changed ExpressRoute tier to Local to avoid cost for egress traffic"
    },
    "codes": [
      "saving",
      "billing_mode"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/imma/fogg-env/commit/7de45302cfa8a7dca88ab61b3021091cc480b495",
    "content": {
      "message": "cheap mode by default"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/thiringai-evans/cloud_scripts/commit/5127d7a6eccc6965d1e51928d50ee01a46b38871",
    "content": {
      "message": "provide an instance \n\n A t3.nano instance is provided, its checked for cost estimation monthly using the policies provided."
    },
    "codes": [
      "instance",
      "awareness",
      "policy"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/aeternity/terraform-aws-devnet/commit/f4113a8f7e52991dfb75f63688aeba77bac76b01",
    "content": {
      "message": "Use 3 int-sync nodes on cheaper instance (#21)"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/schubergphilis/terraform-aws-mcaf-matillion/commit/3b0e2fe42d660664c49d54ae8706de004a9b4176",
    "content": {
      "message": "Add opt-in variables\n\nInstead of building a full deployment it feels better to explcitly\nenable certain functionality, a) from an deployment POV and from\nadditional cost awareness.\n\nFollowing are now opt-in:\n\n* RDS instance\n* ALB\n\nSigned-off-by: Stephen Hoekstra <shoekstra@schubergphilis.com>"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/JustinYeoh93/infrastructure/commit/3236aa7d5e0924e4f32b9f4e959f6e69c6dfd278",
    "content": {
      "message": "Removed justin/test secret to reduce cost"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/kody-abe/terraform/commit/169c7768b0b1584945362c035a2b227d2f579466",
    "content": {
      "message": "Switched to use DynamoDB On-Demand billing (#17)"
    },
    "codes": [
      "saving",
      "billing_mode"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/pumbaasdad/aws-account/commit/42c288d84b5f7853c36dcc55d181b8e16d3304a5",
    "content": {
      "message": "Fix billing-alarm"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/eafuna/terraform-sample/commit/4671e08a4500a9b55edcf6e16dc1bab4f52d759a",
    "content": {
      "message": "Updated terratest\n\n- (manual) verified success deployment\n- new approach on golang\n- implemented initandplan only to save cost during testing\n- \ttest assertions on expected values for ec2 and s3 complete\n\nIssues encountered with http_helper\n\ntodo:\n- validate content of website through regex\n- reintroduce initandapply\n- posible packaging of test for better utility"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/smarman85/a_new_hope/commit/de97a6b8033c866c3b711468207aa4890049daaa",
    "content": {
      "message": "use a slightly cheaper instance"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/barloes/projectQuickStart/commit/ca3e05785ef8f3377131e300a00b47fed047051a",
    "content": {
      "message": "added cheapest ec2 poss for testing"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/wellcomecollection/archivematica-infrastructure/commit/ce576be106257496e20d946d6eab5f8783dada92",
    "content": {
      "message": "Use gp3 for our persistent EBS volumes\n\nIt's ~20% cheaper and the 3000 IOPS we previously got as burst are now\navailable as sustained, for free."
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/wellcomecollection/archivematica-infrastructure/commit/4b495fef39e68590aea98af324340adafadfb334",
    "content": {
      "message": "Only run a single RDS instance for Archivematica\n\nWe don't put much load on the database, and each db.r5.large costs ~$170/mo."
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/jxeldotdev/todoapp-infrastructure/commit/72b82b591111e1ceddb71b4017e2a102e7d4f1c3",
    "content": {
      "message": "Added Billing alerts"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/binbashar/le-tf-infra-aws/commit/a873443bd618ac9c14d12210ed4d12a11cc1f733",
    "content": {
      "message": "BBL-33 nat gateway disable in binbash-dev account for cost saving purposes"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/binbashar/le-tf-infra-aws/commit/19c37f7e4e65d14e760f1ff8cf60cfd1e98c1a8b",
    "content": {
      "message": "BBL-33 nat gateway disable in binbash-dev account for cost saving purposes"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/binbashar/le-tf-infra-aws/commit/bbfbd2484ace2819ffceac9155b995ab870ee3a3",
    "content": {
      "message": "BBL-33 nat gateway disable in binbash-dev account for cost saving purposes"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/binbashar/le-tf-infra-aws/commit/2afa0c4652393014778f9b2fefcf975d6ca22ae2",
    "content": {
      "message": "jenkins stack temporary destroyed to save costs"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/binbashar/le-tf-infra-aws/commit/0208ae3bc238f029b1faf6bdc3552dbe6147657b",
    "content": {
      "message": "Rename EKS 'vpc' layer as 'network' and fix an issue with nodes not b\u2026 (#175)\n\n* Rename EKS 'vpc' layer as 'network' and fix an issue with nodes not being able to join the cluster due to NACL rules blocking traffic between private subnets\n\n* Disable NATGW to save costs"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/binbashar/le-tf-infra-aws/commit/c00d24a1626e7d88a33431e4b403651acf158e05",
    "content": {
      "message": "EKS updates (#177)\n\n* Rename EKS 'vpc' layer as 'network' and fix an issue with nodes not being able to join the cluster due to NACL rules blocking traffic between private subnets\n\n* Disable NATGW to save costs\n\n* Update EKS private subnets NACL to allow all traffic betwen them. Also fix the highest port number in another rule"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/binbashar/le-tf-infra-aws/commit/0662f7e1bfc70e21d8229e142b98b6c303addbc3",
    "content": {
      "message": "Change to SNS topic for costs"
    },
    "codes": [
      "saving",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/binbashar/le-tf-infra-aws/commit/89fd3ebb3ba1ccc9a9019ad250b93a336d4bc03e",
    "content": {
      "message": "Adding kms permissions for SNS encrypted topic + std cost-management alerts in proper topic"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/PaloAltoNetworks/terraform-aws-vmseries-modules/commit/1af5d07714182455cafd972556b507409f9559ae",
    "content": {
      "message": "in app1_ec2 change the VM to bitnami ready-made nginx\n\nThe bitnami library offers a ready-made http/https server, which saves\ntime during testing. On the previous ubuntu image, the path to have\nhttps was unneccessarily bumpy:\n- the inbound ssh needed to work\n- the user needed ssh/putty locally\n- apt update\n- apt install ngnix\n- for these commands, the outbound also always needed to work, yet\nanother difficulty.\n\nAll these steps are not needed with a ready-made image.\n\nThe bitnami image is free of cost as well.\n\ncommit-id:b903e3ed"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/renovate-tests/hackstack-local/commit/5e8b6f6b88db49a0508ea670d71070c92cfdbaca",
    "content": {
      "message": "changed variable names and moved es inside vpc to prevent egress cost"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/mads-hartmann/cloud.mads-hartmann.com/commit/667f5715c19534bfe5b01ad692979447412fd033",
    "content": {
      "message": "Make WAF optional. Disable on all sites due to cost"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Ecklebe/IaC-Templates/commit/aeab4d039020c0d63209d7f800f9c34fae4060e6",
    "content": {
      "message": "adapt aks creation, reduce the costs"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/JamesWoolfenden/terraform-azurerm-mariadb/commit/2b30952f26e8a8e66fe8b6eddf9c4657e0d074d4",
    "content": {
      "message": "costs"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/whyn07m3/terraform-examples/commit/7edc348c186c8578be7639958bfcd38fd9150a56",
    "content": {
      "message": "adds cloudwatch billing alarm"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/elliotpryde/personal-infrastructure/commit/772c5ad20818738b09d01cd70ca3de80cbf66dcb",
    "content": {
      "message": "Disable a few of the health checks to reduce costs"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/elliotpryde/personal-infrastructure/commit/7c4205cd130c5463d7f39aa6f281e198c143d0d9",
    "content": {
      "message": "reduce number of health checks to save on costs"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/fpco/terraform-aws-foundation/commit/cfe92035f1b281cddfcf62664ec6b96e85e0ac32",
    "content": {
      "message": "improved persistent-ebs-volumes module, made encryption optional. lowered default instance types for ELK stack to reduce dev/test costs"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/aiidalab/aiidalab-k8s/commit/65e44242c0b5f27d64ff714e5a8eed71ca02ab26",
    "content": {
      "message": "AWS: Update versions and fix issues (#6)\n\n* Update versions and fix issues\n\n* Remove adminuser in config\n\n* Update the \"tested with\" versions in README.\n\n* Update terraform lock file.\n\n* Set default region to Stockholm (should be cheaper than Ireland).\n\n* Adjust autoscaler parameters.\n\n* Update bot policy.\n\n* Replace t3a.medium with t3.medium for the core instance.\n\n* Update autoscaler policy and role name.\n\nCo-authored-by: Simon Adorf <simon.adorf@epfl.ch>"
    },
    "codes": [
      "saving",
      "area"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/robgmills/jumpbox/commit/028bbe114d099b0388be9a46adcab80d9383a518",
    "content": {
      "message": "Add an AWS Budget for this service\n\nKeep costs under close supervision with an AWS Budget and notification.\nThe budgeted amount was based on the [ngrok.io pricing for the _Pro_\ntier](https://ngrok.com/pricing) since that's most comparable in terms\nof final features.\n\nFilters costs based on a customr _Service_ resource tag and sends an\nalert at 80% of forecasted monthly budget."
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/tale-toul/Harbor/commit/804ab045e330d3b669282e55c5bcc4660ebffea8",
    "content": {
      "message": "Delete ebs root volumes on instance termination\n\nNow the root ebs disks attached to the EC2 instances are deleted when\nthe instances are terminated.  Before they were kept as available\nincreasing the billl of AWS."
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/paas-alpha-tsuru-terraform/commit/6085ce34a95fbc4a8d651a3f6a0405811952af93",
    "content": {
      "message": "aws: Remove router-int load balancer\n\nThis was used by Ansible on AWS to configure the Nginx sslproxy machines to\nreverse-proxy to the routers. The hostname was referenced in the variable\n`hipache_host_internal_lb`. We no longer need this since moving Nginx to the\nsame machines as the router.\n\nAlthough we might want to revisit this again later when we test one\napplication talking to another application through the router - which we\nwouldn't want to pay external traffic for."
    },
    "codes": [
      "saving",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/KiaraGrouwstra/sentia-data-engineering-assignment/commit/7e5c997c23683a707aa384b6a07801210b1839b8",
    "content": {
      "message": "monitor costs"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/AdrianNeatu/blog-terraform/commit/3ba302c69eb2a491a5b23e94084b4ddd24a4a703",
    "content": {
      "message": "billing alert"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/AdrianNeatu/blog-terraform/commit/12cf2cb526ed6b2d12106894ca735cf81ba432ff",
    "content": {
      "message": "billing alert"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/jenkins-x/terraform-aws-eks-jx/commit/cce6b14692fccd30c027851607a9526151d4c3d2",
    "content": {
      "message": "using on-demand as default billing for dynamodb\n\nadding info on dynamodb\n\nfixing copy error"
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/CodeForPoznan/Infrastructure/commit/2709680d633653ca9bd68a023981a75427daaa7a",
    "content": {
      "message": "convert bastion to NAT GW, change type to cheapest one avaiable"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Civil-Service-Human-Resources/lpg-terraform-paas/commit/59477d3dc237e72252bde005b783213b7e8ed961",
    "content": {
      "message": "WebApp SKU updates. Now defaults to Basic B1 for all except Prod env\n\nWhy:\n* Costs\n\nThis change addresses the need by:\n* Going to basic app plans for dev/demo/test\n\nChanges made:\n* Updated modules & envs to utilise new webapp SKUs"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/mhemeryck/iac/commit/9ec3047d95969e8d2b729e6545672c3084ea846d",
    "content": {
      "message": "hcloud hetzner setup\n\nThey are much cheaper and you get much more RAM. Multicloud ftw."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/elastic/kibana-buildkite/commit/a0b86b729325ed2bd9ec2d310e2c8b8b2b08f579",
    "content": {
      "message": "Run purge cloud deployments hourly (#17)\n\nThis pipeline currently runs in a minute, once per day.\nRunning this more frequently will save costs by spinning\ndown cloud deployments quicker."
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ibm-cloud-architecture/iks_vpc_lab/commit/629819ce0c440760be155874cb42ab497f0304bd",
    "content": {
      "message": "change cluster flavor\n\nto save cost for long running architecture infra"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/opszero/terraform-aws-kubespot/commit/ac39d3b9140e90bfd4b9eaa01200385bf10cc15d",
    "content": {
      "message": "Egress Only Internet Gatway (#97)\n\nReduce the cost of not running a NAT.\nLimits are you can only access IPv6 Endpoints."
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/opszero/terraform-aws-kubespot/commit/decc9706133af9978ada6cf58a1a63343468e8a0",
    "content": {
      "message": "Simplification and Cost Effectiveness (#100)\n\n* Removed EIP from bastion and vpn\n\n* Renamed some tags\n\n* Added configuration\n\n* Remove the custom bastion\n\n* Move things into public subnet\n\n* Added public ip for public subnet\n\n* Remove node ssh\n\n* Added performace\n\n* Disable\n\n* Added OSSEC to bastion\n\n* Added bastion\n\n* Install foxpass\n\n* added bastion\n\n* Removed Packer Dependency\n\n* Added documentation\n\n* Fix the README"
    },
    "codes": [
      "networking",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/angry-tony/typhoon-k8s/commit/aaa8e0261a01a62fa4a343788b3df20b9e93ee38",
    "content": {
      "message": "Add Google Cloud worker instances to a target pool\n\n* Background: A managed instance group of workers is used in backend\nservices for global load balancing (HTTP/HTTPS Ingress) and output\nfor custom global load balancing use cases\n* Add worker instances to a target pool load balancing TCP/UDP\napplications (NodePort or proxied). Output as `worker_target_pool`\n* Health check for workers with a healthy Ingress controller. Forward\nrules (regional) to target pools don't support different external and\ninternal ports so choosing nodes with Ingress allows proxying as a\nworkaround\n* A target pool is a logical grouping only. It doesn't add costs to\nclusters or worker pools"
    },
    "codes": [
      "cluster",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/devops-ci-cd/weather_cache_infrastructure/commit/3378831be84bb1825c847a001c3c240318d991dc",
    "content": {
      "message": "minimized db server costs"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/devops-ci-cd/weather_cache_infrastructure/commit/28b9313a2bb11c357deeaa256950fb0acf220bc4",
    "content": {
      "message": "minimized db server costs"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/donkeysharp/the-dummy-tfaws/commit/5ea19806f7fcba7727e8111fc5ff4946b9ba72c8",
    "content": {
      "message": "Add module to create low cost vpc with 2 AZs and a nat instance"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Quansight/qhub/commit/265d6c5b8a1e01032402d4b2c94d12d5365e5e6e",
    "content": {
      "message": "Prometheus grafana (#733)\n\n* add grafana traefik route\n\n* grafana working\n\n* prometheus-helm-chart-working\n\n* initial integration - wip\n\n* add external-url variable\n\n* add external-url variable\n\n* add tls var\n\n* add tls var\n\n* add tls var\n\n* cluster monitoring docs\n\n* fix debug change\n\n* fix formatting, delete ingress\n\n* add monitoring by default, fix routing service name\n\n* terraform format\n\n* Update monitoring instructions\n\n* don't include helm chart in repo\n\n* terraform format\n\n* terraform format\n\n* add the values file back\n\n* remove values files\n\n* terraform fmt\n\n* terraform fmt\n\n* up minikube memory\n\n* set CI minikube memory to 6500mb\n\n* move kubernetes tests to new file\n\n* use self-hosted action runner (cirun.io)\n\n* add .cirun.yml\n\n* Misc fixes\n\n- move yaml to root\n- fix syntax\n- install python from miniconda\n- install node\n- install cypress dependencies\n\n* Install cypress after k8s tests\n\n* use cheapest acceptable DO droplet\n\n* add release notes\n\nCo-authored-by: Adam Lewis <balast@users.noreply.github.com>\nCo-authored-by: Adam-D-Lewis <>\nCo-authored-by: Amit Kumar <dtu.amit@gmail.com>"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/davidski/tf-public_subnet/commit/ad841964564c461513643bb13531dc07111efc7d",
    "content": {
      "message": "Move to AZ us-west-2a by default for cheaper spot-instances"
    },
    "codes": [
      "area",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/jshcmpbll/Cloud-Mac-KVM/commit/361885d22c0304cb44683f9b005f82ca5e269841",
    "content": {
      "message": "Changing base image size and type to reduce cost"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/goodpen/gke-v.1.0/commit/45053a0862bf97f0525862c411fa4da5d59ac397",
    "content": {
      "message": "switch to cheaper e2 GCP vms"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/RavenProject/seed-node-fallback-domain-config/commit/5ba235b0c935de75c2cb2f57686bc246816a7f74",
    "content": {
      "message": "Remove two GCP seed nodes\n\nExcessively high bandwidth costs."
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Pavel-Hrabec/Azure-Terraform-Budget/commit/4759aa86a84ee099d6ce3f3987520a5b6cd1ffd3",
    "content": {
      "message": "edited variable and main.tf to immediately alert when significant cost arise"
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/rshurts/gke-cd-with-spinnaker/commit/3bc712aba0c797053b5cdc113e3e46afb6cff8a5",
    "content": {
      "message": "Reduce instance size to keep costs down while experimenting"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/telia-oss/terraform-aws-terraform-init/commit/e8c7b2eb22d08ddd1a1bb375cb6efa4165c9098f",
    "content": {
      "message": "Change DynamoDB billing mode to On-Demand"
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/dextacy10-13/blc-techtask-cicd/commit/1268beb9d3c5d46469ebe130c1d6ace7387f1ff7",
    "content": {
      "message": "Consolidate Dev Servers\n\nTask 1 only requires PHP to do unit testing so could share PHP server with Task 2.  Also shows example of mutable infrastructure.  AWS limits EIP to 5 per region (have requested increase), for now can either consilated resources or use multiple regions.  For cost makes sense to consolidate."
    },
    "codes": [
      "area",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/kmishra9/PL2-AWS-Setup/commit/0d7b5b0f6f92ff6cfde1f17ad96d1b1459a0957a",
    "content": {
      "message": "Updated example instances to use newer, cheaper instance types"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/kaz/kiritan.com/commit/1cd96c7f71e56629ffa07c38e12c4da19fcfc5f7",
    "content": {
      "message": "cost optimization"
    },
    "codes": [
      "saving",
      "instance",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/relaycorp/cloud-gateway/commit/759b1d641a430bf189354e9a4866e72fe56c22dc",
    "content": {
      "message": "Upgrade instances to the expensive, CPU-optimised ones\n\nMake it rain!"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/stSoftwareAU/sts-network/commit/bf59a4c995822ccfdeee64781345c12ebefa967f",
    "content": {
      "message": "reduced costs"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Azure/devops-governance/commit/4894def3589d532789cc8af0c0b197727e0fb3f4",
    "content": {
      "message": "tf: remove Azure Container Registry for cost savings"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/thatscotdatasci/terraform-lambda/commit/6e85cf4f7e7f76c159c2e30952a775d6facaf638",
    "content": {
      "message": "As customer KMS keys cost money, adding option to use S3 default key"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cisagov/cool-sharedservices-nessus/commit/5403a8978053a1299b0afe8d7fc59e914fc5e354",
    "content": {
      "message": "Upgrade the root volume type to gp3\n\ngp3 is cheaper and the baseline configuration offers better\nperformance than gp2 for volumes smaller than 2TB.  It also allows the\nvolume size and IOPS to be configured separately, whereas the two are\ncodependent with gp2."
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/imma/fogg-tf/commit/6089a019df4c39231e596af627ebd910877d11d0",
    "content": {
      "message": "no fargate, cost monies"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/lean-delivery/terraform-module-aws-core/commit/25bbff736936b64a6120ef9608498830ecec33c0",
    "content": {
      "message": "NAT cost optimization (#4)\n\n* Issue \"NAT cost optimization #2\" (#3)\n\nsome improvements:\n* add ability to disable NAT\n* restore ability to setup only one NAT in case if it ec2-based\n* add auto-detecting latest Amazon NAT AMI\n* add test example\n* remove redundant parameters\n* Update README.md"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/dmcgowandmc/minifoundations/commit/832682e0b54ede90b021b4ed575dcc9e761135ce",
    "content": {
      "message": "Finished adding in networking components. To keep costs down, default config has only one AZ"
    },
    "codes": [
      "saving",
      "area"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/dmcgowandmc/minifoundations/commit/941e316bcd70779e868311253f663f8acd5ef155",
    "content": {
      "message": "Removing ssm endpoints due to costs. Enabling dynamodb endpoints"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/trajano/terraform-s3-backend/commit/f4b61c7bedae856439f01499de1ec9050b4c40fc",
    "content": {
      "message": "Added generated example and used PAY_PER_REQUEST for DynamoDB"
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/trajano/terraform-s3-backend/commit/cb9f00a2f6f23b44f7db08863ef5fb0b9ea0bc0c",
    "content": {
      "message": "Added lifecycle rule for the S3 bucket to reduce long term costs"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/verify-infrastructure/commit/d34dcd505074546f77367786f5362891050c3440",
    "content": {
      "message": "Make network consistent in all environments\n\nWe shouldn't have different network topologies, what affects cost is the\nnumber of boxes/apps, the additional subnet/nat gateway is not expensive\n\nNetwork load balancers have weird config which cannot change which\nforces us to have 3 IPs but we cannot have two IPs in the same subnet.\n\nThis makes the network consistent in all environments, and is\nterraformed everywhere"
    },
    "codes": [
      "networking",
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/verify-infrastructure/commit/70a31f9b9a167e5025bb4099cc584044c47e699b",
    "content": {
      "message": "HUB-748 Remove hosted instance of Sentry\n\nNow that we use the cloud version of Sentry we don't need to host our own version.  Sending `staging` events to the cloud version is more cost effective than running an ec2 instance and database etc.\n\nThis should remove anything that is used solely by the hoseted instance.  Variables remain for use by with the cloud instance."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/poseidon/terraform-digitalocean-kubernetes/commit/bacda9d75b329997eb849183cf36ff8f97a9ee83",
    "content": {
      "message": "Change DO worker_type default from s-1vcpu-1gb to s-1vcpu-2gb\n\n* On DigitalOcean, `s-1vcpu-1gb` worker nodes have 1GB of RAM, which\nis too small as a default, even for most cost constrained developers"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cloudtruth-demo/ops/commit/3945c8e2bd7a2b7e73169aceae9dc1817ec8292c",
    "content": {
      "message": "teardown compute infra to save costs till we need it"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/itzik-bd/parking-status/commit/5f9f8fb118a42d7b18ec082be41665ed2b9eb992",
    "content": {
      "message": "optimize costs - remove SQS as it has a lot of \"Number Of Empty Receives\" when application is not in use + introduce lock on \"capture-trigger\" lambda by setting max concurrency to 1"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/dylanmtaylor/dylanmtaylor-terraform-aws/commit/44016d6a8e496b69308a81e88af8c9ef8b710ab3",
    "content": {
      "message": "Change root to 15GB to reduce storage costs"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ministryofjustice/modernisation-platform-environments/commit/ec5ec7bac99558a4e95e95788f846eb618edb861",
    "content": {
      "message": "Remove/Disable the `ppud-data-transfer` EC2 instance.\n\nNow we've imported the current PPUD snapshot, spin everything down so\nthat we're not incurring unnecessary costs."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ckilpatrick20/github-actions/commit/1ab89439a42782f55a2ad591530118dbabd9a447",
    "content": {
      "message": "fix: changing instance class back to t2. t3 was too expensive"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/guillaumekh/wg-terraform-template/commit/effee9cbc473af5d07cfc3aacece50aa6e59753a",
    "content": {
      "message": "use cheaper t4g.nano instance"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/poldi2015/chat-app/commit/cb45bf17da799afaa789206e3fcd39d9403e0567",
    "content": {
      "message": "Changed dynamoDB to pay per use model"
    },
    "codes": [
      "saving",
      "billing_mode"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/jsnider-mtu/tfw-eks/commit/daaceda701c88a0ad7162425f57bd9cabf6d8d9c",
    "content": {
      "message": "That was expensive"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/mdimarino/iac/commit/0cfeda4f4f737523e9e09858c25526db10de70c4",
    "content": {
      "message": "Remove comments from NAT Gateways to avoid costs during test phase."
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/roysjosh/terraform-unifi/commit/da9e2869456610a0228cb14f850c6eccddbb15e0",
    "content": {
      "message": "Decrease root volume to 20G to save costs"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Antmounds/fastly-demo/commit/a433a5d5788a358e7437cb15097d68bd1220eda7",
    "content": {
      "message": "First release 0.2.0\n\n## [0.2.0] - 2020-06-23\n### Added\n- Fastly now has 2 backends to loadbalance between instead of just 1 instance.\n- Instances now use cheapest spot instances by default for demonstration purposes.\n- Changelog with human-friendly updates.\n\n### Changed\n- App name is now interpolated into tag and resource names instead of hardcoded.\n\n### Security\n- Moved subdomain url and ssh key name to .tfvars file"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/JamesWoolfenden/terraform-azurerm-windowsscaleset/commit/d5ac10f438b093d600047b0c9790e71bbbaa0ab2",
    "content": {
      "message": "costing fo razure"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/chqaversion/s3-bucket-versioning/commit/e9dcc211445f9e315d894e61cd147cf5acf151d9",
    "content": {
      "message": "Enable bucket key for reducing the cost of SSE-KMS with Amazon S3 (#115)\n\nCo-authored-by: Krishna Mohan Bandi <krishna.bandi@internetfusion.co.uk>\nCo-authored-by: cloudpossebot <11232728+cloudpossebot@users.noreply.github.com>\nCo-authored-by: Yonatan Koren <me@yonatankoren.com>"
    },
    "codes": [
      "saving",
      "feature",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/stuartsan/lighthouse-lambda-parallel/commit/4214c1c19120650c6888d1c1a8d107755d6c6c2e",
    "content": {
      "message": "initial commit; using kube job approach, but abandoning that.\n\nthe idea of using GKE, having a node pool optimized for running the LH batch jobs, and\nusing kube Jobs to manage a bunch of LH runs in parallel seemed promising. the pool\ncould regularly sit at 0, then autoscale when the LH pods are scheduled, keeping the\ncost low. but alas. the autoscaler is too slow for this to work the way i want.\ntesting with a job that manages 70 pods: the pods go PENDING, and the autoscaler\ntakes ~1m to add one node and have it become Ready; then another ~1m to add another; etc.\n\nit took 5 minutes to run 70 pods this way, and i want to run ~5000, so this is a\nshow-stopper. something like https://github.com/atlassian/escalator seems\npromising for this kind of rapid autoscaling of nodes for this kind of\nbursty workload, but it only works on AWS.\n\nkeeping this commit here in case it is useful to someone or myself later.\n\ngonna use AWS lambda instead for this project!"
    },
    "codes": [
      "awareness",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/openinfrastructure/terraform-google-multinic/commit/7a9c468b88d2edee19007cff6529a20a38eeb363",
    "content": {
      "message": "Add preemptible input var, defaults to false\n\nPreemptible instances are 20% of the cost of normal instances, which are\nwell suited for development and testing."
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/openinfrastructure/terraform-google-multinic/commit/620750695d8f83d79451948d2a43bf920b48c062",
    "content": {
      "message": "Autoscaler\n\n * Setup iperf on endpoints for autoscale testing\n * Configure endpoints in the same zone to reduce zone egress network\n   costs.\n * Switch endpoints to Debian 10 for iperf2 multi-client support\n * Run iperf directly from the startup script, it was misbehaving when\n   run from systemd.\n * Run 180 sequential iperf runs, 10 seconds each to create persistent\n   upward pressure on the autoscaler and TCP window sizes.\n * Add role label with value of multinic-endpoint, iperf-client, or\n   iperf-server to help build charts in the Monitoring console.\n * Set max replicas per zone to 6"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cjbischoff/tf-terraform-management/commit/e357e4e6879f9f6fb09cf28a40684d30d0e63400",
    "content": {
      "message": "[feature/Create-terraform-remote-state-bucket] Completed deployment\n\n- removed backend state; maintained locally\n- removed kms due to cost\n- paramertized my infra deployments"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ssfsx17/fantasy-manager/commit/bded5e3007349a6c47c509336ee9306d234902d6",
    "content": {
      "message": "define SSH keypair for bastions through variable\nwork on private VPC module some more - untested due to cost"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/CheesecakeLabs/django-drf-boilerplate/commit/e4003aa3c51b789e2a2b5828768a7d0f34659209",
    "content": {
      "message": "feat(terraform): remove private subnets and NAT setup\n\nThis is due an unnecessary cost increase due to the NAT pricing"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/richardhughes/infra-modules/commit/48015a86eda461d99b580b69db5922acbe5bd28e",
    "content": {
      "message": "Add billing budget"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/kabisa/terraform-datadog-costs/commit/adaf03ac0d5a324880e9e6aa10d54b9de6b4e635",
    "content": {
      "message": "updated costs alerts to use generic monitor 0.5.1"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/patheard/terraform-cantrill-aws-associate/commit/bc7484cd34698f2724e5d9f241fd9f53d953e3a3",
    "content": {
      "message": "feat(alerts): billing threshold alert\n\nAlso includes Terragrunt project structure setup."
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/covidapihub/terraform-covidapihub/commit/3c5d381a20fbd287f1003271ee1ba64272325074",
    "content": {
      "message": "Create 3 persistent VPCs for dev envs (#31)\n\nTo practice bringing EKS clusters up and down, it helps to have\npersistent networks to host them.\n\nOne of these (dev1) already exists. Here we test setting our nat gateway\nand interent gateway booleans to false on an existing network. We want\nto be able to do this, becuase these are compute resources that cost\nmoney and aren't needed when there are no clusters or EC2s running in\ntheir VPC."
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/midl-dev/tezos-auxiliary-cluster/commit/9cbfebaab11cb3466b160d18ef2eb46c0b875d55",
    "content": {
      "message": "cheaper vms"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/NLnetLabs/rpki-deploy/commit/8bd6e745475f635d6f6b6929a545afa2e1d9dd57",
    "content": {
      "message": "FIX: Default to a smaller Droplet size as when iterating on the scripts the cost can add up fast."
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/mattermost/mattermost-cloud-database-factory/commit/3794d676f4375842364a080e78750b41d9af22d5",
    "content": {
      "message": "Add a cost allocation tag to postgres instances\n\nIf this commit applied the postgres instances will have a tag that is used for cost_allocation\nIssue: MM-36254"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/simplygenius/atmos-recipes/commit/d27b48345a3827b8a10cb5388e42bd5cbea484bb",
    "content": {
      "message": "allow a less redundant (cheaper) NAT setup for the vpc"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/jonbamber/terraform_state_bucket/commit/8c7ca9bbb2c89d622129ace7b10b655851615982",
    "content": {
      "message": "Use AWS-managed KMS key for S3 object encryption\n\nUsing a customer-managed KMS key was a needless expense.\n\nThis change switches to using the AWS-managed key."
    },
    "codes": [
      "saving",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/RootPrivileges/terragrunt-aws-modules/commit/1ee78d8228a83a42a8e099d0821ce6f32ee92dad",
    "content": {
      "message": "Remove (expensive) Glacier transition for logs"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/nhsconnect/prm-mhs-infra/commit/f6c64d1b6ed367803a833da293e84d61573a7fd8",
    "content": {
      "message": "[PRMT-932] Added DNS servers as part of MHS deployment\n\nThis allows to stop using expensive AWS DNS resolver.\n\nWe only need to forward the ncrs.nhs.uk to the HSCN DNS servers. It is\nnot accomplished by having 2 EC2 instances running an unbound DNS."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/wellcomecollection/buildkite-infrastructure/commit/300ed87cba6912103723b03d62c9b0bf93df36ea",
    "content": {
      "message": "Snapshot bucket (#4132)\n\n* Adds an S3 bucket for Elastic Cloud snapshots\n\nWe currently rely on Elastic Cloud to hold our snapshots for the catalogue API ES cluster.\n\nIn order to make working with Catalogue API ES data easy to do locally and to do outside of Elastic Cloud for reduced time and expense, it is useful and more arguably more reliable to hold these in an S3 bucket we have direct access to.\n\nTo complete this change we will need to make some changes to the Elastic Cloud configuration.\n\nSee: https://www.elastic.co/guide/en/cloud/current/ec-aws-custom-repository.html\n\n* add some documentation for ES snapshot storage\n\n* moved to critical stack\n\n* principals needs to be a list"
    },
    "codes": [
      "saving",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/TimonB/tf-azure-example/commit/ce89df3cebc6487146391afe9517661053229f77",
    "content": {
      "message": "optimize costs"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/TimonB/tf-azure-example/commit/b49579fbecbe8002932fdfb86146f83efd60bfcf",
    "content": {
      "message": "optimize costs"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ywarezk/academeez-iac/commit/3a96bab387bf898fd71023961e0cd957488e357b",
    "content": {
      "message": "decided to create dev cluster as preemptable and in warsaw to make things super cheap"
    },
    "codes": [
      "saving",
      "area"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ywarezk/academeez-iac/commit/f1383adda80c8d0960fa8153c2a82f03ce81e874",
    "content": {
      "message": "created a super cheap cluster"
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/taldanzig/nginx-multistream/commit/94e101feda4af9eef9980dc024c9d1a95afe3e6f",
    "content": {
      "message": "easily enable / disable (to save costs)"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/JamesWoolfenden/terraform-gcp-instance/commit/e88177b8ea5d4ef9f53f70ade27232de7a360800",
    "content": {
      "message": "costs"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ninthnails/terraform-aws-camellia/commit/0019704e14723aaf326840ab36c594c3f514a2d4",
    "content": {
      "message": "Specs and optional resources for cost optimization during development."
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ninthnails/terraform-aws-camellia/commit/9438dcebde79c0726d69ae6a15c99b9c5c0f461c",
    "content": {
      "message": "Stand alone cluster support, cost optimization for development"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/maddevsio/aws-eks-base/commit/c528b5e97d9d1b57087bd60644a885dd9b4a2294",
    "content": {
      "message": "added vpc_s3_endpoint to reduce traffic costs"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alghanmi/terraform-modules/commit/f4e8069ff11b7ca7a15ce25843b26d00fb399ade",
    "content": {
      "message": "Support bill alerts and console user"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alghanmi/terraform-modules/commit/570d3d8440ed399ed8b30bffe1fd7a2adc197771",
    "content": {
      "message": "Remove logging due to cost"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/andybaran/GCP-IOT-BigDataInfra/commit/cacd8afbc18273e42e4027b36b3a5e9c37c04405",
    "content": {
      "message": "removing dataflow to keep cost down while testing"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/rackspace-infrastructure-automation/aws-terraform-ec2_autorecovery/commit/137c1c3085bb95b74e2d5a695c3343ca25830fd3",
    "content": {
      "message": "Fix error when creating 0 instances\n\nWhy would anyone want to create zero instances?\n\nSuppose there are templates being re-used for multiple environments, and in\na development or test environment, multiple major application roles/functions\n(let's call them \"foo\" and \"bar\") are handled by a single instance to save on\ncosts. In production, however, each major application role lives on its own\ninstance(s) for better resiliency.\n\nSince Terraform doesn't support conditional invocation of modules,\nspecifying an `instance_count` of `0` for \"foo\" and \"bar\" in dev/test\nand an `instance_count` of `1` for \"foobar\", while specifying an\n`instance_count` of `1` for \"foo\" and \"bar\", respectively, and `0` for\n\"foobar\", would permit reuse across environments and avoid a lot of\ncopy-paste that may drift.\n\n**Note:** The SSM document association portion was the only part of this\ntemplate that was currently producing errors, so it's the only one I\naddressed. This will technically still create all of the other base\nresources like roles, iam policies, etc. We can tie those to\n`instance_count` as well, if desired."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/lacework/terraform-gcp-audit-log/commit/324c8a499794f6a789d583c4c460af9475171215",
    "content": {
      "message": "feat: add folder exclusions (#49)\n\n**Breaking Changes**\n\n* Update Google provider version to 4.4.0 minimum\n* Change k8s_filter to true by default - This will result in cost savings for our customers since we don't currently use the GKE logs.  When k8s audit logging reaches GKE, we can re-evaluate the default setting.\n* Change ubla to true by default - This will put our storage bucket in compliance with CIS Benchmark 5.2\n* Remove the bucket logging configuration options - This check was removed from the CIS benchmarks in 1.1 since the preferred method is to use Cloud Audit Logging.  Our CIS 1.2 report reflects this change, and we should be removing the 1.0 reports from the system \"soon\""
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/hazelops/terraform-aws-ecs-datadog-agent/commit/a10df203e8892e1a868bd88d798589719a582ba6",
    "content": {
      "message": "Use public.ecr.aws registry for the default datadog-agent image\n\nUsing public.erc.aws registry provides a better integration on the\nAWS ECS environment:\n* reduce network cost\n* better availability"
    },
    "codes": [
      "saving",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/trivbuit/Scalr-Demo/commit/773446c8f3192a447b8956f71b1ac90e7a9b8afa",
    "content": {
      "message": "Update cost"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/thoughtbot/flightdeck/commit/c784bc0a3f747b66ab7cd01f23bbbdbad3bfe705",
    "content": {
      "message": "Disable annotations/labels in logs by default\n\nFluent Bit can add the pod annotations and labels to log output which\nmakes it possible to find and aggregate logs by labels. While this could\nbe useful, it creates extremely large log entries, which can be\nexpensive in Cloudwatch.\n\nThis removes most of the Kubernetes metadata from the log output. It\nleaves namespace, container name, and pod name, which makes it possible\nto filter by a particular application or process."
    },
    "codes": [
      "awareness",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/00inboxtest/terraform-google-vault/commit/1d0b5db7f310dc6a47af3130a97e5373d9cdaddf",
    "content": {
      "message": "feat: enable auto-healing, update to Debian 10 (#119)\n\nThis patch adds an auto-healing policy to automatically re-create the\nvault cluster instance if the vault server stops.  One of the nodes in\nthe instance group is active as per [Vault HA][ha].  The other nodes are\npassive and forward requests to the active node.  Two different health\nchecks are used because passive nodes return non-200 status codes by\ndefault.\n\nIn addition, this patch:\n\n * Update Vault to 1.6.0 by default\n * Update image to Debian 10 by default\n * Defaults to e2-standard-2 instance types, which are less expensive\n   and more performant than n1-standard-1.\n * Improves startup (and auto-heal recovery) time by starting the vault\n   service as quickly as possible in the startup-script.\n\n[ha]: https://www.vaultproject.io/docs/concepts/ha.html"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/hellupline/terraform-eks-cluster/commit/2bd01358b3a30d1680074f9bbd120da3a1456450",
    "content": {
      "message": "Switch to a public-only setup\n\nAside from being cheaper to operate due to not having a NAT gateway, \nthis should help understand the benefits and limitations of calico."
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/openaustralia/infrastructure/commit/63ee190c0ae1832bb72681e1e4b1b14a9367b4bb",
    "content": {
      "message": "Move to more modern EBS storage - better performance, lower cost"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Amberoat/didactic-octo-eureka/commit/494706fc421a0ddda47f7d543b7e7a296c378c26",
    "content": {
      "message": "use b2s for cheaper cost"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/phillhocking/aws-ubuntu-irssi/commit/1532e0c298ec4f8d7d749a884f7c46f2a2cf53d3",
    "content": {
      "message": "Cheap: single core/standard volume type instead of EBS"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cds-snc/cloud-based-sensor/commit/ccd141993815d0b1f444d12bbd2bdcdd9548e20e",
    "content": {
      "message": "feat: set satellite bucket object expire to 14 days (#118)\n\nUpdate the S3 module version so that it is possible to control if\ndelete markers are replicated to the destination bucket.\n\nBy doing this, and not enabling delete marker replication, it means\na shorter object expiration of 14 days can be set on the satellite\nbucket objects which will save storage costs.\n\nThe central log archive bucket will keep its 90 day object expire\nlifecycle rule so we'll still have the logs if needed."
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cds-snc/cloud-based-sensor/commit/10bb572d477197bd3874532bfd364de1cb496d05",
    "content": {
      "message": "feat: CloudWatch alarm for billing changes (#126)\n\nAdd a CloudWatch alarm, SNS topic and Slack notification Lambda\nfor estimated billing changes that are greater than a percentage\nchange threshold in a six hour period.\n\nNote that billing alerts must be enabled on the payer account\nfor the `EstimatedCharges` metric to be available in CloudWatch."
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cds-snc/cloud-based-sensor/commit/ad9ba03731ec0520164bb23a90968f2100dca517",
    "content": {
      "message": "Update billing alarm (#143)\n\n* Update billing alarm\n\nChanging the period to 12 hours because according to AWS:\n\n>The ML algorithm we use currently works well with dense data that exhibit seasonality and trends. In this case, since this metric is expected to have only 1 datapoint every 12 hours, this feature may not work well for this metric...(metric being 'billing' in this case)\n\nSo in light of this increasing period to 12 hours, and number of evalution periods to compare to alarm points.\n\n* Revert comparison operator"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/sa-proj/proj-azure/commit/baa5b2d85892a85403c5dac807c891bb1b7b6dc7",
    "content": {
      "message": "sql license - pay as you go"
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/emeloibmco/Schematics-Classic-Infrastructure-BareMetal/commit/2d2b279e98ba4b58b7931fbf7f7869d5d38ac305",
    "content": {
      "message": "Hourly Billinng Template"
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/robertdebock/terraform-aws-vault/commit/e3b6520960a88aacbf03339dc1368f680a8bee9a",
    "content": {
      "message": "Development cheaper, deduplicate, fix and sort ami_patterns."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/robertdebock/terraform-aws-vault/commit/757edca9d6fb2231ebdcf03ec611183c59eaf39b",
    "content": {
      "message": "Cheaper bastion host and cooldown configurable."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/e91e63/terraform-tekton-pipelines/commit/432683fcf7921a699f1557e89818b3bef5dba61a",
    "content": {
      "message": "Removing cache due to costs"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/weiztech/gcloud-python-cost-control/commit/43b3c46b84b984800a6a013a93555b7bedf5c81a",
    "content": {
      "message": "add cost control code + terraform deployment code to GCP"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/mtheoryx/graph-ql-cloud-demo/commit/83f393acfdc39810b29f898ff2d8a3f247571aa6",
    "content": {
      "message": "Get bastion working, save cost snapshot"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/JamesWoolfenden/terraform-aws-cognito/commit/e8004aa1293b04a63fe33895e81ed2b9e6d3af94",
    "content": {
      "message": "costs"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/tesera/terraform-modules/commit/3cd4d7b55ac2003153fd0670151ab395ae182431",
    "content": {
      "message": "feat: update the Dynamo table to be pay per request."
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Nercury/infra/commit/4fe56a9426257d6c1a3d4288732b6e256b6b9d35",
    "content": {
      "message": "Tags and less security to make it cheaper"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/tsub/ecs-sandbox/commit/8501fafbba186919c9e9b55a6a3fc72b4fb80909",
    "content": {
      "message": "chore: Cost cutting in NAT Gateway"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/boffbowsh/dc-terraform-provisioning/commit/7e1b0531992b592980c232994b7248801073fcd0",
    "content": {
      "message": "Configure VPC\n\nUses Ash\u2019s NAT instance module for a t2.nano instead of the expense of\nan AWS NAT Gateway."
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/MichaelDeCorte/TerraForm/commit/3799ee8b9677d02254eb6d6f50f3732df4c8374e",
    "content": {
      "message": "dynamodb support for pay per request"
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/dharapvj/workflow-testing/commit/24fa52712f844e2b459329e28af96bc369024d25",
    "content": {
      "message": "Minor tweaks to use single CP machine and spot instances to reduce costs. Also added role creation details for external-dns"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ausmartway/gcp-playground/commit/4676fee3828f732d296feb81be51f9602610f537",
    "content": {
      "message": "reduce cost"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ken5scal/secure-brigade-terraform/commit/f751747d53b467f72498c50deef79828c2b378be",
    "content": {
      "message": "remove security feature from tokyo region for cost purpose"
    },
    "codes": [
      "saving",
      "area"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/schubergphilis/terraform-aws-mcaf-vpc/commit/6ca41e5ad697201a1d225e5d15134e547ee6ced3",
    "content": {
      "message": "Add variable for creation of NAT Gateway(s)\n\nAdd option to not create NAT Gateways for private subnets to save costs."
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/rmaheshvarma/terraform/commit/c866a7dd2575dd2a3f4af83f5f081a5004d0e478",
    "content": {
      "message": "basic aws billing module"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/covid-videoplattform/covid-videoplattform/commit/83d8b928ecb3f271a058bb30eaa1e05ce10e0434",
    "content": {
      "message": "reduce costs of infrastructure"
    },
    "codes": [
      "saving",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/marceloboeira/certifications/commit/da24f25924b6584137d7315e6eea61a31be86406",
    "content": {
      "message": "Add CloudFront Section + Lab with Distribuction\n\nIt is disabled because of the costs, but it is easy to enable it"
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/marceloboeira/certifications/commit/985612e94b1ba7d6a881c6b2fe074449f47dd45d",
    "content": {
      "message": "Remove database and cache to avoid costs"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/marceloboeira/certifications/commit/48f4dbd21e8912511bf5b7441a2a9e207f025d8e",
    "content": {
      "message": "Cost saving deletes\n\n- Once the course is finished I might just re-enable it all and delete\n  the state"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/davidcallen/parkrunpointsleague/commit/21627e4057b3446b511e4369ca366e297cfc87eb",
    "content": {
      "message": "Add support for backbone account to be used standalone (no need for costly TGW)."
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ministryofjustice/network-access-control-infrastructure/commit/95c70a0a304c55adacc3ba0605dd2b9aa2730f6f",
    "content": {
      "message": "Expire old versions of ECR images to save on costs (#75)\n\nThis will remove an image after 14 days. Images of any version can be\nbuilt from version control."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/bhfsystem/fogg/commit/81e606a72e7c2e06c2f6d9c204086157aa82eac3",
    "content": {
      "message": "use the cheapest ebs disk sc1"
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/bhfsystem/fogg/commit/7cc487f270d553f819fea0cf96e872c374979305",
    "content": {
      "message": "use cheap ebs for root"
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cisagov/cool-assessment-terraform/commit/3138943ab4d15cc256d322e1128862ef11383c73",
    "content": {
      "message": "Upgrade all volume types to gp3\n\ngp3 is 20% cheaper, and the baseline configuration offers better\nperformance than gp2 for volumes smaller than 2TB.  It also allows the\nvolume size and IOPS to be configured separately, whereas the two are\nintertwined with gp2."
    },
    "codes": [
      "saving",
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/sawyers/ec2-dev-env/commit/f0bbb7e656d0f9785f1cc7cead6b420e1862f4df",
    "content": {
      "message": "Allow easy deploy/tear down of dev agent\n\nTo avoid costs of long lived instances I made this to let me apply / destroy\nagents as needed along with updating my ssh config\n\nThis should let me have a custom on demand box as needed."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Young-ook/terraform-aws-sagemaker/commit/df4c2102d59c5cae88c9ed30489b2d00bc75aafd",
    "content": {
      "message": "fix(s3): intelligent-tiering archive opt-out option (#34)\n\n* fix(s3): intelligent-tiering archive opt-out option\n\n* docs(s3): intelligent-tiering archive blog\n\n* feat(s3): random petname for s3 bucket name\n\n* docs(s3): lifecycle rule screenshot\n\n* docs(s3): storage cost optimization whitepaper"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-infrastructure/commit/8475fbe51281a76757f7046c6c591056db182286",
    "content": {
      "message": "Enable envoy access logs for all apps\n\nAs described in the AWS docs on AppMesh \"Observability\":\n\nhttps://docs.aws.amazon.com/app-mesh/latest/userguide/observability.html\n\n> When you send Envoy access logs to /dev/stdout, they are mixed in with\n> the Envoy container logs. You can export them to a log storage and\n> processing service like CloudWatch Logs using standard Docker log\n> drivers such as awslogs. For more information, see Using the awslogs Log\n> Driver in the Amazon ECS Developer Guide. To export only the Envoy\n> access logs (and ignore the other Envoy container logs), you can set the\n> ENVOY_LOG_LEVEL to off. For more information, see Access logging in the\n> Envoy documentation.\n\nIt's possible that this approach would get expensive at\nproduction-levels of traffic. I think it's worth doing for now though,\nas it might help use troubleshoot issues while we're developing."
    },
    "codes": [
      "awareness",
      "increase",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-infrastructure/commit/1e588a004172338404c487e679df5d503f0cab48",
    "content": {
      "message": "Reduce external-dns's polling interval and enable watches.\n\nThe idea here is to reduce the QPS to Route53, since Route53 has a hard\nlimit of 5 QPS. The watches should be comparatively cheap as long as we\ndon't create an unreasonable number of Ingresses+Services (and we'd\nalmost certainly hit other scaling limits first anyway).\n\nSee https://github.com/kubernetes-sigs/external-dns/blob/1b57fd7/docs/tutorials/aws.md#throttling\nand https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/DNSLimitations.html#limits-api-requests-route-53"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-infrastructure/commit/a51a3bfcd73fd55ecd43aa36ce3f266f0cefc2dc",
    "content": {
      "message": "Give the EKS environment its own frontend memcache.\n\nWe can't share the Frontend memcache with the EC2 environments, because\nFrontend uses it to cache rendered HTML which contains links to Rails\nassets. My mistake \ud83e\udd26\n\nAlso:\n\n- Tweak the config for the shared Redis while we're there; it's very\n  similar to the memcache one so let's make the two consistent.\n- Remove the defaults for instance types and added those vars to the\n  per-environment `.tfvars`, to make it less likely that we'll forget to\n  set them appropriately when adding staging and prod.\n- Hardcode the standard ports for Redis and memcached; we'd never want\n  to run them on nonstandard ports so on balance it's cleaner to get rid\n  of the module variable and save on interface clutter.\n- Use a slightly smaller and cheaper instance type for Redis.\n- Use provider default tags to simplify tagging the Redis cluster and\n  associated resources. Saves the repeated boilerplate merge on every\n  tagged resource.\n\nTested: applied successfully in the test account. Clusters/instances\nshow up healthy in the AWS web console."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/alphagov/govuk-infrastructure/commit/ee2b2286a02d49c7e9992d0a39ce950583e16bb7",
    "content": {
      "message": "Add k8s resources to Argo Workflows components\n\nThere are 3 types of Argo Workflows components: controller, server\nand executor where k8s resources can be set. They seem to have\ntheir default values removed from the [Helm chart](https://github.com/argoproj/argo-helm/blob/master/charts/argo-workflows/values.yaml)\n\nThe values are added back and were obtained from [here](https://argoproj.github.io/argo-workflows/cost-optimisation/)\n\nRef:\n1. [trello card](https://trello.com/c/54xe0QEk/892-add-resource-limits-to-workflow-and-job-pods)"
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/companieshouse/ceu-terraform/commit/8733c7f4d1b395d6a8e236d0106cfae56d4ded06",
    "content": {
      "message": "Removing SNS and alarm actions from Development account as unnecessary cost"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/gedejong/infra/commit/df7d6a56fb9c390574c45500fbae61dbba0044d8",
    "content": {
      "message": "Don't enforce guardduty (too expensive)"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/v3nd3774/AWSEMRInferencePipelinePOC/commit/75771876ca81699edfb4bb1afa202d6426abae1e",
    "content": {
      "message": "got terraform to start planning emr cluster, need to analyze expected cost and actually turn on and log in to notebook on cluster now"
    },
    "codes": [
      "cluster",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cloud-geek/AWS-Cloud-Architect-master/commit/a662717d80209735e93b6b8e537f21b53c06b469",
    "content": {
      "message": "Added project 2 files\n\nDesign, Provision and Monitor AWS Infrastructure at Scale\nIn this project, you will plan, design, provision, and monitor infrastructure in AWS using industry-standard and open source tools. You will practice the skills you have learned throughout the course to optimize infrastructure for cost and performance. You will also use Terraform to provision and configure AWS services in a global configuration."
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/AndreasSko/infrastructure/commit/e7c56ae03560224a4d40370ce812f05662cee20b",
    "content": {
      "message": "Increase size of server to CPX11\n\nWe are low on storage for some situation. So instead\nof adding 20GB, we can just increase the size, pay\na few cents more a month and benefit from another\nvCPU :thumbsup:"
    },
    "codes": [
      "awareness",
      "increase",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cliffano/feedpaper/commit/d13218e98814d7ac3bc65fc862c0119123f44d68",
    "content": {
      "message": "Use one dynamodb table to reduce cost."
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/HarsheshShah08/HS-Terraform/commit/e0d0f044c54ebf491c122664d03e0cfe5d2b0823",
    "content": {
      "message": "Public networked created and private network has been masked to save the cost"
    },
    "codes": [
      "domain",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/akerl/aws-account/commit/91967d4089ad9580ceae62f7845581935c892455",
    "content": {
      "message": "add billing alarm"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/akerl/aws-account/commit/9fcff4b82a2ad4d0ed24656243da51f767f2968a",
    "content": {
      "message": "bump billing alarm timer"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/zbmowrey/cloud-admin/commit/738d187f603e5a52e6c6338abcd6096e22e819a0",
    "content": {
      "message": "Initial Commit\n\nSets up the infrastructure for Organizations, OUs, SCPs, and cost & usage reporting."
    },
    "codes": [
      "awareness",
      "policy"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/poseidon/typhoon/commit/85252dec6ed7f0b24517089150ec89bf32b45fb2",
    "content": {
      "message": "Switch FCOS workers to official Fedora CoreOS AMIs\n\n* Fix worker nodes to use official Fedora CoreOS AMIs,\ninstead of the older Poseidon built AMIs (now removed).\nThis should have been part of #1038, but was missed in\ncode review\n* Poseidon build AMIs have been deleted (so I don't have\nto keep paying to host them for people)"
    },
    "codes": [
      "saving",
      "feature",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/geoschem/gc-cloud-infrastructure/commit/3ab33196d8b4063c33a261b3f637b1fe3ae8b4df",
    "content": {
      "message": "Improve error reporting and modularize common functions (#7)\n\n* added alarm for billing costs\n\n* added config variable to specify whether to use spot or on demand queue for benchmark step function\n\n* added optional config variable to skip run dir creation and to set time period (only for GCC currently)\n\n* fixed bug where OutputDir was being deleted before simulation run\n\n* add 1Day option for GCHP\n\n* added trap for non zero exit codes\n\n* added get-repo.sh utility to benchmarking\n\n* create set-config.sh setting for updating simulation settings\n\n* created get-input-data.sh to download the data from s3\n\n* small updates\n\n* improved error reporting\n\n* added service user for github"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/geoschem/gc-cloud-infrastructure/commit/ca751ba8410d8d2a0eb3579012ae319c95e3aef6",
    "content": {
      "message": "Additional improvements to dockerfiles and terraform scripts (#8)\n\n* added alarm for billing costs\n\n* added config variable to specify whether to use spot or on demand queue for benchmark step function\n\n* added optional config variable to skip run dir creation and to set time period (only for GCC currently)\n\n* fixed bug where OutputDir was being deleted before simulation run\n\n* add 1Day option for GCHP\n\n* added trap for non zero exit codes\n\n* added get-repo.sh utility to benchmarking\n\n* create set-config.sh setting for updating simulation settings\n\n* created get-input-data.sh to download the data from s3\n\n* small updates\n\n* improved error reporting\n\n* added service user for github\n\n* added email notifications and improved error handling for benchmarks\n\n* updated spot instance allocation strategy to use capacity instead of price\n\n* added gettext dependency in dockerfile"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/rremer/terraform-minecraft/commit/bac050d515cd6991e17cd1a34197d84982f5931d",
    "content": {
      "message": "Gcp (#1)\n\n* switch to google cloud (cheaper and more performant)\n* break aptdaemon dep out of compute provisioner into its own module\n* rm ntp module, since minecraft server complains about time changes and does not require perfect time\n* setup acls for ssh from generated public nat ip of provisioner\n* assert/generate public rsa key from private on provisioning\n* deploy with persistent volume"
    },
    "codes": [
      "saving",
      "provider"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/grimoire-ca/root.tf/commit/7dc51b5edd98b9f4ada7a745ca56ba6dc3799662",
    "content": {
      "message": "Start expiring old state versions.\n\nIt's a small cost saving, but it adds up."
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/miiingle/infrastructure/commit/3210c2185c0a554fa8b5aed45e7daa2c861954ce",
    "content": {
      "message": "oops, too expensive"
    },
    "codes": [
      "instance",
      "awareness",
      "increase"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/chapas/tf-az-kubernetes/commit/bcc6e190b8f8a12b590089fb755c4f552f179ad0",
    "content": {
      "message": "updated template to help reduce cost with default settings"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/nagpach/terraform-example-aws-vpc/commit/35d26fd046185ae079e09fa6435c41ae685e679e",
    "content": {
      "message": "Disable NAT gateways for now.  We don't need them and they cost $$$"
    },
    "codes": [
      "networking",
      "saving",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/pelias/terraform-elasticsearch/commit/21c1827f4507eae217d43d99ad8cb1bbb1337e21",
    "content": {
      "message": "Use r4.xlarge instead of r3.xlarge\n\nCheaper and with much faster network access!"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/pelias/terraform-elasticsearch/commit/8454c8ee25e821abde10b73a2fec691269e41822",
    "content": {
      "message": "Update default variable values for cluster sizing\n\nIt's gotten cheaper and easier to run a full planet cluster!"
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ironpeakservices/infrastructure/commit/2ca24fa9114b5b4389768d5ab93c1e6d99bb287c",
    "content": {
      "message": "feat(scaleway): switch to cheaper DEV1-M instance"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/scirner22/provenance-infra/commit/599aa2776677bcf05ebf5f6377a2eb82f5d2220c",
    "content": {
      "message": "Scale back nodes to cut costs."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/appbricks/cloud-inceptor/commit/782a0a3c30cf83bcaeacc942789ccc903576fe8a",
    "content": {
      "message": "allow configuring node without eip or fixed domains allowing for a minimal low-cost setup"
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/cisagov/freeipa-server-tf-module/commit/99fd319a72d25441acf36fd2c167a875e9028935",
    "content": {
      "message": "Upgrade the root volume type to gp3\n\ngp3 is 20% cheaper, and the baseline configuration offers better\nperformance than gp2 for volumes smaller than 2TB.  It also allows the\nvolume size and IOPS to be configured separately, whereas the two are\nintertwined with gp2."
    },
    "codes": [
      "saving",
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/firehawkvfx/firehawk-prototype-deprecated/commit/894fb1d80c7a3953b7a51d7acd5e9b942faced8f",
    "content": {
      "message": "added ability to sleep nat gateway to save cost when idle"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/pxc-smart-business/savannah/commit/54b40eb1ae8763b13ecde14a2e04779342208cef",
    "content": {
      "message": "use cheaper instances for permament peer"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/Xin00163/terraform/commit/f69ce3812180a20bbda69ff1432a1cd36342bc3b",
    "content": {
      "message": "Removed ALB for cost and time taken to build/destroy"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/bortolo/Three-tier-app-infrastructure/commit/316179eeaaba01499ac86fa7027f004708daf9ee",
    "content": {
      "message": "New custom policies for billing and EC2"
    },
    "codes": [
      "awareness",
      "policy"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/lowflying/OVPN---TF/commit/be1245d8634025277ba79a4155ee88d7eaffcdfb",
    "content": {
      "message": "- Updated Ubuntu to 16.04\n- Instance type is not t3.nano (cheaper)\n- terraform fmt\n- Modifications to userdata.sh to adhere to new Ubuntu version, DNS\nchanges, and handling of apt upgrade without CLI."
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/jeffawang/infrastructure/commit/9f610811aea8c523332e9dccad9bb0800b70691e",
    "content": {
      "message": "remove nat gateway (too damn expensive)"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/tomoconnor/cheap-ass-vpn/commit/659dd8a34701684f7c574ce35bc37ad9f70d56e1",
    "content": {
      "message": "Cheap-Ass VPN initial"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/TalkingFox/SignalWs/commit/935d9d683608b4d8a97ef6ccc2c8ab7c14eec0d0",
    "content": {
      "message": "reduce read/write capacity to 1 for min cost.\nChange list to map."
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/EtchUK/Etch.OrchardCore.SiteBoilerplate/commit/cc3460b79e8811df25a34a10c34c2a0ad2b6280f",
    "content": {
      "message": "Update terraform script DB edition\n\nUpdate the terraform script to set the DB edition to 'Standard' and the\nobjective name to 'S0' to prevent expensive databases being created by\ndefault. Projects can scale this up as required by changing their script\nmanually."
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/robertdebock/git-terraform-demo/commit/5638b1a044215292a5e3fa405b6a0567c6b35436",
    "content": {
      "message": "Medium is expensive and not required."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/robertdebock/git-terraform-demo/commit/686374095321975d851932a77b139d627f50c7d5",
    "content": {
      "message": "Cheaper is better."
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/JacobTLeBlanc/PortfolioTF/commit/68c7bfb39a40f2783cfdae9ab5206c6747b8e7f3",
    "content": {
      "message": "Removed route53 for costs reasons"
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/grahamgilbert/terraform-repo/commit/08ff1d9788127d4fb9aa6616e22ed0b4ed406106",
    "content": {
      "message": "Man this is expensive"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/stephengrier/my-infra/commit/e5742d6f4f93dd432c9d8d0a31493d43c45aaff1",
    "content": {
      "message": "Add NAT instances to ecs module\n\nThe ecs module creates NAT gateways for the ECS hosts to use for\negressing out to the internet. However, NAT gateways are rediculously\nexpensive. The solution is to use NAT instances instead, which are EC2\ninstances configured to act as NAT boxes. Because they are just EC2\ninstances we can use spot pricing to massively reduce the cost.\n\nTo be resilient we need a NAT instance in each availability zone. To\nensure this is the case we need an ASG in each AZ which maintains\nexactly one spot instance. We also create an ENI for each NAT instance,\nwhich will persist as spot instances come and go and provide a static ID\nwhich can be used in routing tables."
    },
    "codes": [
      "networking",
      "saving",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/ministryofjustice/cloud-platform-terraform-monitoring/commit/87401ba23af26d379d8132cc09fd7cd212773ba1",
    "content": {
      "message": "cheaper storage for test clusters (#114)\n\n* cheaper storage for test clusters"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/sasidharkollimarla/terraform/commit/87d2fc1f203860095dc632c536d1820176941419",
    "content": {
      "message": "Shoul;d execute cost capping policy"
    },
    "codes": [
      "saving",
      "policy"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/maxjahn/terraform-oci-azure-adb/commit/e2295fe41d430f42795605b6356e69b393625999",
    "content": {
      "message": "Changed ExpressRoute tier to Local to avoid cost for egress traffic"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/codequest-eu/terraform-modules/commit/ffe23d4c2cd78035bef0dfb261701e7ed8dd588d",
    "content": {
      "message": "fix(meta): unnecessary lock table capacity, which bumped the cost a lot"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/dgorbov/terraform-s3-backend-setup/commit/81f82740760a357a86b3a77f9ed400624edcb218",
    "content": {
      "message": "DynamoDB costs optimisation"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/tresvitae/tf_eks_simpleweb/commit/200e62726a588acf05f58a1087d71eb1b98f4e0a",
    "content": {
      "message": "Decrease costs"
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/smerck/blog/commit/3e543412b6680057ffb10b0fbba777ccb3205882",
    "content": {
      "message": "feat: adding terraform configurations for blog (#8)\n\n* feat: adding terraform configurations for blog\n\nPreviously, terraform was used to create a k8s cluster and run the blog on it. This PR switches the blog to run as a digital ocean app rather than on a dedicated kubernetes cluster for cost reasons.\n\n- Adding terraform for DO App as a module\n- Adding terraform for updating/deploy the do app module\n- Adding DNS records for @smerc.dev and @smerc.io email aliases\n- Output & variables for the associated modules & infrastructure."
    },
    "codes": [
      "domain",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/AnalysisByDesign/abd-cloud/commit/50c96dd99f04083d8a2f0d432ad2b0b3c291d3b9",
    "content": {
      "message": "Remove admin elb, asg, etc due to cost"
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/miend/rlt-test/commit/ad71247c2d4e986815412ecf0963a0c9282f466f",
    "content": {
      "message": "Terraform config for a very basic Kubernetes cluster deployment. Node pool size is 1 for cost savings, but in the future should be a higher number/should be multi-regional."
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/naftulikay/titan/commit/a0ea4fd84a409fe4ee853effa9f309a005b0efc1",
    "content": {
      "message": "Optional NAT\n\nThis feature allows users to disable NAT gateway creation,\ntypically for cost-savings or fully-private networks.\n\nI have tested this by hand in with NAT enabled and disabled by\ncreating an instance in the DMZ, shelling to it, reaching the\ninternet, then creating an instance in the admin layer, shelling\nto it, and validating that it can (nat_enabled=true) or cannot\n(nat_enabled=false) reach the internet."
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/bjacobel/terraform/commit/27738267ad6467f453f998419eca4b870b824faa",
    "content": {
      "message": "Prevent T3 Unlimited from chewing up my bill"
    },
    "codes": [
      "awareness",
      "policy"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/figurate/bedrock/commit/bffc023eeff075ef281b1fd261897f4c7216b354",
    "content": {
      "message": "Update default instance for more cost-effective choice"
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/singaporewaketools/iaac/commit/197502b1ac4bab77b9ab017b755c4d75ddaa218b",
    "content": {
      "message": "Add billing alerts"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/singaporewaketools/iaac/commit/cebf7f883b1325a4db1c0e7b3cd0d6684a7c41b0",
    "content": {
      "message": "Fix billing alert subscriptions"
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/sbogacz/terraform-aws-state-backend/commit/174486319f3b956807d56e5433880f9978884f93",
    "content": {
      "message": "use DynamoDB On Demand billing for the lock table to save on cost"
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/nationalarchives/tdr-grafana/commit/213ea410b9a2146cc804213dedeacdff4f07a74a",
    "content": {
      "message": "Move to Jenkins VPC\n\nJenkins VPC already available in the TDR management account.\n\nAdding Grafana to existing VPC means no need to create new VPC with additional cost involved"
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "commit",
    "url": "https://github.com/giuseppeborgese/terraform-locking-s3-state/commit/6b4e59e8b844417dc5c247bdef1b0adb8e2e7028",
    "content": {
      "message": "PAY_PER_REQUEST model"
    },
    "codes": [
      "saving",
      "billing_mode"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/coreos/tectonic-installer/issues/1932",
    "content": {
      "title": "Terraform Modules",
      "body": "FEATURE REQUEST\r\n\r\nFirstly thanks for whats here so far - the terraform modules in this repo are really useful for creating clusters, thanks to the vanilla flag. The modules themselves have also been designed in a way that makes them reusable outside the scope of a Tectonic install.\r\n\r\n```\r\nmodule \"tectonic-aws\" {\r\n   source = \"git@github.com:coreos/tectonic-installer?ref=1.6.7-tectonic.2//platforms/aws\r\n   # Additional variables here, as documented\r\n}\r\n```\r\n\r\nAlthough outside the documented method for using the modules, using them as a source to a terraform module call provides a clean interface between our existing code and the modules. I plan to reuse/replace modules as required, and hopefully fork and contribute any changes. I've tested this approach using 0.10.0 and Terraform workspaces and used variable files for different clusters.\r\n\r\nThis approach works but some of the modules are missing outputs that would be useful outside the module, and using the modules at the top level like this, there are no outputs. Some useful ones would be the console ELB address, and API ELB (so these can be added to a different RT53 zone). I'm sure there are others that could prove useful too.\r\n\r\nI've also been looking into what parts of Tectonic are open source and which require the license. From what I can see the majority is open source and free to use under various licenses - either released by coreos, kubernetes, prometheus or others - only the console and a few operators seem to require the enterprise tectonic license. Vanilla mode disables all these features, could installation of these possibly be enabled/disabled as individual components?\r\n\r\nThis issue isn't really a request for these features to be implemented by someone else, more just asking in advance if PRs of this nature would be suitable for merging: additional outputs and flags to install dex, prometheus etc.",
      "comments": [
        "Seconded. We are evaluating Tectonic, and I have added several outputs so we can do other things with the resources created by the Tectonic modules. \r\n\r\nFor instance, we are outputting the autoscale group names so we can attach [notifications](https://www.terraform.io/docs/providers/aws/r/autoscaling_notification.html) to them.\r\n",
        " @user @mrwacky42 Thanks for using our modules and I am glad that they are useful outside too :-) I generally have no objections on adding more outputs, but note that we do not maintain a \"stable\" contract currently on module outputs/inputs, but rather only on the top-level `config.tf` and platform `variables.tf`.\r\n\r\nTo differentiate those outputs from internal inter-module wiring variables I could think of adding another `output-ext.tf` which maintains a stable contract for them.\r\n\r\nHaving said that internal refactorings might imply unifying or separating existing internal modules so I am a bit careful defining stable contracts as of now but generally like the idea.\r\n\r @user thoughts?",
        "I am glad that I am not the only one who found it odd that the vanilla disabled all of the awesome dex identity, prometheus, heapster, and other features. If only we could disable the pay for features precisely, that or I can install with a fake tectonic key and rip out the pay for features that what i dont want per cluster.",
        "Disappointed to see that vanilla kubernetes mode is being removed after the 1.8.4 release per #2629. We will now need to look to evaluate alternative options for installation instead. At the moment I'm not sure if this will be using a fork of these modules, re-evaluating tectonic or moving back towards kops instead.\r\n\r\nThe usefulness of Tectonic is limited for us as we didn't want to use the installer program or the tectonic console. I can understand why your focus is moving back towards the core paid product though.\r\n\r\nThis change will effect the usefulness of the terraform modules, and so I'm going to close this issue."
      ]
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/coreos/tectonic-installer/issues/1384",
    "content": {
      "title": "Add Spotinst integration for worker nodes as alternative to ASG [AWS]",
      "body": "## Feature Request: Add Spotinst support; AWS\r\n\r\nSpotinst is a service that takes advantage of [Amazon's Spot Instance market](https://aws.amazon.com/ec2/spot/) to create cost-effective EC2 instances. Spotinst works by creating an elasticgroup and scaling through that rather than a conventional ASG. More details on elasticgroups can be found at https://spotinst.com/products/workload-management/elastigroup.\r\n\r\nMy group has swapped their ASGs out for Spotinst elasticgroups and have already seen 58% cost reduction on the worker compute pool. Another example of Spotinst usage in k8s can be found at http://blog.spotinst.com/2017/07/03/success-story-front-gate-tickets.\r\n\r\nWith the above, I feel it could be advantageous for Tectonic to provide _compatibility_ with Spotinst through the installer. Due to the [more] ephemeral nature of Spot Instances, this feature is only recommending this option for workers. Spotinst also features a cluster autoscaler that, like the standard autoscaler, request more nodes (in this case through elasticgroups) when capacity thresholds are reached.\r\n\r\n### design\r\n\r\nBy design, this feature would introduce 1 *required* parameter: `tectonic_aws_spotinst_worker_pool`, defaulted to `false`. Setting this parameter to true would have the following impacts.\r\n\r\n- Spotinst Elasticgroup is created\r\n  - Due to limitations in the cluster_autoscaler, it will currently create 1 elasticgroup per AZ. This ensures the autoscaler always scale a node appropriate to satisfy scheduling needs. For example, a node that must be scheduled in a certain AZ since that's where pod(s) PV live that need to be scheduled.\r\n  - Eventually we'll move away from this and use 1 elasticgroup containing all AZ/subnets.\r\n- No ASG is created\r\n- No LaunchConfiguration is created\r\n  - user-data is now part of the elasticgroup and the launch config would not be used\r\n\r\n### other notes\r\n\r\n- This issue is related to https://github.com/coreos/tectonic-installer/issues/768 but favors Spotinst over conventional [Spot Fleet](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html) provided by AWS. \r\n\r\n/cc @user @sym3tri ",
      "comments": [
        "What is incompatible? Does the ASG teardown what Spotinst sets up?",
        "We're working on the next generation of the installer which will integrate Tectonic and Open Shift. We'll consider this for that project, but will not be adding this feature in this repo. \r\n\r\nSee our blog for any additional details:\r\nhttps://coreos.com/blog/coreos-tech-to-combine-with-red-hat-openshift\r\n"
      ]
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/ministryofjustice/modernisation-platform/issues/198",
    "content": {
      "title": "Check billing for regional services configured as part of baselines",
      "body": "As we've deprecated regional services for restricted regions within the Modernisation Platform (#197), we should check the billing for each account within the Modernisation Platform to ensure we've removed the regional services appropriately and aren't being billed unnecessarily.",
      "comments": [
        "Through February there was no billing for regional services that we have previously deprecated, e.g. SecurityHub in ap-southeast-1."
      ]
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/ministryofjustice/modernisation-platform/issues/721",
    "content": {
      "title": "Add S3 mount for Performance Hub",
      "body": "## User Story\r\n\r\nPerformance Hub needs to able to read and write too persistent storage from a container.\r\n\r\nDesign document needs to be updated to show the S3 mount.\r\n\r\n## Value\r\n\r\nBaseline requirement for the functioning of the app\r\n\r\n## Questions / Assumptions\r\n\r\n<!-- Additional information to explain approach taken -->\r\n\r\n## Definition of done\r\n\r\n<!-- Checklist for definition of done and acceptance criteria, for example: -->\r\n\r\n- [ ] design docs have been updated\r\n- [ ] another team member has reviewed\r\n- [ ] Update user data for ECS cluster server to install and configure rclone.\r\n- [ ] Mount S3 bucket on all ECS hosts.\r\n- [ ] Reconfigure ECS task definition so containers can see S3 mount point. \r\n- [ ] Log into container instance and verify that you can see the s3 bucket, and that you can read and write from it.\r\n\r\n\r\n## Reference\r\n\r\n[How to write good user stories](https://www.gov.uk/service-manual/agile-delivery/writing-user-stories)\r\n",
      "comments": [
        "Originally we were going to use Amazon's EFx File Service to build this capability. Unfortunately EFx requires a domain control to either exist somewhere on the network, or one can be created specifically for managing access. This adds extra cost and complication, so we are going back to S3. We need to find a free tool that lets us mount S3 buckets as a file system mount, basically like s3fs on Linux.",
        "We are now code complete on this feature, however there is an issue around Windows credentials which means we can't automate the final stage, because we have a chicken and egg situation because of the Windows password not being known when the bootstrap code runs.\r\n\r\nFor now we are going to keep the manual step and see if we can resolve this in the future. ",
        "We found another problem, Docker on ECS for security reasons does not support bind mounts to a separate volume. Which is why we have not been able to get the S3 mount to work. This means are only option is to go back to EFx or re-write the application code to use S3 natively. \r\n\r\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/bind-mounts.html\r\n\r\n_Windows containers can mount whole directories on the same drive as $env:ProgramData. _\r\n\r\nWhich means on sub-folder on the C drive can be used for bind mounts.\r\n\r\n"
      ]
    },
    "codes": [
      "saving",
      "awareness",
      "provider"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/ministryofjustice/modernisation-platform/issues/889",
    "content": {
      "title": "create a local sandbox transit gateway",
      "body": null,
      "comments": [
        "An issue has been identified due to the process of testing Transit Gateway peering using the Sandbox account.\r\n\r\n**details**:\r\nTransit Gateway peering of two Transit Gateways in the same region is not supported yet in London (eu-west-2).",
        "Reviewed Transit Gateway peering workaround using a routing VPC with two routing subnets.\r\n\r\nThe solution worked but had several reasons to justify it's use\r\n- Complex routing due to the fact that the default route on Mod Plaformform is used for NAT gateway\r\n- Increased cost of Transit Gateway connections\r\n- Increased cost of Transit Gateway traffic\r\n- This solution will have to be undone when AWS implement the Same Region Peering in Q4 2021\r\n\r\nFor the above reasons this solution has been parked for now, this can be implement if Transit Gateway Peering is urgently required."
      ]
    },
    "codes": [
      "networking",
      "increase"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/ministryofjustice/modernisation-platform/issues/1089",
    "content": {
      "title": "Use S3 module in bastion to prevent older versions from never being deleted",
      "body": "## User Story\r\n\r\nWe have S3 versioning enabled in bastion to allow recovering logs that are accidentally deleted.\r\n\r\nWe also have S3 lifecycle management configured to gradually transition logs to cheaper storage. Therefore, logs remain in S3 for 30 days before moving to IA (Infrequent Access) storage. After 60 days they move to Glacier and after 180 days they expire and are permanently deleted.\r\n\r\nIn order to prevent older versions from being retained forever, in addition to the lifecycle policy for the current version we need to configure a separate lifecycle policy for noncurrent versions. Otherwise, older versions will never be moved to cheaper storage and will never be expired/deleted.\r\n\r\nThe S3 bucket module allows both current and noncurrent version lifecycle management: https://github.com/ministryofjustice/cloud-platform-terraform-s3-bucket It would appear to make sense reusing the S3 bucket module in the bastion module.\r\n\r\n## Value\r\n\r\n<!-- Describe the value and purpose of the changes -->\r\n\r\n## Questions / Assumptions\r\n\r\n<!-- Additional information to explain approach taken -->\r\n\r\n## Definition of done\r\n\r\n<!-- Checklist for definition of done and acceptance criteria, for example: -->\r\n\r\n- [ ] readme has been updated\r\n- [ ] user docs have been updated\r\n- [ ] another team member has reviewed\r\n- [ ] tests are green\r\n\r\n## Reference\r\n\r\n[How to write good user stories](https://www.gov.uk/service-manual/agile-delivery/writing-user-stories)\r\n",
      "comments": [
        "https://github.com/ministryofjustice/modernisation-platform-terraform-bastion-linux/pull/11",
        "https://github.com/ministryofjustice/modernisation-platform-environments/pull/105",
        "https://github.com/ministryofjustice/modernisation-platform-terraform-bastion-linux/pull/12",
        "https://github.com/ministryofjustice/modernisation-platform-terraform-s3-bucket/pull/17",
        "https://github.com/ministryofjustice/modernisation-platform-terraform-bastion-linux/pull/13"
      ]
    },
    "codes": [
      "saving",
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/ministryofjustice/modernisation-platform/issues/314",
    "content": {
      "title": "Investigate a resource removal tool for sandbox accounts",
      "body": "We should investigate a removal tool like Gruntwork's [cloud-nuke](https://github.com/gruntwork-io/cloud-nuke) or [aws-nuke](https://github.com/rebuy-de/aws-nuke) to periodically clean core and member sandbox accounts (if applicable).\r\n\r\nThis will help us reduce cost of resources not managed via Terraform.",
      "comments": [
        "Duplicate of https://github.com/ministryofjustice/modernisation-platform/issues/1124"
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/ministryofjustice/modernisation-platform/issues/1156",
    "content": {
      "title": "Delete the justice-on-the-web-sandbox account",
      "body": "## User Story\r\n\r\nI've confirmed with @user , Adam Brown and @user that this account is no longer used and can be deleted.\r\n\r\n## Value\r\n\r\nClean up our unused AWS accounts and reduce costs.\r\n\r\n## Questions / Assumptions\r\n\r\n<!-- Additional information to explain approach taken -->\r\n\r\n## Definition of done\r\n\r\n<!-- Checklist for definition of done and acceptance criteria, for example: -->\r\n\r\n- [ ] readme has been updated\r\n- [ ] user docs have been updated\r\n- [ ] another team member has reviewed\r\n- [ ] tests are green\r\n\r\n## Reference\r\n\r\n[How to write good user stories](https://www.gov.uk/service-manual/agile-delivery/writing-user-stories)\r\n",
      "comments": []
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/ministryofjustice/modernisation-platform/issues/1463",
    "content": {
      "title": "ITHC Consider/Implement AWS DNSSEC",
      "body": "## User Story\r\n\r\nAs a modernisation platform engineer \r\nI want to investigate AWS DNSSEC for public Route53 zones\r\nSo that our public domains are secured\r\n\r\n## Value\r\n\r\nOur IT Healthcheck identified some points of configuration that could be adjusted to improve our security posture. AWS offer DNSSEC for domains in Route53, and as we publicly advertise gov.uk addresses we should consider the value of implementing DNSSEC for them. Implementing DNSSEC may come with additional costs in technical time, but will improve the security of services we offer, protecting them from situations such as DNS poisoning attacks.\r\n\r\n## Questions / Assumptions\r\n\r\nSee ITHC report for further details. Consider the cost in technical time from implementing/maintaining DNSSEC and the potential impact of failing to properly maintain this once implemented.\r\n\r\n## Definition of done\r\n\r\n- [ ] decision on DNSSEC has been reached with tech arch / product owner\r\n- [ ] DNSSEC successfully implemented\r\n- [ ] team documentation has been updated\r\n- [ ] another team member has reviewed\r\n- [ ] tests are green\r\n- [ ] public FQDNs tested successfully\r\n\r\n## Reference\r\n\r\n[How to write good user stories](https://www.gov.uk/service-manual/agile-delivery/writing-user-stories)\r\n",
      "comments": []
    },
    "codes": [
      "awareness",
      "increase",
      "provider"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/ministryofjustice/modernisation-platform/issues/1203",
    "content": {
      "title": "Spike - could we do a bastion per VPC?",
      "body": "## User Story\r\n\r\nCurrently each account is creating a bastion, that's potentially a bastion per application environment.\r\n\r\nNumber of environments - development, test, preproduction, production = 4\r\nNumber of applications - potentially hundreds, let's say 100\r\nNumber of bastions - 400 +\r\n\r\nThis spike is to look at if we could securely switch to using a bastion per VPC, which would reduce the number of bastions needed massively.\r\n\r\nNumber of environments - 4\r\nNumber of business units - 8\r\nNumber of bastions - 32 max\r\n\r\n## Value\r\n\r\nReduce costs\r\nReduced energy consumption\r\nNo need for application teams add the bastion module to their code\r\n\r\n## Questions / Assumptions\r\n\r\n<!-- Additional information to explain approach taken -->\r\n\r\n## Definition of done\r\n\r\n<!-- Checklist for definition of done and acceptance criteria, for example: -->\r\n\r\n- [ ] spike complete and presented to the team\r\n- [ ] another team member has reviewed\r\n- [ ] tests are green\r\n\r\n## Reference\r\n\r\n[How to write good user stories](https://www.gov.uk/service-manual/agile-delivery/writing-user-stories)\r\n",
      "comments": []
    },
    "codes": [
      "networking",
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/ministryofjustice/modernisation-platform/issues/1091",
    "content": {
      "title": "Research and PoC - Turn off instances in an environment overnight",
      "body": "## User Story\r\n\r\nThere is no reason to have non production instances (EC2, RDS, etc) constantly running, outside of working hours they should be shut down.\r\n\r\n## Value\r\n\r\nSave money and increase the sustainability of the platform.  This also encourages work life balance if instances are not accessible at night.\r\n\r\n## Questions / Assumptions\r\n\r\nThere should be some way of excluding instances eg a tag.\r\nProduction instances should not be included.\r\nThis would need to be across all the mp accounts, could this be done centrally or would it need to be done on a per account basis?\r\nPossible solutions include systems manager automation documents or a lambda.\r\nWe need to ensure instances can be started and stopped on a schedule without human intervention, application teams will also need to ensure this.\r\n\r\nhttps://aws.amazon.com/blogs/mt/systems-manager-automation-documents-manage-instances-cut-costs-off-hours/\r\n\r\n## Definition of done\r\n\r\n<!-- Checklist for definition of done and acceptance criteria, for example: -->\r\n\r\n- [ ] readme has been updated\r\n- [ ] user docs have been updated\r\n- [ ] another team member has reviewed\r\n- [ ] tests are green\r\n\r\n## Reference\r\n\r\n[How to write good user stories](https://www.gov.uk/service-manual/agile-delivery/writing-user-stories)\r\n",
      "comments": [
        "https://github.com/ministryofjustice/modernisation-platform/pull/1253",
        "One solution that I got working on Sprinkler is by using the third-party terraform module terraform-aws-lambda-scheduler-stop-start as follows:\r\n\r\n```hcl\r\n#------------------------------------------------------------------------------\r\n# Schedule stop/start EC2 instances\r\n#------------------------------------------------------------------------------\r\n\r\nmodule \"stop_ec2_instance_nights\" {\r\n  source                         = \"github.com/diodonfrost/terraform-aws-lambda-scheduler-stop-start?ref=3.1.3\"\r\n  name                           = \"stop_ec2_instance_nights\"\r\n  cloudwatch_schedule_expression = \"cron(0 21 ? * * *)\" # Everyday at 21:00 GMT\r\n  schedule_action                = \"stop\"\r\n  autoscaling_schedule           = \"false\"\r\n  ec2_schedule                   = \"true\"\r\n  rds_schedule                   = \"false\"\r\n  cloudwatch_alarm_schedule      = \"false\"\r\n  scheduler_tag                  = {\r\n    key   = \"stop_nights\"\r\n    value = \"test\" # only for the test environment\r\n  }\r\n}\r\n\r\nmodule \"start_ec2_instance_mornings\" {\r\n  source                         = \"github.com/diodonfrost/terraform-aws-lambda-scheduler-stop-start?ref=3.1.3\"\r\n  name                           = \"start_ec2_instance_mornings\"\r\n  cloudwatch_schedule_expression = \"cron(0 6 ? * * *)\" # Everyday at 6:00 GMT\r\n  schedule_action                = \"start\"\r\n  autoscaling_schedule           = \"false\"\r\n  ec2_schedule                   = \"true\"\r\n  rds_schedule                   = \"false\"\r\n  cloudwatch_alarm_schedule      = \"false\"\r\n  scheduler_tag                  = {\r\n    key   = \"stop_nights\"\r\n    value = \"test\" # only for the test environment\r\n  }\r\n}\r\n\r\nresource \"aws_kms_grant\" \"stop_start_scheduler\" {\r\n  key_id            = aws_kms_key.ebs.id\r\n  grantee_principal = module.start_ec2_instance_mornings.lambda_iam_role_arn\r\n  operations        = [\r\n    \"Decrypt\",\r\n    \"DescribeKey\",\r\n    \"CreateGrant\"\r\n  ]\r\n}\r\n```",
        "https://github.com/ministryofjustice/modernisation-platform/pull/1260",
        "We had a conversation with the team after today's scrum and decided that AWS Instance Scheduler is another alternative which might be the closest to what we will need in the end: https://aws.amazon.com/solutions/implementations/instance-scheduler/. While the terraform module would work, AWS Instance Scheduler allows cross-account and flexible scheduling, which might be beneficial for configuration at platform level. Only downside is that it's a new feature and there doesn't seem to be terraform module for it yet. We need a separate POC to see how this solution would work.",
        "Having used AWS Instance Scheduler previously, it's supplied as a cloudformation template by default. It could be migrated into terraform with some work, although there are (from memory) some interactions with dynamodb which are configured using an AWS-supplied CLI python package. Again (from memory) I believe those can be conducted either manually, or ported into Terraform as they're just DynamoDB entries ( relevant resources in the AWS provider are probably aws_dynamodb_table / aws_dynamodb_table_entry ).",
        "Having looked into this the following things stand out:\r\n\r\n1. Instance Scheduler is supplied in two cloudformation templates. Would we recreate this in terraform, and if so where would we store the code? \r\n2. Which account would the main Instance Scheduler resources be deployed?\r\n3. The scheduler works by reading a tag - eg `availability` - and then matching the value to a schedule defined in dynamodb - eg `weekdays`. Is a single tag/key relationship enough for us?\r\n4. Do we expect this to be something teams opt into or opt out of?\r\n",
        "1. For the proof of concept we're using the Amazon-supplied Cloud Formation Templates.\r\n2. We've discussed internally which accounts to make use of.\r\n3. We've agreed that the scheduler is sufficient as is.\r\n4. We've agreed that the scheduler will be something teams will opt out of.\r\n",
        "At present we have two CFTs deployed and can see CloudWatch logs indicating that we're successfully assuming a role in a remote account and querying instances appropriately. We also see an error message indicating that the schedule we've set up is not matching the schedule name being read despite the two appearing identical.\r\nI have raised a support request in the calling AWS account to see what support AWS can offer us.",
        "Resolved issues we had with schedules not reading.\r\nConfirmed that we can apply a schedule across accounts and shut down / start up instances based on schedules.\r\nMight be some more value we can drive out of this card, but generically it looks like this will meet our needs and will demo / discuss next steps with @user and the rest of the team.",
        "Successfully demonstrated. Will raise issues to move this along into production use"
      ]
    },
    "codes": [
      "saving",
      "instance",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/ministryofjustice/modernisation-platform/issues/1601",
    "content": {
      "title": "Spike: Investigate use of AWS Shield advanced",
      "body": "## User Story\r\n\r\nWe want to ensure we have the means to identify and mitigate DoS attacks, AWS shield https://aws.amazon.com/shield/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc\r\nIs a product that may help with this offering greater protection and monitoring.\r\n\r\nCheck if this is something we want to implement, discuss with other teams to gauge their interest in using it. (It is applied at an org level)\r\n\r\n## Value\r\n\r\n<!-- Describe the value and purpose of the changes -->\r\n\r\n## Questions / Assumptions\r\n\r\n<!-- Additional information to explain approach taken -->\r\n\r\n## Definition of done\r\n\r\n<!-- Checklist for definition of done and acceptance criteria, for example: -->\r\n\r\n- [ ] speak with other teams to see if they are interested\r\n- [ ] present findings to team\r\n- [ ] cost analysis\r\n- [ ] another team member has reviewed\r\n\r\n\r\n## Reference\r\n\r\n[How to write good user stories](https://www.gov.uk/service-manual/agile-delivery/writing-user-stories)\r\n",
      "comments": [
        "## Intro\r\n\r\nAWS Shield Advanced is a paid version of their free offering AWS Shield\r\n\r\n## Features comparison\r\n\r\nTable - https://aws.amazon.com/shield/getting-started/\r\n\r\nFeature | Shield | Advanced\r\n---|---|---|\r\nCost | Free, WAF additional | $3000 per month per org, egress traffic costs, WAF included for protected resources, Firewall manager policies included\r\nProtects | Everything | CloudFront, Route 53, ELB, EC2, Global Accelerator, NLB\r\nProtection offered | DDoS - Layer 3 and 4 (eg SYN/UDP floods, reflection attacks) | DDoS - Layer 3 and 4, Automatic layer 7 mitigations (WAF for CloudFront only), Additional DDoS mitigation capacity for large attacks\r\nSupport | Standard AWS support | AWS Shield Response Team\r\nMonitoring | Standard Cloudwatch metrics | DDoS specific metrics\r\nCost protection | No | Don't pay for usage from a DDos attack\r\n\r\n## How to implement\r\n\r\nSubscribe once and agree to costs. It then needs to be enabled in each account individually.\r\nThis can be done across multiple accounts using AWS Firewall Manager.\r\nAWS Firewall Manager can also be used to add protected resources across accounts.  Policies can be scoped to accounts or OUs, and also tags.  So for example we could automatically apply Shield Advanced to all resources with the \"shield-advanced\" tag in the Modernisation Platform OU via Auto remediate any noncompliant resources.  We would probably want to only include external facing endpoints to keep costs down. \r\nAutomatic application layer DDoS mitigation can also be enabled to automatically generate WAF rules to block DDoS traffic, this is only for CloudFront distributions.  Manual WAF mitigations and rules can be applied across CloudFront and ELBs.\r\nSimilar applications can be managed together in protection groups.\r\n\r\n## Shield Response Team (SRT) support\r\nYou can give the SRT access to assist during an attack, there is also an option for proactive engagement where they are notified and start working on an attack before you contact them, they can then apply WAF rules to mitigate the attack with our permission.  This looks to have to be manually configured per account.\r\n\r\n## Terraform\r\n\r\nAWS Firewall Manager policies can be [implemented via Terraform](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/fms_policy).  \r\nThere needs to be a Firewall Manager Administrator account, so this would most likely be the organisational security account and can be [implemented with Terraform](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/fms_admin_account).\r\nResources can also be added to AWS Shield Advanced at an account level [via terraform](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/shield_protection) although it's not clear how this would work or if it would be needed if we were using auto remediation, I suspect if manually added it would just show as already in compliance for auto remediation. Firewall Manager doesn't support Amazon Route 53 or AWS Global Accelerator so these would need to be added manually.\r\n\r\n## Visibility\r\nActivity is visible in the Shield dashboard per account, there are also some nice Cloudwatch metrics which we could alert on:\r\nDDoSDetected \r\nDDoSAttackBitsPerSecond\r\nDDoSAttackPacketsPerSecond \r\nDDoSAttackRequestsPerSecond\r\n\r\n## Credit\r\nTo apply for credit for usage costs during an attack you need to request it within 15 days of the end of the billing month in which the attack occurred.\r\n\r\n## Quotas\r\nhttps://docs.aws.amazon.com/waf/latest/developerguide/shield-limits.html\r\n",
        "Discussion on Slack here - https://mojdt.slack.com/archives/CA454PY2C/p1648208672780309\r\n\r\nAgreed that this will be implemented - \r\n\r\n[jake (they/them)](https://app.slack.com/team/U02NYKNM456):rainbow:  [16 hours ago](https://mojdt.slack.com/archives/CA454PY2C/p1648486990787259?thread_ts=1648208672.780309&cid=CA454PY2C)\r\nGonna pick this up on Friday to make sure it\u2019s enabled correctly (ie doesn\u2019t mess up billing).\r\n\r\nPlanning to:\r\ncreate a new SSO group to view Firewall Manager in the delegated administrator to give everyone visibility of shared rules (they\u2019re $100/policy)\r\ncreate a pipeline to apply new Firewall policies\r\n:thank_you:\r\n1\r\n\r\n\r\n\r\n\r\n\r\n[jake (they/them)](https://app.slack.com/team/U02NYKNM456):rainbow:  [16 hours ago](https://mojdt.slack.com/archives/CA454PY2C/p1648487043658949?thread_ts=1648208672.780309&cid=CA454PY2C)\r\n(Friday being 1st April, as the docs aren\u2019t clear on pro-rataing, and $3K seems a lot for 4 days).",
        "ADR here - https://github.com/ministryofjustice/modernisation-platform/blob/main/architecture-decision-record/0018-use-aws-shield-advanced.md"
      ]
    },
    "codes": [
      "networking",
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/ministryofjustice/modernisation-platform/issues/1484",
    "content": {
      "title": "Guidelines for providing Instance Scheduler to Modernisation Platform customers",
      "body": "## User Story\r\n\r\nAs a modernisation platform team member \r\nI want to define guidelines for using Instance Scheduler\r\nSo that we offer a consistent experience to customers that balances cost-efficiency with user needs. \r\n\r\n## Value\r\n\r\nThis story should cover the discussion of topics related to incorporating the AWS Instance Scheduler with the Modernisation Platform. This story may require some user research.\r\n\r\n## Questions / Assumptions\r\n\r\n* Assumption: We will use an \"opt-out\" approach to scheduling instances.\r\n* Assumption: We will include EC2 and RDS instances in scheduling scope.\r\n* Assumption: Schedules will be defined by the Modernisation Platform team in the code that calls the Instance Scheduler module\r\n* Assumption: A default schedule operating during weekdays will run between 06:50 and 19:00\r\n* Question: How will new accounts be added to the table of accounts in scope? (eg, how will values be added to the dynamoDB entry for `cross_account_roles`? How will customers define their scheduling requirements?)\r\n* Question: What will default times look like?\r\n* Question: How will tags be added to instances? (eg, how will we ensure that a user-created resource such as an EC2 instance gets tagged with `auto-scheduler=default`)\r\n* Question: Will production resources be excluded from scheduling? If so, how?\r\n\r\n## Definition of done\r\n\r\n- [ ] full discussion has taken place\r\n- [ ] any required user research has taken place\r\n- [ ] Architectural Decision Record has been updated\r\n- [ ] subordinate stories have been created as required\r\n\r\n## Reference\r\n\r\n[How to write good user stories](https://www.gov.uk/service-manual/agile-delivery/writing-user-stories)\r\n[Recreate AWS Instance Scheduler in Terraform](https://github.com/ministryofjustice/modernisation-platform/issues/1483)",
      "comments": []
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/clingen-data-model/architecture/issues/94",
    "content": {
      "title": "Review argo logging and adjust levels appropriately to save noise/cost",
      "body": "",
      "comments": [
        "closed via #96 "
      ]
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/clingen-data-model/architecture/issues/95",
    "content": {
      "title": "Review recent costs in clingen GCP projects",
      "body": "",
      "comments": [
        " @user Looking like the huge majority of our clingen costs are in the infrastructure for GKE (this includes the CPU/RAM for the VM instances that GKE is managing, as well as the disk storage for those instances). There's a few other misc things in the top 10 (like  static IP addresses for public endpoints, HTTP load balancing for directing traffic to genegraph, submitter, argo, etc. Overall I'd say it's around 75%+ of the costs are in GKE itself, and in those few charges related to operating our apps. I think reducing the GKE costs would involve some engineering time to make us a little more resilient -- we can run [preemptible](https://cloud.google.com/compute/docs/instances/preemptible\n) instances for dramatically cheaper, but we'd need to make our app gracefully handle instance terminations/restarts every ~24 hours. Perhaps the work @user is doing on resuming / snapshotting queue processing might help here.\n\nI looked at logging costs specifically too see what the noisy argo (or other) logging may be contributing, and they're pretty negligible -- under $5/month for all the logs in all three projects combined. So, argo log noise is purely an annoyance thing, at these costs.",
        "This is great info. Sounds like our most beneficial option is to follow up\non Kyle\u2019s work to see if it enables us to go the preemptible route. Unless\nI missed something. Let\u2019s you and I keep an eye on this and see if/when\nKyle\u2019s work is stable enough to begin investigating further. I\u2019ll defer to\nyou to decide timing on this.\n\nOn Mon, Jun 28, 2021 at 12:47 PM Steve Jahl ***@***.***>\nwrote:\n\n> @user <https://github.com/larrybabb> Looking like the huge majority\n> of our clingen costs are in the infrastructure for GKE (this includes the\n> CPU/RAM for the VM instances that GKE is managing, as well as the disk\n> storage for those instances). There's a few other misc things in the top 10\n> (like static IP addresses for public endpoints, HTTP load balancing for\n> directing traffic to genegraph, submitter, argo, etc. Overall I'd say it's\n> around 75%+ of the costs are in GKE itself, and in those few charges\n> related to operating our apps. I think reducing the GKE costs would involve\n> some engineering time to make us a little more resilient -- we can run\n> preemptible <https://cloud.google.com/compute/docs/instances/preemptible>\n> instances for dramatically cheaper, but we'd need to make our app\n> gracefully handle instance terminations/restarts every ~24 hours. Perhaps\n> the work @user <https://github.com/theferrit32> is doing on\n> resuming / snapshotting queue processing might help here.\n>\n> I looked at logging costs specifically too see what the noisy argo (or\n> other) logging may be contributing, and they're pretty negligible -- under\n> $5/month for all the logs in all three projects combined. So, argo log\n> noise is purely an annoyance thing, at these costs.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/clingen-data-model/architecture/issues/95#issuecomment-869841146>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAHKEG6ZWYCX5U3OLEERF5DTVCRQTANCNFSM47OBS3AQ>\n> .\n>\n-- \nLarry Babb\nPrincipal Software Engineer\nClinical Genome Resource I clinicalgenome.org\n<http://www.clinicalgenome.org>\nBroad Institute of MIT and Harvard\n***@***.***\n"
      ]
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/clingen-data-model/architecture/issues/112",
    "content": {
      "title": "Protecting graphql endpoints from abuse",
      "body": "At our meeting last week, we discussed a desire to protect our graphql endpoints from abuse.\n\nThe easy hammer/nail here is that if we're using a google cloud ingress, we can use cloud armor (https://cloud.google.com/armor/) to do things like blocking a single IP, or range of IPs. Cloud Armor also provides a certain amount of DDOS protection with it. Without too much effort here, we can ban client IPs that we've identified as abusive. We currently use cloud armor in the prod project to restrict access to the argo ingress -- in case someone wants to look at some prior art.\n\nIf this is going to be a public resource (or even, a partially public one in which we issue auth tokens for certain users), implementing rate limiting in our app is probably the right approach to prevent someone from running away with queries. I think this needs to be done at the app level -- anything at the google cloud level is probably not going to be smart enough, and will likely stick us with a bunch of vendor lock-in.\n\nI took at look at what our gnomad friends are up to, and their rate limiting approach can be found here: https://github.com/broadinstitute/gnomad-browser/blob/main/graphql-api/src/graphql/rate-limiting.js\n\nGenerally speaking, they're keeping track of the number of requests from specific IPs, and limiting that over a time window. There's also a cost function -- more complex graphql queries incur a higher cost, the higher the cost of your queries, the fewer you're allowed to make during a window. They are using this library for calculating complexity cost: https://github.com/slicknode/graphql-query-complexity\n\nFound an interesting blog post on how shopify did this as well: https://shopify.engineering/rate-limiting-graphql-apis-calculating-query-complexity",
      "comments": []
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/clingen-data-model/architecture/issues/80",
    "content": {
      "title": "Increase monitoring and visibility of kafka/confluent",
      "body": "I think it's desireable from a cost projection and performance perspective to spend some time figuring out how to monitor and track metrics for our kafka streams in confluent.\n\nBest case scenario is probably to see if we can get the metrics from confluent plumbed into our GCP monitoring account, so that we can dashboard and alert from there with the rest of our app metrics. Next best, is to turn on any monitoring and alerting capabilities that we have in confluent to ensure that we're aware of cost problems and performance issues without manually checking a dashboard.",
      "comments": [
        " @user the `ccloud` CLI can be used to list the clusters we have, list the topics+partitions in the clusters, and then a micro python/clojure program can be used to find info for each topic, like how many messages there are, what is the timestamp of the first and last message, and other stuff we might want to know that's not readily available in the Confluent UI or client. Maybe do some sampling from the topic and estimate the average message size. Could make it a kubernetes job to run once a day or something.",
        " @user Thanks! I'll take a look.\r\n\r\nConfluent does have an API for metrics, which might be easier to work with, depending on what format the ccloud cli is outputting metrics in: https://docs.confluent.io/cloud/current/monitoring/metrics-api.html\r\n\r\nI also found this: https://github.com/Dabz/ccloudexporter, which exposes the metrics on an HTTP api appropriate for Prometheus to scrape (which I think can use google monitoring as long term storage for the metrics it collects). Prometheus is something that I'm considering deploying anyway for other reasons, so this might be the way to go if that ends up being the case."
      ]
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/digitalocean/Kubernetes-Starter-Kit-Developers/issues/71",
    "content": {
      "title": "Question on costs",
      "body": "Is there a ballpark on the costs to run this? If I left it running for a month what is the baseline cost involved with no workload.",
      "comments": [
        "Hi @user \r\n\r\nThe following is an illustration. The actual cost will be based on your usage. A reasonable lab set up would be to have DOKS installed with Starter Kit. For example, the following configuration will cost you $65/month, prorated hourly.\r\n\r\n1. DOKS -> $40 per month for 2 AMD/Intel nodes with 4GB/2VCPU each.\r\n2. Block Storage for 10 days of retention (<10GB)-> $1.\r\n3. DO Spaces (<50GB) -> $1.\r\n4. DO Load Balancer -> $10 per month.\r\n\r\nOne obvious question is the resources used by the Starter Kit components. Based on our analysis, it is about 1 CPU & 2GB RAM. So you will have the remaining compute/memory resources available for your workloads.\r\n\r\nHope it helps."
      ]
    },
    "codes": [
      "instance",
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/DoSomething/infrastructure/issues/129",
    "content": {
      "title": "Experiment: Serverless GraphQL, with Terraform & CircleCI",
      "body": "We use GraphQL to power our submission & campaign galleries, and our new user profiles. Starting [this quarter](https://docs.google.com/document/d/1Z0LesRih2rql2mumuM3126kkZC2Qw4IbCHMAl7iw77A/edit), we're also using GraphQL to fetch data for broadcasts in Gambit. As we send more traffic to GraphQL, we want to make sure things keep running smoothly!\r\n\r\nOne of the difficult things about scaling DoSomething.org is that our traffic is not consistent. For example, here's what traffic to [`graphql.dosomething.org`](https://graphql.dosomething.org) has looked like over the past 3 days:\r\n\r\n![production-rpm-last-3-days](https://user-images.githubusercontent.com/583202/52079553-d827fc80-2563-11e9-913b-bfb898186d65.png)\r\n\r\nRather than have systems over-provisioned during low-traffic periods (or require manual intervention to scale up and down), I wanted to explore using \"serverless\" techologies \u2013 Lambda and DynamoDB \u2013 to scale up and down transparently and only pay for what we use.\r\n\r\nI also wanted to take the opportunity to see whether we could manage our serverless applications with Terraform, since that's where we've begun collecting the rest of our infrastructure. Our previous foray into this world, [Bertly](https://github.com/DoSomething/bertly), was built with the [Serverless framework](https://serverless.com) which provides a batteries-included starting point but locks us into provisioning via per-app CloudFormation templates.\r\n\r\nThere are a few downsides to this forced dependency on CloudFormation:\r\n1. CloudFormation is only able to provision AWS resources, so if we wanted to depend on a resource not built by Amazon, it'd have to be provisioned and hooked up by hand.\r\n2. We lose the ability to connect to other Terraform resources automatically (via module inputs and outputs), and therefore increase the chance of accidentally connecting the \"wrong wires\" together.\r\n3. Because Serverless provisions app infrastructure as part of the deploy process, it means any developer (or CI service) that's able to deploy a serverless application would also have read & write access to a large portion of our AWS resources. Yikes!\r\n4. It adds conceptual overhead \u2013 instead of having everything in one place, it's one more tool to learn.\r\n\r\n## Goal\r\nWith that in mind, I set out to move our [GraphQL service](https://graphql.dosomething.org) to run on \"vanilla\" Lambda, using Terraform to provision the deployment bucket, function, roles, logging, and API gateway.\r\n\r\nThis work was mostly tracked in #119, DoSomething/graphql#46, and #128. \r\n\r\n## Process\r\nHere's a high-level overview of the work that went into this:\r\n\r\n\ud83d\udecb I configured [a re-usable `lambda_function` module](https://github.com/DoSomething/infrastructure/pull/119#issuecomment-457675183) that provisions a deploy bucket (seeded with a \"hello world\" example app), API Gateway w/ Lambda proxy, Lambda function, CloudWatch log buckets, a base execution role, and a minimally priviliged access key for deploys.\r\n\r\n\ud83e\udd5e I added support for [swappable cache drivers](https://github.com/DoSomething/graphql/pull/48), so we could use Redis (on Heroku) and DynamoDB (on Lambda) with the same backend code. DynamoDB offers on-demand scaling and pricing (and [no VPC requirement](https://github.com/DoSomething/infrastructure/pull/119#issuecomment-457675183)), which fits well with the serverless philosophy we're going for here.\r\n\r\n\ud83d\udd75\ufe0f I ran into some spooky timeouts & dug into [why they might be happening with X-Ray](https://github.com/DoSomething/infrastructure/pull/119#issuecomment-457762658). It turned out there were two culprits: occasionally [slow cold-boots when using less RAM](https://github.com/DoSomething/infrastructure/pull/125) and [an issue with our module loader](https://github.com/DoSomething/graphql/pull/46#issuecomment-458720632).\r\n\r\n\ud83d\udce6 Using the Terraform module, I easily [spun up QA & production environments](https://github.com/DoSomething/infrastructure/pull/128) for GraphQL.\r\n\r\n\ud83e\udd39\u200d\u2642\ufe0f I configured [CircleCI](https://circleci.com) to [run our builds & deploy to each Lambda environment](https://github.com/DoSomething/graphql/pull/46#issuecomment-459106455) (using the minimally priviliged AWS access key provisioned by Terraform). Plus, [a handy approval UI for prod deploys](https://github.com/DoSomething/graphql/pull/52#issuecomment-460441073)!\r\n\r\n## Benchmarks\r\nSince one of the goals here was scalability & performance, I did a lot of load testing with [Siege](https://www.joedog.org/siege-manual/).\r\n\r\nI've summarized the results below, comparing a `topic` query (used by Gambit & hitting a local cache), and a `postsByCampaignId` query (used by Phoenix's campaign gallery & hitting Rogue and Northstar).\r\n\r\nI ran these tests from `mjolnir.dosomething.org` against our QA environment, with applicable Heroku dynos temporarily scaled up to match their production capacity. To make sure the results were representative, I ran each test until it completed 50,000 requests.\r\n\r\n\r\n#### Broadcast query (simple)\r\n|                           | Duration | Throughput | Average | Worst | Comparison                          |\r\n|--------------------------------|----------|------------|---------|-------|:-----------------------------:|\r\n| **Lambda backend** &nbsp;&nbsp;&nbsp;&nbsp;  | 1.6m     | 518.4 rps  | 0.08s   | 7.71s | :trophy:          |                  \r\n| **Heroku backend**             | 3.2m     | 257.9 rps  | 0.18s   | 1.27s | :small_red_triangle_down: 125% |\r\n\r\n\r\n<details>\r\n<summary>raw <code>topic</code> test results</summary>\r\n\r\n```\r\nsiege -c50 -r1000 --content-type \"application/json\" 'https://rpkpk1dw7b.execute-api.us-east-1.amazonaws.com/qa POST {\"query\":\"{topic(id: \\\"G8LCAYO9WuGC2UW8mQi6w\\\") { name,  ...on AskYesNoBroadcastTopic {text,saidYes}}}\"}'\r\n\r\nTransactions:\t\t       50000 hits\r\nAvailability:\t\t      100.00 %\r\nElapsed time:\t\t       96.45 secs\r\nData transferred:\t       39.34 MB\r\nResponse time:\t\t        0.08 secs\r\nTransaction rate:\t      518.40 trans/sec\r\nThroughput:\t\t        0.41 MB/sec\r\nConcurrency:\t\t       42.51\r\nSuccessful transactions:       50000\r\nFailed transactions:\t           0\r\nLongest transaction:\t        7.71\r\nShortest transaction:\t        0.02\r\n\r\n\r\n\r\nsiege -c50 -r1000 --content-type \"application/json\" 'https://graphql-qa.dosomething.org/graphql POST {\"query\":\"{topic(id: \\\"G8LCAYO9WuGC2UW8mQi6w\\\") { name,  ...on AskYesNoBroadcastTopic {text,saidYes}}}\"}'\r\n\r\nTransactions:\t\t       49999 hits\r\nAvailability:\t\t      100.00 %\r\nElapsed time:\t\t      193.82 secs\r\nData transferred:\t       39.34 MB\r\nResponse tme:\t\t        0.18 secs\r\nTransaction rate:\t      257.97 trans/sec\r\nThroughput:\t\t        0.20 MB/sec\r\nConcurrency:\t\t       46.32\r\nSuccessful transactions:       49999\r\nFailed transactions:\t           1\r\nLongest transaction:\t        1.27\r\nShortest transaction:\t        0.02\r\n```\r\n\r\n</details>\r\n\r\n#### Campaign Gallery query (complex)\r\n|                           | Duration | Throughput | Average | Worst | Comparison                          |\r\n|---------------------------------|----------|------------|---------|-------|:-----------------------------:|\r\n| **Lambda backend** &nbsp;&nbsp;&nbsp;&nbsp; | 21.6m    | 38.6 rps   | 1.28s   | 5.90s |    :trophy:      |                \r\n| **Heroku backend** | 40.4m    | 20.6 rps   | 2.41s   | 9.48s | :small_red_triangle_down: 88% |\r\n\r\n\r\n<details>\r\n<summary>raw <code>postsByCampaignId</code> test results</summary>\r\n\r\n```\r\nsiege -c50 -r1000 --content-type \"application/json\" 'https://rpkpk1dw7b.execute-api.us-east-1.amazonaws.com/qa POST {\"query\":\"query CampaignGalleryQuery($campaignId: String!, $count: Int, $page: Int) {\\npostsByCampaignId(id: $campaignId, count: $count, page: $page) {...PostCard,...ReactionButton,__typename}}\\nfragment PostCard on Post {id,type,status,url,text,tags,quantity,user {firstName,__typename},__typename}\\nfragment ReactionButton on Post {reactions, reacted,  __typename}\",\"variables\":{\"campaignId\":\"5646\",\"count\":12,\"page\":1}}'\r\n\r\nTransactions:\t\t       50000 hits\r\nAvailability:\t\t      100.00 %\r\nElapsed time:\t\t     1295.03 secs\r\nData transferred:\t      159.26 MB\r\nResponse time:\t\t        1.28 secs\r\nTransaction rate:\t       38.61 trans/sec\r\nThroughput:\t\t        0.12 MB/sec\r\nConcurrency:\t\t       49.39\r\nSuccessful transactions:       50000\r\nFailed transactions:\t           0\r\nLongest transaction:\t        5.90\r\nShortest transaction:\t        0.18\r\n\r\n\r\n\r\nsiege -c50 -r1000 --content-type \"application/json\" 'https://graphql-qa.dosomething.org/graphql POST {\"query\":\"query CampaignGalleryQuery($campaignId: String!, $count: Int, $page: Int) {\\npostsByCampaignId(id: $campaignId, count: $count, page: $page) {...PostCard,...ReactionButton,__typename}}\\nfragment PostCard on Post {id,type,status,url,text,tags,quantity,user {firstName,__typename},__typename}\\nfragment ReactionButton on Post {reactions, reacted,  __typename}\",\"variables\":{\"campaignId\":\"5646\",\"count\":12,\"page\":1}}'\r\n\r\nTransactions:\t\t       49999 hits\r\nAvailability:\t\t      100.00 %\r\nElapsed time:\t\t     2423.95 secs\r\nData transferred:\t      159.26 MB\r\nResponse time:\t\t        2.41 secs\r\nTransaction rate:\t       20.63 trans/sec\r\nThroughput:\t\t        0.07 MB/sec\r\nConcurrency:\t\t       49.77\r\nSuccessful transactions:       49999\r\nFailed transactions:\t           1\r\nLongest transaction:\t        9.48\r\nShortest transaction:\t        0.15\r\n```\r\n\r\n\r\n</details>\r\n<br>\r\n\r\nIn both cases, the Lambda function is able to handle significantly more load than our Heroku dyno, without any human intervention to scale up resources. The gallery query hits a performance boundary based on Rogue & Northstar's capacity, but still performs nearly twice as well.\r\n\r\nI also looked at the latency distribution for these queries (measured with Apollo Engine), which is harder to compare side-by-side but still somewhat interesting: [`topic` (Heroku)](https://user-images.githubusercontent.com/583202/52143409-5f8b7380-2629-11e9-808e-92379479ce5c.png), [`topic` (Lambda)](https://user-images.githubusercontent.com/583202/52143418-65815480-2629-11e9-9c97-3eca9c33f03e.png), [`postsByCampaignId` (Heroku)](https://user-images.githubusercontent.com/583202/52143394-53071b00-2629-11e9-8d67-9c321111abc5.png), [`postsByCampaignId` (Lambda)](https://user-images.githubusercontent.com/583202/52143428-6d40f900-2629-11e9-9c1b-a927358d1983.png). \ud83e\udd13 \r\n\r\n## Conclusion\r\nI'm really happy with how this experiment has turned out so far! In summary:\r\n\r\n**Performance:** There were some massive performance wins. For both light and heavy loads, we roughly doubled the amount of throughput we could handle with this application. Alongside effortless horizontal scaling, we also get the peace-of-mind of pay-per-request pricing, so we don't end up paying for beefy servers we don't use during lulls.\r\n\r\n**Workflow:** I think the workflow for provisioning new serverless apps with Terraform & then deploying them with CI works really smoothly, and has some nice ergonomic & security benefits over how the Serverless framework does things: no credentials on local machines, deploy credentials are limited to _just_ deploying, and consolidating our infrastructure into one tool.\r\n\r\n**Bonus:** I tried out [CircleCI](https://circleci.com) as a continuous integration tool here (instead of Wercker or Jenkins). I'll make a separate issue to track progress on that, but the **tl;dr** is that I'd love to get us on a more industry-standard CI service & be able to move more of our jobs (including scheduled jobs, like DB refreshes) into code. None of the work done in this experiment is _inherently_ tied to CircleCI, though.\r\n\r\nWith this in mind, I think it's worth thinking about where else we could use Serverless to simplify our infrastructure. Two places that come to mind are Blink (which is the poster child for uneven traffic spikes!) and Rogue's image formatting endpoint (which has unusually CPU and memory-intensive, but infrequent, work each time it needs to render an uncached image).",
      "comments": [
        "Finally, a checklist of some things that still need work before we can consider this \"shipped\": \r\n\r\n- [x] The `lambda_function` module needs docs, and ideally a high-level \"getting started\" guide.\r\n- [x] Are there changes we'll need to make our Terraform template more flexible for other apps? What about Python? Applications that don't use the generic \"proxy\" route?\r\n- [x] How will custom domains work, especially if we don't have the DNS record purchased in Route 53?\r\n- [x] We need to hook up these log groups to our [Papertrail log forwarder](https://github.com/DoSomething/lambda-papertrail).\r\n- [ ] How should our GraphQL playground & authentication flow work in a serverless world?\r\n- [ ] How should offline development work for these applications? What about when multiple functions are composed into a single app via API Gateway?",
        "I've made some updates to our CircleCI config in DoSomething/graphql#52. The deployment graph is now [a little clearer](https://github.com/DoSomething/graphql/pull/52#issuecomment-460308794), CircleCI automatically loads deploy credentials via Amazon SSM, and deployment logic has been extracted to the [`dosomething/lambda`](https://github.com/DoSomething/lambda-orb) orb so it can be shared between projects.",
        "Probably good to point out _orbs_, which are a CircleCI-specific entity\u2014a modular workflow package:\r\n\r\nhttps://circleci.com/docs/2.0/orb-intro/\r\n\r\n>CircleCI Orbs are shareable packages of configuration elements, including jobs, commands, and executors. CircleCI provides certified orbs, along with 3rd-party orbs authored by CircleCI partners. It is best practice to first evaluate whether any of these existing orbs will help you in your configuration workflow. ",
        "> How will custom domains work, especially if we don't have the DNS record purchased in Route 53?\r\n\r\nIt looks like the answer is \"easily\"! We used a [manually provisioned](https://dosomething.slack.com/archives/C03T8SDJJ/p1549405167032700) ACM wildcard certificate (since it requires us to attach a CNAME to our domain for validation) and are now able to set a custom domain for each app via #133. Each CNAME target is available on the [Custom Domain Names](https://console.aws.amazon.com/apigateway/home?region=us-east-1#/custom-domain-names) page.",
        "> We need to hook up these log groups to our [Papertrail log forwarder](https://github.com/DoSomething/lambda-papertrail).\r\n\r\nThis is done in #134, and we can now easily forward any other function's logs using the existing forwarder function and the new (optional) `logger` parameter on the `lambda_function` module.",
        "> The `lambda_function` module needs docs, and ideally a high-level \"getting started\" guide.\r\n\r\nI added a \"getting started\" guide for this (and demo'd at Sprint Retro) in #137, and wrote up more detailed usage instructions for the `lambda_function` and `api_gateway_proxy` modules in #143.",
        "> Are there changes we'll need to make our Terraform template more flexible for other apps? What about Python?\r\n\r\nAdded support for `python2.7` and `python3.7` runtimes in #143 as well.",
        "> Applications that don't use the generic \"proxy\" route?\r\n\r\nFinally, tackled this one in #146 by creating a generic `api_gateway` module which can accepts a list of function & path mappings (so a single \"app\" could be composed from a bunch of lambdas).",
        "Fixed up some tiny bugeroos in https://github.com/DoSomething/graphql/commit/b6f2db1b93ab9f7b91b44bba1bfcef2a1f98ec4f, https://github.com/DoSomething/graphql/pull/58, and https://github.com/DoSomething/infrastructure/pull/148. I think we're ready to swap these in place of the Heroku backends on dev/QA & see how it goes! :v:",
        "Deleted the old Heroku/Fastly resources for this application in #157, since things have been happily running on Lambda for around a week now. If we need to spin this back up in the future, it should be as easy as reverting that pull request.",
        "> How should our GraphQL playground & authentication flow work in a serverless world? \r\n\r\nI've taken a stab at this in DoSomething/northstar#840.\r\n\r\n>  How should offline development work for these applications? What about when multiple functions are composed into a single app via API Gateway?\r\n\r\nI'd looked into [serverless-offline](https://github.com/dherault/serverless-offline), which unfortunately requires a Serverless framework template, and [localstack](https://github.com/localstack/localstack) which seemed promising, but again didn't have a clear way to \"wire things up\".\r\n\r\nIn the meantime, I made some cleanups for consistency between local dev and production in DoSomething/graphql#62 so this is more consistent for developers.",
        "Since this experiment is effectively wrapped up, I'm going to close this issue out!"
      ]
    },
    "codes": [
      "saving",
      "billing_mode"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/DoSomething/infrastructure/issues/139",
    "content": {
      "title": "Standardize how we use continuous integration in projects.",
      "body": "We currently use [Wercker][1] and three(!!) separate [Jenkins][2] instances to run tests on pull requests, automatic & manual deploys, and scheduled jobs (like database refreshes). It would be nice to standardize how we handle this across the board.\r\n\r\n### Requirements\r\nThere are a few requirements I think our CI system must satisfy:\r\n- We **must be able to author jobs and workflows in code**. This is important for visibility, tracking changes over time, and ensuring that changes go through a standard review process.\r\n- We need support for jobs that trigger on pull requests, commits to `master`, and scheduled tasks.\r\n- We don't want to manage \"raw\" infrastructure ourselves (e.g. being responsible for securing or updating software on our own Jenkins instance).\r\n\r\nIdeally, we'd also satisfy these requirements:\r\n- The CI tool should run jobs in Docker containers so we don't have to worry about environment inconsistencies, like updating a dependency used by one project breaking another.\r\n- We should not be the only organization using the service. (Seriously, who else uses Wercker?)\r\n- Ideally, we could use a free/open-source (or low-cost) plan.\r\n\r\n### Options\r\n\r\nWe currently use [Wercker][1] and [Jenkins][2]. I think we can rule out Jenkins, since there doesn't seem to be a good \"hosted\" option (and maintenance for these instances is a continuous drain on our engineering bandwidth). Wercker has met our needs in the past, but it seems to have stagnated under the Oracle umbrella & I've never run into anyone else who uses it.\r\n\r\nSome other options that are worth investigating are [CircleCI][3], [Drone.io][5], and [Travis CI][4].\r\n\r\n[1]: https://www.oracle.com/corporate/acquisitions/wercker/\r\n[2]: https://jenkins.io\r\n[3]: https://circleci.com\r\n[4]: https://travis-ci.org\r\n[5]: https://drone.io",
      "comments": [
        "Seriously, [who uses Wercker?](https://stackshare.io/wercker/in-stacks) I don't recognize any of those companies.",
        "One note from a nonprofit CTO re: CircleCI vs Drone:\r\n\r\n>We are in the process of moving from CircleCI to Drone right now, primarily due to costs. No strong opinions, because CircleCI has worked well and our experience in Drone.io is relatively new. Our costs on Circle went up by about 8x or so with their new pricing model for larger containers and based on our volume. We previously had a great discount.\r\n\r\nGood flag to watch costs carefully as we try adding workloads to Circle. Jenkins, for all its shortcomings, is free, and we've never had to think twice about setting up a job on there.",
        "For that team ^^^, Drone costs \"$0 for us as a non-profit and open source codebase.\"\r\n\r\nFrom another org, a moderate endorsement of Travis:\r\n\r\n>We were also part of the group that had to find a replacement for Wercker and we went the Travis route. So far nothing to complain about.",
        "Going to close this issue since we no longer use Wercker and have shifted our now consolidated repositories to use CircleCI (although we still use Jenkins a bit for database refreshes!)."
      ]
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/hashicorp/terraform-azurerm-terraform-enterprise/issues/71",
    "content": {
      "title": "Option for private deployment",
      "body": "I just finished a deployment and it is failing to install.  It is hanging on healthstatus from a public IP.  We don't have public IPs as a workable option in our private environment with our hub/spoke model and User Default Routes forcing tunneling traffic.  I am not sure how difficult it would be to eliminate Public IPs and use the Private IPs instead.  There should also be an option which would utilized Private DNS with an option to disable it.  With have a 3rd party solution.  I am happy to work on code.  If there are prior thoughts or plans please share.",
      "comments": [
        "Challenges\r\n- LB need to have a type/count solution to pick between the different type of frontend configurations\r\n- a subnet needs to be passed in -- should provide the option of which subnet",
        "One serious challenge we've encountered with trying to accomplish this thing is Azure Internal Load Balancers have issues if backend nodes send traffic to the load balancer and get mapped back to themselves. TCP flows break entirely because of how Azure re-writes source IPs. This causes basically everything to fail because the install process involves nodes calling the load balancer to access the Kubernetes API server. For the first primary VM, this will always get mapped back to itself, stalling the entire installation.\r\n\r\nSo far we've had to resort to using a public LB with an NSG locking down access to only the public IP of the load balancer (effectively only allowing self-referential access from the backend nodes). There are other solutions such as using a scale set of VMs running something like Nginx or HAProxy to act as a reverse proxy but that adds additional cost and complexity.\r\n\r\nReference: https://docs.microsoft.com/en-us/azure/load-balancer/concepts-limitations#limitations",
        "AH I see, I will have to take a second look at this from a security perspective given your locking down at a NSG level.  I might need to adjust my routes for the load balancer.",
        "A quick disclaimer since I don't think I really made it clear before, I'm not directly involved with Hashicorp or maintaining this module. I'm currently trying to implement a deployment of this for a client. The only other workaround we found so far would be to put a cluster of VMs running nginx or HAProxy behind an internal load balancer and use them as a proxy for self-referential traffic. This would require modifying this module considerably and adds additional cost and management overhead.",
        "Hi there! @user hit the nail on the head with one of the challenges we've run into with the Azure implementation re: why it's currently a public LB (and the Google one too, but I think in a slightly different way). \r\n\r\nThere are some options in this area, one of which is HAProxy or similar, and another is something like what is in the refactor of the Google modules where there's a bit of a IPTables \"sandwich\" playing this role: https://github.com/hashicorp/terraform-google-terraform-enterprise/pull/38 (I believe it's the `internal_lb` module). \r\n\r\nWe're looking to do a similar refactor over here very soon which would fix this issue or at very least make it so you could replace it with your own solution in the middle if ours didn't suit the particular situation. ",
        "> So far we've had to resort to using a public LB with an NSG locking down access to only the public IP of the load balancer (effectively only allowing self-referential access from the backend nodes). There are other solutions such as using a scale set of VMs running something like Nginx or HAProxy to act as a reverse proxy but that adds additional cost and complexity.\r\n> \r\n> Reference: https://docs.microsoft.com/en-us/azure/load-balancer/concepts-limitations#limitations\r\n\r\nHow do you access the console then? Assuming you need to have DNS setup for the public IP to point to tfedev.domain do you just setup DNS internally with a different hostname and use subject alternative names to connect?  Are you using azure dns?",
        " @user We are using split-horizon DNS to access the console/app over the internal load balancer using internal AD DNS (not automated unfortunately).\r\n\r\nFor initial testing and debugging, we're also whitelisting our own public IP in the NSG and using the public LB to access the app. That's also our emergency access model if for some reason we can't reach the app over the internal network due to a misconfiguration, outage, etc.",
        "> @user We are using split-horizon DNS to access the console/app over the internal load balancer using internal AD DNS (not automated unfortunately).\r\n\r\nAh, ok so you are using two load balancers?  \r\n\r\nI am trying to read up on the Load Balancer Floating IP and wondering if there is a mechanism to handle this \r\n",
        " @user Can't really advise on your current setup but yes we have two load balancers because it's not possible mix public and private frontends on Azure Load Balancer."
      ]
    },
    "codes": [
      "increase"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/kbst/terraform-kubestack/issues/219",
    "content": {
      "title": "AKS: disable log analytics",
      "body": "It would be nice to be able to disable log analytics for an AKS cluster. As far as I can tell, right now it's not customizable via kubestack",
      "comments": [
        "for more context: we currently log a lot so using log analytics (even though we don't want to) costs us a lot of money, which essentially means we cannot rely on kubestack to manage our infrastructure any more, as it would automatically re-create the analytics for our cluster.",
        "I have no objection against making this configurable using a variable. "
      ]
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Geeks-Academy/terraform/issues/196",
    "content": {
      "title": "Cost reporting in Azure",
      "body": "Reporting costs from Azure to #aws_costs (channel name will change to #costs in future)",
      "comments": [
        "Added, described implementation, and set alarms on [my blog](https://wachulec.me/posts/slack-notifications-on-azure-budget-consumption/)."
      ]
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Geeks-Academy/terraform/issues/225",
    "content": {
      "title": "Create bastion ASG",
      "body": "We have to move application server to private subnet. After that we will not be able to log in to server (no VPN connection - expensive service). Thus, we will create a ASG for creating EC2 instances on demand when logging into application server will be required.",
      "comments": []
    },
    "codes": [
      "networking",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/ministryofjustice/cloud-platform-infrastructure/issues/499",
    "content": {
      "title": "Resize cluster worker nodes",
      "body": "We can use r5.xlarge instances, instead of r5.2xlarge, thereby saving $5,000/month in unnecessary hosting costs.",
      "comments": [
        "This has been done."
      ]
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Eximchain/terraform-aws-quorum-cluster/issues/15",
    "content": {
      "title": "Option to run on dedicated instances",
      "body": "# Problem\r\nRunning on standard EC2 instances runs a risk of introducing vulnerabilities that exploit the fact that the physical hardware is being shared with other virtual machines. [EC2 Dedicated Instances](https://aws.amazon.com/ec2/purchasing-options/dedicated-instances/) seem to solve this problem by running on physical hardware dedicated to a single customer for an additional cost. However, the current terraform infrastructure does not support that.\r\n\r\n# Solution\r\nEach role should have an independent option to run on dedicated instances instead of regular instances.",
      "comments": [
        "Looks straightforward to add in the Terraform config",
        "Just remembered, definitely need vault servers on dedicated instances"
      ]
    },
    "codes": [
      "instance",
      "increase"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Eximchain/terraform-aws-quorum-cluster/issues/5",
    "content": {
      "title": "Bootnodes should be replaceable",
      "body": "# Problem\r\n\r\nWhen specifying bootnodes, quorum nodes use an enode address which includes the IP address of the bootnode. If a bootnode crashes and is replaced, the IP address will change, and an already active node will have no way of updating their bootnodes.\r\n\r\n# Potential Solutions\r\n\r\n- Dynamically update the nodes' list of bootnodes\r\n- Use an Elastic IP for bootnodes (The problem here is that accounts are limited to 5 per region, so that means accepting some limitations on how many networks one account can run at once and how many bootnodes per region they can have)\r\n- Use a NAT gateway or other networking component to allow nodes to refer to a fixed IP while replacing the underlying instances behind the scenes.",
      "comments": [
        "Just suggesting something here. Instead of fixed IP,  the nodes could be using dynamic DNS, so that nodes register their DNS name with a dynamic DNS server, so even if they have different IPs, they could still be contacted by their DNS name. Would that help?",
        "Thanks for the input! Our original issue with that is that geth can't handle enode addresses involving DNS names. However, we talked about it today and @user thinks it will be reasonable to extend geth to handle DNS bootnodes. \r\n\r\nDynamic DNS server might be something to think about, it sounds like it could be cheaper. John and I were just thinking of throwing an ELB in front of each bootnode.  I'm generally in favor of pushing as much load onto AWS as possible, even if it is throwing money at the problem. With that in mind, if Dynamic DNS necessarily means maintaining our own DNS server I might prefer just going with the ELB solution.",
        "For the sake of documentation, here's a thorough write-up of what I've learned about the problem and how to tackle it based on my conversations with Louis & Juan yesterday.  I'm still cleaning up on the [plaintext passwords](https://github.com/Eximchain/terraform-aws-quorum-cluster/issues/14) thing, but I think I've got a handle on this.\r\n\r\nThere seems to be one key unresolved question here: do running nodes need an updated list of bootnodes?  Juan thought they could discover all new peers from each other, so the list only needs to be valid at start time, but Louis thought they always need a list of active bootnodes.  The latter requirement makes the problem a good bit harder, as we need to signal active nodes that they need to update the bootnodes in their supervisor config (*see bottom*).  Assuming the list only needs to be accurate at the start, here are a couple of approaches for ensuring that the list is updated each time a new bootnode is added:\r\n\r\n- Rebuilding an enode address in the [`init-quorum` recovery case](https://github.com/Eximchain/terraform-aws-quorum-cluster/blob/master/packer/instance-scripts/init-quorum.sh#L270) using the new IP, then saving it to Vault/GitHub for other nodes later on\r\n- **Maintain a fixed DNS address for each bootnode, ensure that they end up behind the right one, and have each node's `geth` resolve those addresses.**\r\n\r\nThe former approach would mostly involve modifying that recovery case of the `init-quorum` script, although we would need the script to get write auth for the right Vault/GitHub endpoints.  The latter approach means the addresses stored in Vault/GitHub should never have to change -- this option seems more promising.  \r\n\r\nThe required geth modifications look pretty straightforward.  Those enode URLs ([`enode://[hexUsername]@[IP address]:[TCP port]?discport=[UDP port]`](https://github.com/ethereum/wiki/wiki/enode-url-format)) strictly use IP addresses, but like Louis said, it should just be [a couple lines in the CLI parser](https://github.com/Eximchain/quorum/blob/master/cmd/utils/flags.go#L618) to resolve DNS addresses down to IPs.  \r\n\r\nThat reduces the problem to making sure that we recover the node to the same DNS address each time.  There are a few ways to do that, sounds like an ELB would be one easy way.  I still haven't done much research on that side of it, not sure what the proper solution is.  I heard something about Auto-Scaling Groups, that sounds like fun.\r\n\r\n*Appendix re: Live Updates*\r\nLouis and I explored how we could tell live nodes that they need to update -- one interesting solution is creating a `FRESH_BOOTNODES` ethereum event, then have these nodes run a Python script which listens for it (like we have for [block metrics](https://github.com/Eximchain/terraform-aws-quorum-cluster/blob/master/packer/instance-scripts/block-metrics.py#L86)).  Receiving this event would trigger a failover process that fetches new bootnode addresses and pauses geth ([like here](https://github.com/Eximchain/terraform-aws-quorum-cluster/blob/master/packer/instance-scripts/backup-chain-data.py#L40)) while we rewrite the supervisor config.  If we did want to do that, we'd want to consider building that event straight into the governance -- we don't want random people saying there are new bootnodes, we could ensure that only one of our validator nodes is allowed to emit the event.",
        "How many bootnodes are there?\r\n- Is there a mechanism to \r\n  - tell other bootnodes a particular bootnode is going down?\r\n  - tell all bootnodes a particular quorum node is going down?\r\n\r\nAsking this because my software upgrade mechanism plans to bring down nodes in other to upgrade software and if a particular node is down, bootnodes shouldn't tell other nodes the downed node is available.\r\n",
        "I believe there are 3 bootnodes for each of the 14 supported regions, so 42.\r\n\r\nAs far as I know, we don't have an easy way to detect when a node is going down.  The failure event might be sudden, so we might not be able to call some graceful exit procedure.  \r\n\r\nLouis' advice to me was focus on when nodes are being turned on, and then detecting whether they're new nodes or replacement ones.  We've already got a running process to hook into on the bootup, so it saves us some headaches of determining whether a network participant is really down or not.\r\n\r\nEDIT: @user & @user (Juan?) can comment more on this, but I believe bootnodes just advertise the list of peers which they're currently connected to.  If you kill a node and it stops being connected anywhere, that might automatically solve the advertising problem.",
        " @user - You don't need to worry about bootnodes pointing to dead instances, they may have a best-effort shutdown, but in general, nodes can die without telling the bootnode. The technical basis for this was a public network, so it had to handle such things gracefully.",
        " @user Thanks for the clarification, Louis!",
        "## Update based on Yesterday's Conversation\r\nThe solution is starting to get clearer here.  We need to replace each of our 42 bootnode instance with a Load Balancer + AutoScaling Group.  The ASG will let us say, \"Make sure there's always an instance here\", without having to worry about actually replacing it ourselves.  The LB will give us a static IP address which is attached to the ASG, so all of the failover work happens automagically.  One happy consequence of this strategy is that we don't need to break the enode protocol, as we'll have a static IP dedicated to the bootnode (or really to the current instance acting as a bootnode).\r\n\r\nThere are [3 types of load balancer](https://aws.amazon.com/elasticloadbalancing/details/#details) (application, network, & classic), we want to use the faster, lower-level network balancer which gives us the static IP.  Terraform's documentation for the [network load balancer](https://www.terraform.io/docs/providers/aws/r/lb.html#network-load-balancer) and the [autoscaling group](https://www.terraform.io/docs/providers/aws/d/autoscaling_groups.html) are pretty good, still getting acquainted with how we specify everything.\r\n\r\nOne wrinkle is that the IP is now known by the the LB, rather than by the instance.  The LB needs to somehow tell the instance what address it's sitting behind. One good thing is that the bootnode doesn't need to know that IP value until it wants to start advertising its enode address, so we might be able to turn on the bootnode and have it fetch that value before actually initializing `geth`.\r\n\r\nOne way to potentially make this design a little cheaper is to use one LB per region which has at least three availability zones, then tie each AZ to a bootnode.  Technically, the network LB gives you one static IP per each AZ.  If we wanted to save on LB's, we could do some fancy work that only initializes a second one if there aren't three AZs in the given region.  That said, it does introduce some complication to the process, having to track which ASG is tied to which AZ and preserving all of those hookups within one LB -- we should quantify the cost of this LB+ASG per each bootnode strategy and see how much we'd really save by reducing our LB count.\r\n\r\n- [This commit](https://github.com/Eximchain/terraform-aws-quorum-cluster/commit/839b4c70c18164efa2c32af0e8b9026f55a26ff2) covers how Louis converted non-bootnode instances to use ASGs.\r\n- [This line](https://github.com/Eximchain/terraform-aws-quorum-cluster/blob/master/terraform/modules/quorum-vault/main.tf#L147) shows how to tell the terraform module which ASG it needs to connect to.\r\n\r\n*All that aside, I'm still cleaning up my big update PR -- just wanted to document our conversation from yesterday in someplace better than a Sublime note*",
        "That PR got merged in on Tuesday, resolving this issue is now my top priority.  Working on it in the [lb-asg-bootnodes](https://github.com/john-osullivan/terraform-aws-quorum-cluster-johno/tree/lb-asg-bootnodes) branch of my fork.",
        "John, if the IP is that of the LB and not the instance, my upgrader would have trouble upgrading the software. Would you be able to provide the IP, or DNS name for the instance?",
        "Glad you pointed the issue out! The question is when & how you want to\nretrieve the instance's IP. The instance will eventually be replaced over\ntime by the autoscaling group, so we need the LB to give us a static IP.\n\nI don't know exactly how, but we might get the IP of the currently\navailable instance by calling the AWS API from your updater at runtime.\n\nHow does this load balancer break the updater? Louis might also have more\ninsight than me into how LBs, ASGs, and EC2 instances are all interacting\nhere\n\nOn Mon, Jul 9, 2018, 22:54 EximChua <notifications@github.com> wrote:\n\n> John, if the IP is that of the LB and not the instance, my upgrader would\n> have trouble upgrading the software. Would you be able to provide the IP,\n> or DNS name for the instance?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/Eximchain/terraform-aws-quorum-cluster/issues/5#issuecomment-403683291>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADpNtdgr9f0Bl8wpHAKqCOGAt905_ispks5uFBdJgaJpZM4RjY5p>\n> .\n>\n",
        "The upgrader uses either IP address or DNS to locate & update the software on the machines.\r\n\r\nIt uses SSH to connect to the target machines, and then *nix Shell Copy (scp) to transfer files.\r\n\r\nIf there's a LB behind 2 or more nodes, then the upgrader can only update the node the LB is redirecting to. The other nodes wouldn't be updated until the LB redirects to them.",
        "I'm not actually convinced there's a problem here. Is there a reason we can't use the LB DNS/IP within the network for making connections and the direct DNS privately for doing updates?",
        "Also @user can we just have terraform + user-data fill the Load Balancer IP or DNS into a data file? I don't think that would force a circular dependency...",
        " @user Yup, I'm getting the load balancer's DNS by writing it to a data file.  The DNS is available as an attribute of `aws_lb`, but the IP isn't -- that's why I'm writing an additional script to resolve the DNS to an IP in `init-bootnode.sh`.\r\n\r\nAlso, @user , we might be alright here because we're designing the system such that each load balancer gets its own node.  Every bootnode will get one load balancer which points at one autoscaling group, and the autoscaling group has size one.  We aren't actually trying to balance load across many machines, just want to ensure that we have a static IP which will always be pointing to some machine.\r\n\r\nIf you get the guarantee that each LB DNS only points to one machine, does that solve your problem?  Note that the specific machine might change over time as dead instances are replaced, so we definitely need that verification code which checks whether a machine has gotten an upgrade.",
        "I'm totally okay with guaranteeing that we have at most one machine per load balancer.\r\n\r @user I'm wondering if maybe there's an AWS CLI call you can make that will get the IPs for a load balancer. We can grant permission to call it to the IAM role the instances use.\r\n\r\nWe can also try a workaround like [this issue](https://github.com/terraform-providers/terraform-provider-aws/issues/3007) suggests.\r\n\r\nAnd regarding the update mechanism, I expect this to reduce to the general problem of making sure replaced instances run the right versions. We definitely do need that, but I also think that will cover us",
        " @user If there is a guarantee that each LB DNS points to one machine (and there are no other machines that needs to be updated), then there's no problem.",
        "Based on some further research that happened yesterday, I'm now swapping out the LB in this solution for an elastic IP address.  \r\n\r\nIt turns out that none of the load balancer options support UDP, which is required for communication between nodes.  That's a hard blocker, and this [six year old issue](https://forums.aws.amazon.com/thread.jspa?threadID=82220) has a direct response from an Amazon rep saying that ELB does not support UDP.  It seems like there's probably a technical reason under the hood, rather than just time constraints, as people have left comments requesting it as recently as October 2017 to no avail.\r\n\r\nDid some research, and the happy outcome is that using elastic IPs ends up being a cleaner solution.  We don't have to spin up as many resources, and the security group rules don't need to be duplicated as LB listeners.  Here's the rundown:\r\n\r\n1. Terraform creates a number of elastic IPs equal to the number of bootnode ASGs required in every region\r\n2. Each bootnode ASG gets its own `user_data` script which includes the public IP and its allocation ID.\r\n3. When the ASG spins up a node and runs `init-bootnode.sh`, the following line (taken from [this StackOverflow question](https://stackoverflow.com/questions/34849360/how-to-assign-eip-to-autoscaling-group-of-vpc-in-cloudformation-template)) will connect the new node to its EIP.  The `--allow-reassociation` option ensures that when a new node gets spun up later and runs the same command, it is allowed to claim the EIP.\r\n\r\n`aws ec2 associate-address --instance-id $INSTANCE_ID --allocation-id $EIP_ID --allow-reassociation` \r\n\r\nOne constraint on this solution is that by default, AWS only lets you reserve 5 elastic IPs per region.  This doesn't get in our way, as we only want 3 bootnodes in each region, but if somebody configures a network with >5 bootnodes in a region, they'll run into issues and have to directly request more from Amazon.  \r\n\r\nTo remedy this issue, I'm making the elastic IP functionality toggled by a boolean variable which defaults to false.  If endusers don't use EIPs, then they'll need to figure out their own strategies for updating bootnode addresses, but that's acceptable.  I'll make sure to describe the behavior in some documentation somewhere.\r\n\r\n*This was covered on the morning call, just want to document the strategy for future reference.*",
        "This issue is being wrapped up now over in #29  ",
        "Merged in #29 "
      ]
    },
    "codes": [
      "domain",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Eximchain/terraform-aws-quorum-cluster/issues/32",
    "content": {
      "title": "Regionalize Backup Procedure",
      "body": "# Problem\r\n\r\nCurrently, the S3 backup procedure implemented in [this script](https://github.com/Eximchain/terraform-aws-quorum-cluster/blob/master/packer/instance-scripts/backup-chain-data.py) currently backs up the entire network to a single S3 bucket. This results in backups costing more money than they need to, since we pay for cross-region traffic. In particular, restoration will involve downloading the full chain data from every node.\r\n\r\n# Acceptance Criteria\r\n\r\n- There is an S3 backup bucket in each region there are quorum nodes\r\n- Nodes back up to and restore from their own region's bucket by default\r\n- There is a command line option to override the backup bucket name, in case we want to force use of another region's bucket.",
      "comments": [
        "**Update: This turned out to be much quicker than expected, been in a good flow and was able to just crank it out.  Let me know if this PR (#34 ) hits the specs you were looking for, @user .**\r\n\r\nAddressing this over on the [regionalized-backups](https://github.com/john-osullivan/terraform-aws-quorum-cluster-johno/tree/regional-backups) branch of my personal repo.  Got the command line overwrite option in there, putting the new backup bucket into `terraform/quorum-cluster-region/main.tf`.\r\n\r\n## Design ?s\r\n- Should I make the existing `data_backup_bucket` variable to `quorum-cluster_region` overwrite the automatic value for the region?  Like making override an optional feature, something you can specify for the whole network? If not, any reason not to eliminate that variable?",
        "Merged the PR"
      ]
    },
    "codes": [
      "area",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Eximchain/terraform-aws-quorum-cluster/issues/22",
    "content": {
      "title": "Integrate with EFS for storage",
      "body": "# Problem\r\n\r\nOur nodes need to store the entire blockchain in storage, and the chain grows over time. It would be extremely expensive to provision all the space we will need to run our network for as long as we expect to need to.\r\n\r\n# Proposed Solution\r\n\r\nWe will need to start with a moderate amount of space (~250 GB) and have a plan in place to upgrade our storage space when it runs short, without taking down the whole network at once.",
      "comments": [
        "We're planning on integrating with EFS to give us elastic storage that can be shared across a region.\r\n\r\nWe hear there's a trick to it and we believe it requires High Throughput mode. Will need to look more into this when we start implementing.",
        "We need to worry about what regions EFS is in.  Good news is they expanded a LOT since I was there. Bad news is they're not quite everywhere.\r\n\r\nEFS is **NOT** available in the following regions:\r\n\r\n- `ap-south-1`\r\n- `ca-central-1`\r\n- `eu-west-2`\r\n- `sa-east-1`\r\n\r\nI also haven't confirmed that High Throughput is available in all those regions yet. I think it will be but I can't say I'm sure.\r\n\r\nWe need to decide what to do about those regions. We can:\r\n\r\n- Disable those regions completely for the tool\r\n- Have those regions fall back on just using a local disk, and possibly not use those regions in our network\r\n- Make the EFS integration completely optional, and fall back on local disk"
      ]
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/bitraf/infrastructure/issues/12",
    "content": {
      "title": "heim: synchronize home directories with dropbox",
      "body": "let members transfer files using dropbox, just like the various computers in the space. this will make it easier to use heim for people who are not familiar with ssh or sftp\r\n\r\nmembers would have to rename their personal directories to match their usernames for this to work\r\n\r\nthe bitraf dropbox account only takes up around 16 GB of space now, which can be easily accommodated on bite. if it was to grow larger than the available space on the ssds in bite, then we could add a large, cheap spinning hard drive. redundancy wouldn't be a concern since the data is stored on dropbox's servers\r\n\r\n- [ ] run the dropbox client in `/dropbox`\r\n\r\n- [ ] have symbolic links like so:\r\n    - `/home/mastensg/dropbox` -> `/dropbox/mastensg`\r\n    - `/home/mastensg/www` -> `/dropbox/mastensg/heim_www`",
      "comments": [
        "i am experimenting with the dropbox daemon on one of my own virtual machines\r\n\r\nit's easy to install:\r\n```\r\ncd ~ && wget -O - \"https://www.dropbox.com/download?plat=lnx.x86_64\" | tar xzf -\r\n```\r\n\r\nsee https://www.dropbox.com/install-linux"
      ]
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/sassoftware/viya4-iac-gcp/issues/79",
    "content": {
      "title": "exceeded quota: gke-resource-quotas when deploying on a GKE cluster created with the iac-gcp tool",
      "body": "Hello\r\nI'd like to report an issue that I'm seeing when using the viya4-deployment container to deploy viya in a GKE cluster and I see a task failure that seems related to the quotas. \r\nNote that the same issue happens when I'm doing a manual deployment. (I created the cluster with the iac tool).\r\n\r\n```\r\nTASK [vdm : Deploy Manifest] *************************************************************************\r\nfatal: [localhost]: FAILED! => {\"changed\": true, \"cmd\": \"kubectl --kubeconfig /config/kubeconfig apply-n viya4gcp --selector=\\\"sas.com/admin=namespace\\\" -f /data/raphpoumarv4fa-gke/viya4gcp/site.yaml --prune\\n\", \"delta\": \"0:01:55.948360\", \"end\": \"2021-05-07 07:51:51.920042\", \"msg\": \"non-zero return code\"          , \"rc\": 1, \"start\": \"2021-05-07 07:49:55.971682\", \"stderr\": \"Error from server (Forbidden): error when  creating \\\"/data/raphpoumarv4fa-gke/viya4gcp/site.yaml\\\": ingresses.networking.k8s.io \\\"sas-model-publish\\\" is forbidden: exceeded quota: gke-resource-quotas, requested: count/ingresses.networking.k8s.io          =1, used: count/ingresses.networking.k8s.io=100, limited: count/ingresses.networking.k8s.io=100\\nError           from server (Forbidden): error when creating \\\"/data/raphpoumarv4fa-gke/viya4gcp/site.yaml\\\": ingress          es.networking.k8s.io \\\"sas-model-repository\\\" is forbidden: exceeded quota: gke-resource-quotas, reque          sted: count/ingresses.networking.k8s.io=1, used: count/ingresses.networking.k8s.io=100, limited: count          /ingresses.networking.k8s.io=100\\nError from server (Forbidden): error when creating \\\"/data/raphpouma          rv4fa-gke/viya4gcp/site.yaml\\\": ingresses.networking.k8s.io \\\"sas-model-studio-app\\\" is forbidden: exceeded quota: gke-resource-quotas, requested: count/ingresses.networking.k8s.io=1, used: count/ingresse          s.networking.k8s.io=100, limited: count/ingresses.networking.k8s.io=100\\nError from server (Forbidden)          : error when creating \\\"/data/raphpoumarv4fa-gke/viya4gcp/site.yaml\\\": ingresses.networking.k8s.io \\\"s          as-natural-language-conversations\\\" is forbidden: exceeded quota: gke-resource-quotas, requested: coun          t/ingresses.networking.k8s.io=1, used: count/ingresses.networking.k8s.io=100, limited: count/ingresses          .networking.k8s.io=100\\nError from server (Forbidden): error when creating \\\"/data/raphpoumarv4fa-gke/          viya4gcp/site.yaml\\\": ingresses.networking.k8s.io \\\"sas-natural-language-generation\\\" is forbidden: ex          ceeded quota: gke-resource-quotas, requested: count/ingresses.networking.k8s.io=1, used: count/ingress          es.networking.k8s.io=100, limited: count/ingresses.networking.k8s.io=100\\nError from server (Forbidden          ): error when creating \\\"/data/raphpoumarv4fa-gke/viya4gcp/site.yaml\\\": ingresses.networking.k8s.io \\\"          sas-natural-language-understanding\\\" is forbidden: exceeded quota: gke-resource-quotas, requested: cou          nt/ingresses.networking.k8s.io=1, used: count/ingresses.networking.k8s.io=100,\r\n```\r\n\r\nIt seems to be a transient issue as it disappears when I run the docker `viya4-deployment  --tags \"baseline,viya,install\"` a second time.\r\nThanks\r\nRaphael",
      "comments": [
        " @user this is a provider quota issue and are in place with all providers. You'll need to request an increase. It's different for each provider. For GCP you can read about it here: https://cloud.google.com/docs/quota there is also a link to follow that talks about adjustments. These default upper limit is set by the provider or imposed by SAS to help keep costs down. ",
        "Hi @user \r\naccording to the error message, it seems to be a resource quota issue : [https://cloud.google.com/kubernetes-engine/quotas#resource_quotas](https://cloud.google.com/kubernetes-engine/quotas#resource_quotas) \r\n\r\nWhat is weird is that when I run the command to check my quota values, it seems that I could create up to 5000 ingresses:\r\n\r\n```sh\r\n[cloud-user@pdcesx03206 PSGEL255-deploying-viya-4.0.1-on-kubernetes]$ kubectl get resourcequota gke-resource-quotas -o yaml --namespace viya4gcp\r\napiVersion: v1\r\nkind: ResourceQuota\r\nmetadata:\r\n  creationTimestamp: \"2021-05-10T10:59:05Z\"\r\n  managedFields:\r\n  - apiVersion: v1\r\n    fieldsType: FieldsV1\r\n    fieldsV1:\r\n      f:spec:\r\n        f:hard:\r\n          .: {}\r\n          f:count/ingresses.extensions: {}\r\n          f:count/ingresses.networking.k8s.io: {}\r\n          f:count/jobs.batch: {}\r\n          f:pods: {}\r\n          f:services: {}\r\n    manager: resourcequota\r\n    operation: Update\r\n    time: \"2021-05-10T10:59:05Z\"\r\n  - apiVersion: v1\r\n    fieldsType: FieldsV1\r\n    fieldsV1:\r\n      f:status:\r\n        f:used:\r\n          f:count/jobs.batch: {}\r\n          f:pods: {}\r\n          f:services: {}\r\n    manager: kube-apiserver\r\n    operation: Update\r\n    time: \"2021-05-10T11:01:01Z\"\r\n  - apiVersion: v1\r\n    fieldsType: FieldsV1\r\n    fieldsV1:\r\n      f:status:\r\n        f:hard:\r\n          f:count/ingresses.extensions: {}\r\n          f:count/ingresses.networking.k8s.io: {}\r\n          f:count/jobs.batch: {}\r\n          f:pods: {}\r\n          f:services: {}\r\n        f:used:\r\n          f:count/ingresses.extensions: {}\r\n          f:count/ingresses.networking.k8s.io: {}\r\n    manager: kube-controller-manager\r\n    operation: Update\r\n    time: \"2021-05-10T12:57:40Z\"\r\n  name: gke-resource-quotas\r\n  namespace: viya4gcp\r\n  resourceVersion: \"69691\"\r\n  selfLink: /api/v1/namespaces/viya4gcp/resourcequotas/gke-resource-quotas\r\n  uid: 58853389-76f4-4e74-978e-eb9d25f0ee90\r\nspec:\r\n  hard:\r\n    count/ingresses.extensions: 5k\r\n    count/ingresses.networking.k8s.io: 5k\r\n    count/jobs.batch: 10k\r\n    pods: 5k\r\n    services: \"1500\"\r\nstatus:\r\n  hard:\r\n    count/ingresses.extensions: 5k\r\n    count/ingresses.networking.k8s.io: 5k\r\n    count/jobs.batch: 10k\r\n    pods: 5k\r\n    services: \"1500\"\r\n  used:\r\n    count/ingresses.extensions: \"0\"\r\n    count/ingresses.networking.k8s.io: \"0\"\r\n    count/jobs.batch: \"1\"\r\n    pods: \"141\"\r\n    services: \"166\"\r\n```\r\n\r\n but I get the error message after only 100 ingresses objects : \r\n\r\n```sh\r\n\"Error : exceeded quota: gke-resource-quotas, requested: count/ingresses.networking.k8s.io=1, used: count/ingresses.networking.k8s.io=100, limited: count/ingresses.networking.k8s.io=100\"\r\n```\r\n\r\nDo you know if anyone else saw the same error ?\r\nThanks",
        "I did run into this, see this thread:\nMatthias Ender: I just ran into the issue described here: google kubernetes engine ... ***@***.***/1618403667655?tenantId=b1c14d5c-3625-45b3-a430-9552373a0c2f&groupId=b88e8d40-08ab-45bb-9990-96459f0fda6d&parentMessageId=1618403667655&teamName=R%26D%20Viya%20Next&channelName=GCP%20Liaisons&createdTime=1618403667655>\nposted in R&D Viya Next / GCP Liaisons at Wednesday, April 14, 2021 8:34:27 AM\n\n\n\nFrom: raphaelpoumarede ***@***.***>\nSent: Monday, May 10, 2021 9:19 AM\nTo: sassoftware/viya4-iac-gcp ***@***.***>\nCc: Subscribed ***@***.***>\nSubject: Re: [sassoftware/viya4-iac-gcp] exceeded quota: gke-resource-quotas when deploying on a GKE cluster created with the iac-gcp tool (#79)\n\n\nEXTERNAL\n\nHi @thpang<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fthpang&data=04%7C01%7Cmatthias.ender%40sas.com%7C3bb4b031b7124306f1c608d913b63135%7Cb1c14d5c362545b3a4309552373a0c2f%7C0%7C0%7C637562495492603527%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=%2FBeGmkGObm%2FH7kR3dfo0oGq2ULRpXinTyvzeLiwxcxk%3D&reserved=0>\naccording to the error message, it seems to be a resource quota issue : https://cloud.google.com/kubernetes-engine/quotas#resource_quotas<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fcloud.google.com%2Fkubernetes-engine%2Fquotas%23resource_quotas&data=04%7C01%7Cmatthias.ender%40sas.com%7C3bb4b031b7124306f1c608d913b63135%7Cb1c14d5c362545b3a4309552373a0c2f%7C0%7C0%7C637562495492613521%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=doc2ytMbL5zgVzwCEb85kA3iDJL8qxy%2FMPuj5MteA%2Bk%3D&reserved=0>\n\nWhat is weird is that when I run the command to check my quota values, it seems that I could create up to 5000 ingresses:\n\n***@***.*** PSGEL255-deploying-viya-4.0.1-on-kubernetes]$ kubectl get resourcequota gke-resource-quotas -o yaml --namespace viya4gcp\n\napiVersion: v1\n\nkind: ResourceQuota\n\nmetadata:\n\n  creationTimestamp: \"2021-05-10T10:59:05Z\"\n\n  managedFields:\n\n  - apiVersion: v1\n\n    fieldsType: FieldsV1\n\n    fieldsV1:\n\n      f:spec:\n\n        f:hard:\n\n          .: {}\n\n          f:count/ingresses.extensions: {}\n\n          f:count/ingresses.networking.k8s.io: {}\n\n          f:count/jobs.batch: {}\n\n          f:pods: {}\n\n          f:services: {}\n\n    manager: resourcequota\n\n    operation: Update\n\n    time: \"2021-05-10T10:59:05Z\"\n\n  - apiVersion: v1\n\n    fieldsType: FieldsV1\n\n    fieldsV1:\n\n      f:status:\n\n        f:used:\n\n          f:count/jobs.batch: {}\n\n          f:pods: {}\n\n          f:services: {}\n\n    manager: kube-apiserver\n\n    operation: Update\n\n    time: \"2021-05-10T11:01:01Z\"\n\n  - apiVersion: v1\n\n    fieldsType: FieldsV1\n\n    fieldsV1:\n\n      f:status:\n\n        f:hard:\n\n          f:count/ingresses.extensions: {}\n\n          f:count/ingresses.networking.k8s.io: {}\n\n          f:count/jobs.batch: {}\n\n          f:pods: {}\n\n          f:services: {}\n\n        f:used:\n\n          f:count/ingresses.extensions: {}\n\n          f:count/ingresses.networking.k8s.io: {}\n\n    manager: kube-controller-manager\n\n    operation: Update\n\n    time: \"2021-05-10T12:57:40Z\"\n\n  name: gke-resource-quotas\n\n  namespace: viya4gcp\n\n  resourceVersion: \"69691\"\n\n  selfLink: /api/v1/namespaces/viya4gcp/resourcequotas/gke-resource-quotas\n\n  uid: 58853389-76f4-4e74-978e-eb9d25f0ee90\n\nspec:\n\n  hard:\n\n    count/ingresses.extensions: 5k\n\n    count/ingresses.networking.k8s.io: 5k\n\n    count/jobs.batch: 10k\n\n    pods: 5k\n\n    services: \"1500\"\n\nstatus:\n\n  hard:\n\n    count/ingresses.extensions: 5k\n\n    count/ingresses.networking.k8s.io: 5k\n\n    count/jobs.batch: 10k\n\n    pods: 5k\n\n    services: \"1500\"\n\n  used:\n\n    count/ingresses.extensions: \"0\"\n\n    count/ingresses.networking.k8s.io: \"0\"\n\n    count/jobs.batch: \"1\"\n\n    pods: \"141\"\n\n    services: \"166\"\n\nbut I get the error message after only 100 ingresses objects :\n\n\"Error : exceeded quota: gke-resource-quotas, requested: count/ingresses.networking.k8s.io=1, used: count/ingresses.networking.k8s.io=100, limited: count/ingresses.networking.k8s.io=100\"\n\nDo you know if anyone else saw the same error ?\nThanks\n\n-\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fsassoftware%2Fviya4-iac-gcp%2Fissues%2F79%23issuecomment-836689325&data=04%7C01%7Cmatthias.ender%40sas.com%7C3bb4b031b7124306f1c608d913b63135%7Cb1c14d5c362545b3a4309552373a0c2f%7C0%7C0%7C637562495492613521%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=byP1RWNr7ddpCRsU0ccbsCnN%2FyMg1EvOQBf3Q7XsMI4%3D&reserved=0>, or unsubscribe<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FACUQLJJGGRIRMCAZNC6QBGLTM7MMXANCNFSM44JQEKOA&data=04%7C01%7Cmatthias.ender%40sas.com%7C3bb4b031b7124306f1c608d913b63135%7Cb1c14d5c362545b3a4309552373a0c2f%7C0%7C0%7C637562495492623520%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=RENu4CXmJw1JTKxBW9Bss4IMu7xcYH3XhdV0baQZfSo%3D&reserved=0>.\n",
        "OK so that's interesting.\r\nI did another test and monitored the quota status, by running the \"get resourcequota\" several times during the deployment (and creation of the ingresses).\r\n\r\nThis time I did not see the error message.\r\nIt seems that, by default we start with \"100\" as the limit, but then it seems like after a while it is automatically adjusted to 5k:\r\n\r\n```sh\r\n[cloud-user@pdcesx03206 PSGEL255-deploying-viya-4.0.1-on-kubernetes]$ kubectl get resourcequota gke-resource-quotas -o yaml --namespace viya4gcp\r\napiVersion: v1\r\nkind: ResourceQuota\r\nmetadata:\r\n  creationTimestamp: \"2021-05-10T13:41:29Z\"\r\n  managedFields:\r\n...\r\nspec:\r\n  hard:\r\n    count/ingresses.extensions: 5k\r\n    count/ingresses.networking.k8s.io: 5k\r\n    count/jobs.batch: 10k\r\n    pods: 5k\r\n    services: \"1500\"\r\nstatus:\r\n  hard:\r\n    count/ingresses.extensions: \"100\"\r\n    count/ingresses.networking.k8s.io: **\"100\"**\r\n    count/jobs.batch: 5k\r\n    pods: \"1500\"\r\n    services: \"500\"\r\n  used:\r\n    count/ingresses.extensions: \"0\"\r\n    count/ingresses.networking.k8s.io: **\"61\"**\r\n    count/jobs.batch: \"1\"\r\n    pods: \"141\"\r\n    services: \"166\"\r\n[cloud-user@pdcesx03206 PSGEL255-deploying-viya-4.0.1-on-kubernetes]$ kubectl get resourcequota gke-resource-quotas -o yaml --namespace viya4gcp\r\napiVersion: v1\r\nkind: ResourceQuota\r\nmetadata:\r\n  creationTimestamp: \"2021-05-10T13:41:29Z\"\r\n  managedFields:\r\n...\r\nspec:\r\n  hard:\r\n    count/ingresses.extensions: 5k\r\n    count/ingresses.networking.k8s.io: 5k\r\n    count/jobs.batch: 10k\r\n    pods: 5k\r\n    services: \"1500\"\r\nstatus:\r\n  hard:\r\n    count/ingresses.extensions: \"100\"\r\n    count/ingresses.networking.k8s.io: **\"100\"**\r\n    count/jobs.batch: 5k\r\n    pods: \"1500\"\r\n    services: \"500\"\r\n  used:\r\n    count/ingresses.extensions: \"0\"\r\n    count/ingresses.networking.k8s.io: **\"67\"**\r\n    count/jobs.batch: \"1\"\r\n    pods: \"141\"\r\n    services: \"166\"\r\n[cloud-user@pdcesx03206 PSGEL255-deploying-viya-4.0.1-on-kubernetes]$ kubectl get resourcequota gke-resource-quotas -o yaml --namespace viya4gcp\r\napiVersion: v1\r\nkind: ResourceQuota\r\nmetadata:\r\n  creationTimestamp: \"2021-05-10T13:41:29Z\"\r\n  managedFields:\r\n...\r\nspec:\r\n  hard:\r\n    count/ingresses.extensions: 5k\r\n    count/ingresses.networking.k8s.io: 5k\r\n    count/jobs.batch: 10k\r\n    pods: 5k\r\n    services: \"1500\"\r\nstatus:\r\n  hard:\r\n    count/ingresses.extensions: \"100\"\r\n    count/ingresses.networking.k8s.io: **\"100\"**\r\n    count/jobs.batch: 5k\r\n    pods: \"1500\"\r\n    services: \"500\"\r\n  used:\r\n    count/ingresses.extensions: \"0\"\r\n    count/ingresses.networking.k8s.io: **\"75\"**\r\n    count/jobs.batch: \"1\"\r\n    pods: \"141\"\r\n    services: \"166\"\r\n[cloud-user@pdcesx03206 PSGEL255-deploying-viya-4.0.1-on-kubernetes]$ kubectl get resourcequota gke-resource-quotas -o yaml --namespace viya4gcp\r\napiVersion: v1\r\nkind: ResourceQuota\r\nmetadata:\r\n  creationTimestamp: \"2021-05-10T13:41:29Z\"\r\n  managedFields:\r\n...\r\nspec:\r\n  hard:\r\n    count/ingresses.extensions: 5k\r\n    count/ingresses.networking.k8s.io: 5k\r\n    count/jobs.batch: 10k\r\n    pods: 5k\r\n    services: \"1500\"\r\nstatus:\r\n  hard:\r\n    count/ingresses.extensions: 5k\r\n    count/ingresses.networking.k8s.io: **5k**\r\n    count/jobs.batch: 10k\r\n    pods: 5k\r\n    services: \"1500\"\r\n  used:\r\n    count/ingresses.extensions: \"96\"\r\n    count/ingresses.networking.k8s.io: **\"157\"**\r\n    count/jobs.batch: \"1\"\r\n    pods: \"141\"\r\n    services: \"166\"\r\n[cloud-user@pdcesx03206 PSGEL255-deploying-viya-4.0.1-on-kubernetes]$\r\n```\r\n\r\nSo my understanding is that this issue can randomly happens depending on the rate of the ingresses object creation. I'm not sure about the best way to avoid it at this point.",
        "Closing as this is related to resource quotas and not the tooling"
      ]
    },
    "codes": [
      "awareness",
      "provider"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/wellcomecollection/goobi-infrastructure/issues/212",
    "content": {
      "title": "Expire objects in S3 export-bagit buckets where appropriate",
      "body": "In order to keep storage costs down and ensure that temporary storage does not grow indefinitely we should expire objects in:\r\n\r\nwellcomecollection-workflow-export-bagit\r\nwellcomecollection-workflow-export-bagit-stage\r\n\r\nAfter 4 days (or about as long as the storage service will ever buffer requests).",
      "comments": [
        "I will add this as well, as I am just working on changing versioning/lifecycling for various buckets wrt #238 ",
        "old bags have indeed been deleted from both buckets"
      ]
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/wellcomecollection/goobi-infrastructure/issues/231",
    "content": {
      "title": "Backup of cloud system",
      "body": "Following components might be considered:\r\n\r\n- RDS DB containing workflow status, harvested processes etc.\r\n- EFS containing METS files \r\n- S3 bucket containing configuration\r\n- S3 bucket containing images etc.\r\n\r\n- List the data stores and whether they are rebuildable from other data\r\n- For DB dumps we should keep a couple of weeks and yes we should replicate into another region (though cost and time are factors we need to know roughly).\r\n- Wellcome would expect us to be able to bring up a staging environment replicate backups into that to demonstrate continuity of service\r\n- Wellcome usually keep images for 15 days before deleting them from Goobi so it'd be good to keep the images in the backup just as long or a little longer\r\n",
      "comments": [
        " @user we want to be reassured and have seen your plans for restoring before we are using AWS Goobi in production",
        "datastores involved\r\n\r\n- RDS databases:\r\n  - **goobi**\r\n    - information stored: account information, workflow status and history\r\n    - other sources: none\r\n  - **harvester**: \r\n    - stored information: information about already harvested processes, repository URLs, past runs\r\n    - other sources: process list could potentially be re-created from storage server or presentation system (DB backup a lot easier)\r\n  - **itm**\r\n    - information stored: status and historical data of ITM jobs\r\n    - other sources: partially from Goobi\r\n- **EFS**:\r\n  - stored information: METS files and potentially additional files of Goobi processes, working storage\r\n  - other sources: many METS files can be re-created from archive storage, but only of finished/exported processes\r\n- S3:\r\n  - **wellcomedigitalworkflow-workflow-configuration**\r\n    - stored information: configuration files for applications, Goobi rulesets etc.\r\n    - other sources: none\r\n  - **wellcomedigitalworkflow-workflow-data**\r\n    - stored information: images etc. of Goobi processes (mainly unfinished/WIP processes)\r\n    - other sources: at least for older unfinished processes probably none (old scanned images e.g.)\r\n  - **wellcomecollection-workflow-infra**\r\n    - stored information: terraform state and vars, lambdas archivematica infra files?\r\n    - other sources: partially none or re-creatable from scratch, lambda from github, terraform variables might need backup\r\n  - wellcomecollection-workflow-export-bagit / workflow-export-bagit-stage\r\n    - stored information: bags for ingests (working storage)\r\n    - other sources: should be re-creatable from Goobi until successfully ingested\r\n  - **workflow-harvesting-results**\r\n    - stored information: harvested MARC files (successfully imported as well as problematic)\r\n    - other sources: re-downloadable from IA, only source for processes that have not been successfully imported\r\n  - workflow-upload\r\n    - stored information: images for upload to Goobi (working storage)\r\n    - other sources: scanning PCs?\r\n\r\nressources to be included in backup are **bold**",
        "For EFS it appears that AWS Backup can be a good solution. Easy to set up, allowing automatic migration to cold storage, restore into the same FS as well as restore as new FS. A regular command line backup tool running as some kind of scheduled job, could also do the job of backing up the files to S3.\r\n\r\nRDS could be backed up using RDS snapshots (increasing the current amount of snapshots saved). A different or even additional way would be a kind of scheduled job doing regular SQL dumps to S3 for example.\r\n\r\nS3 is a bit harder, the highest risk here is probably accidental deletion by a user. This could be caught by enabling versioning and automatically dropping old versions after a defined time.\r\n\r\nThe above measures protect, to a certain limit, against accidental loss of data due to some kind of user or script error for example. For better protection against infrastructure problems, replication of backups into a different region would have to be considered.",
        ">  EFS it appears that AWS Backup can be a good solution \r\n\ud83d\udc4d \r\n> RDS could be backed up using RDS snapshots\r\n\ud83d\udc4d \r\n> Versioned S3\r\n\ud83d\udc4d \r\n\r\nGreat - all sounds good.",
        "That all makes sense - do we need to make each of those mechanisms into a ticket to be worked on?",
        "EFS and RDS are saved daily... restore to a new resource for testing purposes is currently running",
        "restoring RDS/EFS to new resources has been tested and content checked successfully"
      ]
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/wellcomecollection/goobi-infrastructure/issues/322",
    "content": {
      "title": "Make Goobi work with Amazon SQS",
      "body": "This will make things easier (and cheaper) infrastructure-wise. We will use the Amazon JMS bridge.",
      "comments": [
        "Currently the sending of messages to the _FIFO_ queue we use results in an error as the MessageGroupID parameter is missing but required for FIFO queues."
      ]
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/wellcomecollection/goobi-infrastructure/issues/423",
    "content": {
      "title": "Add an S3 lifecycle rule to Standard-IA for objects in the working storage bucket",
      "body": "Currently everything stored in `s3://wellcomedigitalworkflow-workflow-data` is stored in the S3 Standard class, even though the vast majority of it is being accessed very infrequently.\r\n\r\nWe should set up a lifecycle rule to cycle objects to Standard-IA \u2013 this would knock a decent chunk off the cost of the bucket with minimal work required on our part.\r\n\r\nHere's an example of how to configure this in Terraform: https://github.com/wellcomecollection/storage-service/blob/main/terraform/modules/critical/s3_replica_primary.tf#L9-L17",
      "comments": [
        "This has been applied to stage earlier this week and most of the data has been migrated to standard_ia. As that was successful, we have now applied the change to production."
      ]
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/wellcomecollection/goobi-infrastructure/issues/433",
    "content": {
      "title": "production worker node possibly scale down to 0",
      "body": "Currently one instance is always running. We should check if scaling down to 0 is feasible.\r\nActual idle time should be checked as well as costs.",
      "comments": []
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Azure/sap-automation/issues/102",
    "content": {
      "title": "Add support for multi disk LUNS : Production & Cost concious setup",
      "body": "**Is your feature request related to a problem? Please describe.**\r\n\r\nCertified Azure production HANA currently requires 4x disks based LVM for Data and 3x disks based Log volumes ( with Write accelerator enabled ).\r\nIn addition, A cost conscious setup ( Non certified ) setup is typically provisioned in non-production (dev,qa,sbx) environment.\r\nCurrent jinja template (https://github.com/Azure/sap-automation/blob/main/deploy/ansible/roles-os/1.5-disk-setup/templates/volume_groups.j2:16) can only process by creating LVMs comprising of single disks.\r\n\r\nThis has a technical limitation as the stripe size set during LVM creation based on number of participating disks and cannot be changed later without a complete rebuild. It also limits performance to 1x the disk irrespective of how many disks you add into the LVM in future.\r\n\r\n**Describe the solution you'd like**\r\nModify the template https://github.com/Azure/sap-automation/blob/main/deploy/ansible/roles-os/1.5-disk-setup/templates/volume_groups.j2:16 code block to below code\r\n\r\n{%     for disk in disks if (disk.host == inventory_hostname) and (disk.type == disktype) %}\r\n{%        for lun in disk.LUN%}\r\n{%         set _ = pvlist.append('/dev/disk/azure/scsi1/lun' ~ lun) %}\r\n{%        endfor %}\r\n{%     endfor %}\r\n\r\nThis will require TF code be  to modified in  how it outputs the sap-automation.yaml file from the templates.\r\n\r\nA sample disks block will be as below\r\n# disks:\r\n#   - { host: 'hana-01', luns: [0], type: 'usrsap'}\r\n#   - { host: 'hana-01', luns: [1], type: 'hanashared'}\r\n#   - { host: 'hana-01', luns: [11,12,13,14], type: 'hanadata'}\r\n#   - { host: 'hana-01', luns: [21,22,23], type: 'hanalog'}\r\n#   - { host: 'hana-01', luns: [31], type: 'hanabackup'}\r\n\r\nNote the comma separated array / list of iSCSI LUN numbers.\r\n\r\nFor Cost Conscious HANA setup, we could go for 4x disks combined in a single Volume group, which carves out 3 LVM , one each for Shared, Data and Log. An identifier for this in  vars/disk_config.yaml file can be added to allow \r\n\r\n**Additional context**\r\nRecommendations/requests here are based on official documentation\r\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/hana-vm-operations-storage\r\n",
      "comments": []
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/bhavikkumar/terraform-master/issues/10",
    "content": {
      "title": "Setup a centralised logging solution",
      "body": "There is many ways to do this but the first way to try out is to make use of a stream solution such as described in the following links:\r\n* https://aws.amazon.com/blogs/architecture/central-logging-in-multi-account-environments/\r\n* https://www.accenture.com/us-en/blogs/blogs-kiran-gekkula-vpc-flow-logs-analysis-aws\r\n* https://blend.com/blog/community/engineering/centralizing-logs-aws/\r\n\r\nIf the costs of the streaming solution is too expensive then we should look at a more traditional logging solution which might translate into us managing our own infrastructure.\r\n\r\nRegardless of the solution all logs should be in SumoLogic and a S3 bucket with lifecycle policies",
      "comments": [
        "Part of this has been completed in commit 47e02d4f2f9f2877cbf1c74e73192dabe09d424b.\r\n\r\nThe remaining item is to send it to SumoLogic from the Kinesis stream using a Lambda function.",
        "In addition to sending logs to SumoLogic, we also need to store them in a S3 bucket since SumoLogic and Kinesis only hold the data for a limited time."
      ]
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/robinbryce/iona/issues/15",
    "content": {
      "title": "enable google cloud identity",
      "body": "Uses firebase. Its a normal gcp project with extra stuff enabled. Its all payg or free. apps can be created by tf\n\n* https://registry.terraform.io/providers/hashicorp/google/latest/docs/data-sources/firebase_web_app_config\n* https://cloud.google.com/identity-platform/docs/web/google",
      "comments": [
        "logged in via google"
      ]
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/robinbryce/iona/issues/28",
    "content": {
      "title": "google's redis is to expensive. replace with bitnami or ot-container kit",
      "body": "Try\n* https://github.com/OT-CONTAINER-KIT/redis-operator as it is a dynamic monitoring solution that handles both standalone & clustered\n\nBut bitnamis solution is good. The standalone model should be able to tolerate deployment in our pre-emptible scenario\n\nhttps://github.com/bitnami/charts/tree/master/bitnami/redis-cluster\nhttps://github.com/bitnami/charts/tree/master/bitnami/redis",
      "comments": [
        "Don't need redis for cert-manager so can just remove memorystore for now. Didn't use traefik's redis kv in the end, we use the file provider for the minimal bootstrappy things, and k8s CRD's for everything else"
      ]
    },
    "codes": [
      "saving",
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/UrbanOS-Examples/common/issues/423",
    "content": {
      "title": "Update RDS instances to use variable sizing for dev environment",
      "body": "RDS instances are using hardcoded sizes in for all environments.  Make the sizing variable so dev can be a smaller instance size for cost savings.\nThis mostly applies to joomla as it uses a larger database but review other instances also.\n\nExample: joomla multi-az t2.large ~ $200/month, changing to t2.small = ~$50/month\n\nAC: \n\n- [ ] The prod and staging plans have no changes\n\nTech Note: \n\nThis will be deployed to prod ",
      "comments": []
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/UrbanOS-Examples/common/issues/407",
    "content": {
      "title": "What metrics from AWS would be valuable to collect?",
      "body": "As a system administrator, I would like to have access to up-to-date metrics about my hosting environment so that I can quickly respond to issues.\r\n\r\nAcceptance Criteria:\r\n+ List of metrics deemed valuable enough to collect\r\n+ Total cost of those metrics (as multiplied across environments)\r\n\r\nTech Notes:\r\n+ S3 and RDS's are top priorities\r\n+ EC2 usage stats may also be valuable",
      "comments": []
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/fecgov/fec-infrastructure/issues/51",
    "content": {
      "title": "Research on adding more storage space for RDS in AWS",
      "body": "The size allocated for the DEV/STG/PRD database in AWS is 2TB.  Currently the database size is approaching 1.7TB.  More storage space is needed.  Research on how much more space is needed and the cost required.",
      "comments": [
        "We can use the https://calculator.s3.amazonaws.com/index.html to help figure this out.\r\n",
        "The current settings (2 TB with IOPS 13000)\r\n\r\n<img width=\"1134\" alt=\"current_setting\" src=\"https://user-images.githubusercontent.com/24943130/40213735-0e066b58-5a25-11e8-910b-853a8a9c22b7.png\">\r\n",
        "The price with increased storage (2.5 TB with IOPS 15000)\r\n\r\n<img width=\"1120\" alt=\"increase_to_2 5t\" src=\"https://user-images.githubusercontent.com/24943130/40213779-498dc82e-5a25-11e8-86c0-4e75724d05d5.png\">\r\n",
        "The price with increased storage (2.2 TB with IOPS 14000)\r\n\r\n<img width=\"988\" alt=\"increase_to_2 2t\" src=\"https://user-images.githubusercontent.com/24943130/40231650-eaae9fdc-5a69-11e8-8a0f-a4279850e826.png\">\r\n",
        "Above is price calculated by the Amazon calculator. \r\n\r\nThe increase of 200G of space across all the 6 instances, also with the increase storage, more IOPS need to keep similar IOPS to Storage ratio (AWS recommand 3:1 to 10:1 ratio) for the production databases (the dev and stage does not use provisioned IOPS) will increase the monthly payment from $13055.75 to $13683.89 (i.e. $628.24 increase).\r\n\r\nThe increase of 500G of space across all the 6 instances, also with the increase storage, more IOPS need to keep similar IOPS to Storage ratio (AWS recommand 3:1 to 10:1 ratio) for the production databases (the dev and stage does not use provisioned IOPS) will increase the monthly payment from $13055.75 to $14420.56 (i.e. $1364.81 increase).",
        "New size is 2.5 TB"
      ]
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/cookpad/terraform-aws-eks/issues/85",
    "content": {
      "title": "NVMe mounting for ephemeral and docker_volume",
      "body": "If machines have nVME storage, it'd be great to make this available - it's cheap *and* fast (notwithstanding it's ephemeral nature!).",
      "comments": [
        "Looks like we could use this https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner",
        "Came across this relevant discussion particularly related to the ECK (Elastic Cloud on Kubernetes) usecase - https://discuss.elastic.co/t/deploying-eck-on-aws-eks-with-i3-and-d2-instances-and-local-volumes/235223"
      ]
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/reschouw-homelab/terraform-homelab/issues/6",
    "content": {
      "title": "Create NAT instance to bypass cost of NAT Gateway",
      "body": "NAT gateways are expensive for a homelab. Spin up a tiny instance to handle that communication instead",
      "comments": []
    },
    "codes": [
      "networking",
      "saving"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/paperphyte/terraform-drone/issues/2",
    "content": {
      "title": "Optimize Costs ",
      "body": "The Spot Fleet introduced in the [v2.1.0](https://github.com/paperphyte/terraform-drone/releases/tag/v2.1.0) release allows reduced cost for build agents.\r\n\r\nBut now the cost for the Server/User interface become high in comparison.\r\n\r\n#### Targets for cost Reductions\r\n##### The Fargate Container of the server is run behind an ALB.\r\n1 ALB \u2248 20 USD / month \u2248  240 USD / year.\r\n\r\n##### The Database\r\n1 t3.micro \u2248 15 USD / month \u2248 180 USD / year \r\n\r\nIdeas:\r\n\r\n~~RDS Instance Scheduler. Disabling it during nights 02.00 - 08.00 can potentially save 25% / year. Important the hours needs to be flexible~~ (solved with reserved instance pricing)\r\n~~ALB. Get rid of it.~~ (solved with cloudwatch events+lambda to update ip and letsencrypt)\r\n~~Optimize Fargate Container with Event / Time based schedules~~ (not worth cost of implementation)",
      "comments": []
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/paperphyte/terraform-drone/issues/1",
    "content": {
      "title": "Add External Secrets Manager",
      "body": "Implement support for (1) External Secrets manager.\r\n\r\n * AWS Secrets Manager ([Drone plugin](https://github.com/drone/drone-amazon-secrets))\r\n   - Deploy lambda function.\r\n   - Accessible by ECS Drone Agents\r\n    \r\n    pros:\r\n    - cost effective short-term\r\n    - acm is aws service\r\n    cons:\r\n    - package and deploy lambda function\r\n    - potentially costly long-term\r\n    \r\nOR\r\n\r\n * Vault ([Drone plugin](https://github.com/drone/drone-vault))\r\n    - Fargate Container\r\n    - Accessible by ECS Drone Agents\r\n    - Private DNS Service discovery\r\n\r\n   pros:\r\n   - easy to manage with terraform\r\n   - integrates with vault if already used\r\n   cons \r\n   - user needs to configure and run vault \r\n\r\nCosts: \r\n - ACM 0.4 USD / secret + charges for lambda, api calls etc.\r\n - Vault/Fargate \u2248 9 usd / month + hosted zone 0.5 usd + charges for some other service calls.",
      "comments": []
    },
    "codes": [
      "increase",
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/moov-io/infra/issues/1",
    "content": {
      "title": "container hosting vs hosted kubernetes cluster",
      "body": "In order to run our `ach` service (REST api), we need to containerize it and run that container somewhere. There's nothing secret inside the container itself, so we can store that on docker hub. \r\n\r\nThe big question is, do we try and go down the route of a \"magic cloud container runner\" or just spin up a hosted k8s cluster?\r\n\r\nHosted container runners charge per-container-per-second, which might add up. (Or they charge per-node). You don't get visibility into the control pane and often have to run cloud LB's (cost).  \r\n\r\nA hosted k8s cluster lets us deploy and connect containers, with the addition of infra services (grafana, logs). We can run multi-zone and spread across. \r\n\r\nInitially I see the following being deployed:\r\n\r\n - apps: ach/rest (maybe put caddy inside this image) \r\n - db: hosted mysql \r\n - infra: nginx/caddy, grafana, prometheus, ELK/EFK/[oklog](https://github.com/oklog/oklog)\r\n - secrets: vault? cloud KMS? ",
      "comments": [
        "FWIW I'm biased towards a hosted k8s setup. ",
        "Done, see https://github.com/moov-io/infra/issues/6 "
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/moov-io/infra/issues/20",
    "content": {
      "title": "lb: HA setup for traefik",
      "body": "**What were you trying to do?**\r\n\r\nI was trying to load `api.moov.io` (or `infra.moov.io`) resources while the node that traefik was running on had gone away.  This meant that connections were dropped until a new node took over and had the Traefik pod scheduled.\r\n\r\nThere's a problem currently with the PersistentVolume accessMode in that only one Pod can hold a write lock for the volume. What if we just deploy a traefik-alpha and traefik-beta ? That way there are 2 lb's up and running.  \r\n",
      "comments": [
        "Oh, this will require a second `type: LoadBalancer` service, which might cost more? Need to double check on that. "
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/chadgeary/cloudblock/issues/29",
    "content": {
      "title": "IP is leaked",
      "body": "I have set this up and deployed on lightsail. My client phone connects successfully via wireguard. However , my original IP is still being leaked rather than using the lightsail\u2019s IP. I can\u2019t figure out what\u2019s wrong and why the IP leaking is happening? \r\n\r\nI have not changed anything on the instance since cloudblock after deployed.",
      "comments": [
        "hey @user - What value did you set for `vpn_traffic` [here](https://github.com/chadgeary/cloudblock/blob/master/lightsail/aws.tfvars#L23)? This setting configures the wireguard clients that are generated.\r\n\r\nIf you deployed with the value set to `dns` - only your DNS traffic is routed via wireguard VPN.\r\n\r\nIf you want to change the behavior to all traffic, open wireguard on your phone and edit the imported VPN profile. Set the allowed IPs field to `0.0.0.0/0`.",
        "Thanks for the reply @user , I managed to fix it by changing the allowed IPs. \r\n\r\nI may change the vpn_traffic and redeploy rather than manually changing the allowed ips each time!\r\n\r\nalso wondered why create extra s3 resource if we can use the lightsail instance itself to host the files? As it would be an extra cost.",
        "Glad it\u2019s resolved \u2600\ufe0f\r\n\r\nFor S3 it\u2019s a matter of low cost backup and convenience.\r\n\r\nAlong with the client configs, the pihole and wireguard data directories are stored here in the event the instance needs to be rebuilt or exported."
      ]
    },
    "codes": [
      "saving",
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/terraform-google-modules/terraform-example-foundation/issues/241",
    "content": {
      "title": "warn about expensive vpn-gateways / provide alternative",
      "body": "This repo creates about 30 projects. It sets a billing alert for 1k, but the first alert is at 50%. It could be quite easy to accidentally run up a bill to $500 before noticing anything. I'd recommend\r\n1. setting billing alerts that start at $10, $50, $100\r\n2. offering a low-cost alternative to the VPN and Dedicated Interconnect\r\n\r\nI use this repo for testing out migration recommendations to help secure customer accounts. I do not have a Dedicated Interconnect, so i chose the vpn option in 3-networks. However, this launches Gateways. Unlike nearly all the other infra in this repo, that can run up substantial bills in a short period of time. What would be a good alternative to the gateways? Would you consider an interim PR for docs that would adjust the default alerting downward and some short scripts to delete the VPN tunnels when the cluster is not in use?  Currently i'm doing it with gcloud cause I haven't done this sort of surgical deletion with tf before.\r\n",
      "comments": [
        "Thanks for the report @user - If you are not using the VPN right now, perhaps you can just comment out the code and destroy the VPN gateway / tunnels?\r\n\r\nAlso in terms of budgets, you should be able to override the budget amount and threshold by modifying the default for the variable, or overriding it in tfvars?\r\n\r\nI agree that it would be worthwhile adding a warning note to the docs in terms of being wary of the cost of VPN and interconnect. ",
        "This issue is stale because it has been open 60 days with no activity. Remove stale label or comment or this will be closed in 7 days"
      ]
    },
    "codes": [
      "networking",
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/GoogleCloudPlatform/pbmm-on-gcp-onboarding/issues/45",
    "content": {
      "title": "Fortigate configuration spec triage",
      "body": "# figure out various fortigate configs\r\n# notes\r\nhttps://github.com/fortinetsolutions/terraform-modules/tree/master/GCP/examples\r\n\r\nhttps://github.com/40net-cloud/fortinet-gcp-solutions/tree/master/FortiGate/architectures\r\n\r\nhttps://cloud.google.com/architecture/architecture-centralized-network-appliances-on-google-cloud \r\n\r\nhttps://cloud.google.com/architecture/gcve-advanced-network-security?hl=en\r\nhttps://github.com/GoogleCloudPlatform/pbmm-on-gcp-onboarding/tree/main/modules/fortigate-appliance \r\n\r\n# design\r\n\r\n\r\n# questions\r\n- see https://github.com/fortinetsolutions/terraform-modules/issues/2\r\n-  so far around the minimum roles on the terraform service account\r\n-",
      "comments": [
        "First architecture to integrate to test deployment with:\r\n https://github.com/fortinetsolutions/terraform-modules/tree/master/GCP/examples/ha-active-passive\r\n",
        "Config start:\r\n- register one of your 60 day eval licenses\r\n- in the CSR repo in your cloud shell\r\n- copy the lic file to [modules/fortigate-appliance](https://github.com/GoogleCloudPlatform/pbmm-on-gcp-onboarding/tree/main/modules/fortigate-appliance/files)\r\n- copy the example tfvars https://github.com/GoogleCloudPlatform/pbmm-on-gcp-onboarding/blob/main/config/TFvars/prod/fortigate-firewall.auto.tfvars to https://github.com/GoogleCloudPlatform/pbmm-on-gcp-onboarding/tree/main/environments/prod\r\n- add to prod/main.tf with fortigate-module.tf contents\r\n- uncomment fortigate at the end of variables.tf\r\n- cycle the project number in prod_host_net in prod-network.auto.tfvars\r\n- pending: merge changes\r\n\r\n\r\n\r\n20220421: Meet Notes:\r\nMike Craigen5:17 PM\r\nhttps://cloud.google.com/architecture/architecture-centralized-network-appliances-on-google-cloud\r\n[https://cloud.google.com/architecture/architecture-centralized-network-appliances-on-google-cloud#:~:text=With-,symmetric%20hashing%2C,-you%20can%20ensure](https://cloud.google.com/architecture/architecture-centralized-network-appliances-on-google-cloud#:~:text=With-,symmetric%20hashing,,-you%20can%20ensure)\r\n\r\nfixes required\r\n(github reference) https://github.com/fortinetsolutions/terraform-modules/tree/master/GCP/examples/ha-active-passive\r\n- IGW needed over private google access - route 0.0.0.0/0 needed on mgmt vpc\r\n- premium to standard (not recommended) net tier switch (hardcoded) - need premium tier  - vms must be in same network tier (premium by default) - cost will increase TBD but recommended\r\n- template bugs....\r\n- wrong interface param? \r\n- static IPs\r\n- keep FGSP in mind for session table sync in active-active only (UTM support - asymmetric flow (AVIPS) - max 4 nodes for security profile otherwise 8/16) - should not need to use as packet return flows are to the same node - symmetric to load balancers\r\n- (infrequent) network bound - doubling vCPU doubles throughput - vertical scaling to a point where the nic speed comes in - up to 10gbps - then horizontal (performance scaling and TCO pricing very close to linear 2^n to 32 bound by vm vCPU licensing 64-128 not 64-112 ie) - bound by rules like vCore/nic (try n2-standard-4 over n2-standard-8)\r\n- fortimanager license required for use - can be on-prem via standard c2g connectivity enablement to avoid public routing\r\n\r\n",
        "\r\n# Meeting minutes:\r\n- discuss fortigate config with original developers\r\n2 options  (rough notes - will fill out later tonight)\r\n- directly in environments/prod - add the fortigate config from TFvars and work out the variables section by adding a new fortigate module in main.tf (uncomment the fortigateConfig in variables.tf\r\n- make a new 5th environment (outside of prod) just for fortigate - will need either a full cloud build/trigger infrastructure or some manual terraform plan/apply\r\n",
        "Checking my license file - I also need the VM image name - I should have it in my eval license email...getting it\r\nthanks Mike C and Martin\r\nprojects/fortigcp-project-001/global/images/fortinet-fgtondemand-705-20220211-001-w-license\r\n\r\n```\r\nadmin_@cloudshell:~/wse_github/fortinetsolutions/terraform-modules/GCP/examples/ha-active-active (fortigate-arg)$ git diff\r\ndiff --git a/GCP/examples/ha-active-active/terraform.tfvars b/GCP/examples/ha-active-active/terraform.tfvars\r\nindex 55db49d..e83013b 100644\r\n--- a/GCP/examples/ha-active-active/terraform.tfvars\r\n+++ b/GCP/examples/ha-active-active/terraform.tfvars\r\n@@ -1,11 +1,12 @@\r\n-credentials_file_path           = \"<credentials_file_path>\"\r\n-service_account                 = \"<service_account_email>\"\r\n-project                         = \"<project>\"\r\n+credentials_file_path           = \"/home/admin_/FGVM08TM22001748.lic\" # modified\r\n+service_account                 = \"terraform-fortigate-arg@fortigate-arg.iam.gserviceaccount.com\" # modified\r\n+project                         = \"fortigate-arg\"\r\n name                            = \"terraform-a-a\"\r\n region                          = \"us-central1\"\r\n zone                            = \"us-central1-c\"\r\n machine                         = \"n1-standard-2\"\r\n-image                           = \"<IMAGE>\"\r\n+#image                           = \"<IMAGE>\" # need image\r\n+image                           = \"windows-cloud/windows-2019\" # verify tf for now\r\n bastionhost_image               = \"windows-cloud/windows-2019\"\r\n bastionhost_machine             = \"n1-standard-2\"\r\n admin_port                      = 8443\r\n\r\n\r\nTerraform plan\r\n\r\n\u2502 Error: JSON credentials in \"-----BEGIN FGT VM LICENSE-----\\nQAA\u2026\r\n\r\n VM LICENSE-----\\n\" are not valid: invalid character '-' in numeric literal\r\n\u2502\r\n\u2502   with provider[\"registry.terraform.io/hashicorp/google\"],\r\n\u2502   on main.tf line 3, in provider \"google\":\r\n\u2502    3:   credentials = file(var.credentials_file_path)\r\n\r\n```\r\n\r\nForget about it - that is the SDK credentials - comment out in cloud shell - thanks Mike C.\r\nrefer to  license_file\r\nhttps://github.com/fortinetsolutions/terraform-modules/blob/master/GCP/examples/ha-active-passive-lb-sandwich/terraform.tfvars#L10"
      ]
    },
    "codes": [
      "networking",
      "increase"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/issues/39",
    "content": {
      "title": "Control-plane-0 should be tainted with 'node-role.kubernetes.io/master=true:NoSchedule'",
      "body": "Suggest to taint master node (control-plane-0) with `node-role.kubernetes.io/master=true:NoSchedule` as currently it is not tainted. \r\nI.e. adding \r\n```\r\n  # Taint Control Plane as master node to avoid scheduling of workloads here\r\n  provisioner \"local-exec\" {\r\n    command = <<-EOT\r\n      kubectl taint nodes \"${self.name}\" node-role.kubernetes.io/master=true:NoSchedule\r\n    EOT\r\n  }\r\n```\r\nto master.tf\r\n\r\nBecause of being not tainted, workload pods can get placed on `control-plane-0` and disrupt cluster \r\n(i.e. Cassandra or HDFS pod placed on `control-plane-0` with almost 100% guarantee disrupts cluster)",
      "comments": [
        "Ok, definitely a good thing to make optional! I for instance like to use all nodes for computing, but am not using things like Cassandra! So will give the option definitely \ud83d\ude4f",
        "Thanks for catching that @valb3r! I think keeping it optional, but tainting the control nodes by default would be the best choice as, in my understanding that's generally the expected behavior for kubernetes clusters? Could be different for k3s though?",
        "AFAIK, it is must have for production k8s clusters.\r\nFor k3s i.e. - https://gabrieltanner.org/blog/ha-kubernetes-cluster-using-k3s also enforces master node taints. \r\nAs here we are talking about Hetzner and therefore cheap clusters I would agree with @user that it is optional parameter depending on workload, but it should be documented",
        "Definitely, will do, but in the meantime, just shoot a PR adding a variable for that if you can, tainted by default indeed.",
        "Added by default, thanks again @mnencia!"
      ]
    },
    "codes": [
      "cluster",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/issues/143",
    "content": {
      "title": "Control plane loadbalancer",
      "body": "How is the control plane HA? I don't see a loadbalancer for the control plane nodes. E.g. the agents connect to the first node, what happens if the first node goes offline? ",
      "comments": [
        "The cluster becomes HA when the control planes nodes number is equal or superior to 3, and always odd numbers as advised by Rancher.\r\n\r\nWhen you have 3 control plane nodes, you can take any of them offline, and the cluster will continue to work. That is what happens when the automatic upgrades of both OS and k3s happen.\r\n\r\nNow, if you are talking of using kubectl in an HA manner, that is not implemented. We use the IP of the first control plane in the `kubeconfig.yaml`. However, if it is not reachable, you can swipe it for other IPs of other control planes manually. But normally, control plane nodes go offline for a few seconds max for a reboot.\r\n\r\nIf you have a better solution for that, please propose it. ",
        "Ah that clears some things up. I thought that you needed a loadbalancer to make the control plane HA. Didn't know that under the hood the agents themselves talk to all 3 control plane nodes. \r\n\r\nKubectl HA is not a priority for me now but if somebody else wants to fix it, here are some general pointers:\r\n\r\n* Hetzner loadbalancer pointing to all control plane nodes\r\n* Kubeconfig must contain loadbalancer public ip instead of control plane ip\r\n* Control plane nodes need to be initialized with the following extra flags:\r\n  * `--tls-san ${lb_private_ip}`\r\n  * `--tls-san ${lb_public_ip}`\r\n ",
        " @user \r\n\r\nYou might want to check this example from TechnoTim his Youtube channel. \r\nHe solves the controlplane HA with Kubevip and MetalLB. Don't know if this is something easy to introduce in your setup, but this takes care of auto-updating the IP\r\nhttps://www.youtube.com/watch?v=CbkEWcUZ7zM\r\n\r\nAnother solution would be to add KeepAlived with 2 extra nodes and a floating IP which in return route traffic to 1 of the 3 control nodes that are healthy. If they go down, it routes to 2nd or 3rd one. \r\nThis method does add 2 extra node as cost",
        "Thanks, folks, great ideas! @user Will check the video ASAP. \r\n\r\nI have been thinking of just having some sort of local lb setup that runs on localhost and points to the latest healthy control plane.\r\n\r\nSince `hcloud` can output JSON, it seems possible to just follow such a tutorial to create a local lb, that we would in turn point kubeconfig towards. https://dev.to/bmf_san/implement-a-load-balancer-in-golang-8gj. Now ideally, one would have to know Go lang, I am not yet too familiar with it.",
        " @user \r\n\r\nFYI, There's a few articles from Keepalived at hetzner too.\r\n\r\nhttps://community.hetzner.com/tutorials/configure-cloud-ha-keepalived\r\nhttps://github.com/alxhlz/hcloud-failover-keepalived\r\nhttps://community.hetzner.com/tutorials/howto-highly-available-load-balancer-hetzner-cloud-ansible\r\n\r\nMaybe these can be helpful too. \r\nI'm not familiar with Golang either\r\n\r\nOther articles about Hetzner and MetalLB I found:\r\nhttps://community.hetzner.com/tutorials/install-kubernetes-cluster\r\n",
        "I think those articles about Keepalived are because in that time Hetzner did not have loadbalancers yet, same with metallb. This problem can be solved using their loadbalancers IMO",
        " @user \r\nCould be, but does that also automatically give a high available control plane? \r\nBecause that's the whole point if the primary node goes down, the IP needs to relocate. If that doesn't happen, your control plane is unreachable. \r\nHence the reason so many people use something like MetalLB with Kubevip because it exposes a Virtual IP that allows relocating. \r\n",
        "Folks, as clarified before we do have full HA. My proposals are just to make the kubectl user comfortable in the unlikely event that he was to hit a node that went down.\n\nSo a localhost and free solution would be ideal.",
        " @user \r\nIn that case a metalLB and kubevip would be an ideal use case over a paid managed LB service. \r\n\r\nAlso, since there are already 3 nodes running your control plane, why not use those same host also as an LB instead of buying another LB service. It's not expensive either to buy but then again 3 nodes can already provide the solution with some changes ",
        "Exactly... Maybe something like that would work. But really, it's almost a nonissue. If you ever were to try to kubectl into a node that is being rebooted or down, you would change the IP in the kubeconfig.yaml file manually to the next control plane node.\r\n\r\nSo unless you folk need this, we can close the issue.",
        " @user I assume you'd need to pay at least for an extra IP anyway, right? Might be much cheaper than a load-balancer. At least in financial terms, in terms of maintenance work, all suggested solutions seem somewhat expensive for me for the relatively little benefit they provide. \r\n\r\nFeel free to open up a PR whenever you got something working :)"
      ]
    },
    "codes": [
      "domain",
      "saving",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/issues/166",
    "content": {
      "title": "[REQ] import into rancher",
      "body": "Our clusters are managed via rancher. The created K3s cluster can be imported successfully with a few modifications.\r\n\r\nI could either offer a PR that optionally does full registration in rancher at the same time ( create rancher cluster and import k3s ) or just customize the few places (set the rights of kubeconfig).\r\n\r\nWhat do you think about this?",
      "comments": [
        "Yes, Sir! :) OOTB support for Rancher would be awesome. Why not even an option of automatically deploying it. Do as you see fit, this project needs to support real world scenarios like the ones you are exposing.",
        "Both solutions would be very useful. (I am currently planning on moving some Clusters from our rancher/bare-meta/datacenter -> hcloud ) and still trying to wrap my head around the ui-node-driver or importing the cluster afterwards (besides the hetzner-controller etc..).\r\n\r\nIm guessing deploying rancher ist very simple, since it is just a helm deployment, just have to figure out the ssl/letsencrypt part.\r\n\r\n```bash\r\nhelm install rancher-stable/rancher \\\r\n  --name rancher \\\r\n  --namespace cattle-system \\\r\n  --set hostname=rancher.xyz.net \\\r\n  --set ingress.tls.source=secret\r\n```\r\n\r\nThe importing although needs a token from the rancher. I'd be very interested if there are any additional steps besides running the agent @user .\r\n",
        " @user  I had a few problems with the ui-node-driver+dockermachine setup, one i could not fix or work around unfortunately. When creating a cluster via terraform I always had the error message: \"default-token already in use\".\r\n\r\nI then switched to setting up the clusters externally and only importing them. That worked right away.\r\n\r\nI am currently building a module to set up rancher via terraform. When it's ready, I'll post the link to the repo here. I hope, I am Sunday ready.",
        " @user  There was almost nothing else to do. Additionally only the rights for the node kubeconfig must be set (see k3s docs). I hope that I can also make a PR until Sunday.",
        "> Yes, Sir! :) OOTB support for Rancher would be awesome. Why not even an option of automatically deploying it. Do as you see fit, this project needs to support real world scenarios like the ones you are exposing.\r\n\r @user do I understand you correctly that you want to have an optional Rancher installation within the module? My feeling is that this is better off in its own repo. Currently the module does exactly one thing, set up a k3s cluster on top of hetzner, which I personally like.\r\n\r\nIf you want to support rancher here, I would suggest we do that in a second repo.\r\n\r\nIn this repo only the read-write to the node kubeconfig necessary for rancher goes.\r\n\r\nIn the second repo I would build two modules, one that registers a cluster with terraform and a second that sets up terraform on such a cluster (and registers itself at the same time).\r\n\r\nWhat do you think?",
        "I did not understand exactly what you meant, but compartmentalizing stuff is a good idea. \n\nIf we can have a second repo that can run rancher to manage a kube-hetzner cluster, it would be awesome!\n\nYou have great ideas @batthebee!",
        " @user Have invited you to join the org so that you can initiate the second repo for the above if you are inspired.\n\nWelcome to Kube-hetzner ! :)",
        "Thanx! :)",
        "Hey folks, just wanted to share that in https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/pull/173, we actually deployed a helm chart with a simple YAML `HelmChart` definition. So no need to use the command line cli, that is a major simplifier IMHO!",
        "I understand that probably it's not wise to install Rancher on the cluster itself and that it would make more sense as a separate module, but just felt I needed to share the above info. It may be useful in another context.",
        "Ah, I retract what I said above, as the author of this [video](https://www.youtube.com/watch?v=eKBBHc0t7bc) does, it's pretty nice to have rancher run on the cluster itself too. But definitely not mandatory!\r\n\r\nProbably that is a use case different from what you are proposing here @user  But both seem very interesting! \r\n\r\nProbably just deploying Rancher into the cluster can be done very easily with a HelmChart declaration, and a flag variable. Will try that in a separate PR as soon as I can.",
        "Finally, will just install it via the helm cli, that way it's easier to get the whole ingress config right. It's not really something that can be automated to the point of an HelmChart YAML file (unless we also do that for cert-manager and collect the email for LetsEncrypt).\r\n\r\nBasically, the only thing required here @user is to make this cluster compatible to be imported in a third-party Rancher install. As you said, formatting the kubeconfig well... etc.  That would indeed be awesome!",
        "Good work! ;)\r\nJust to clean this up (hopefully) - having worked with rancher for +5y, the the difference between same cluster and extra-cluster is \"just\" for security and stability issues. \r\nIt is best practice to have a separate management-cluster running only rancher. This prevents high workloads etc, from interfering with it (or developers from deleting the rancher-daemonset :) ).\r\nIt might be possible to get the rancher-agent token via the terraform-module (haven't checked this yet) and deploy the agent accordingly.",
        "Thanks for clarifying the matter @strowi! That makes perfect sense. \r\n\r\nIn that case, the ideal scenario would be a single node cluster for Rancher only (the beauty of single-node clusters is that it uses Klipper as LB, so removes the need for a Hetzner LB, hence dropping the cost even more).\r\n\r\nAnd now that we have support for Longhorn, we can set up an automatic backup of PVCs to any S3 compatible endpoint, and make everything even more robust.",
        " @user depends on definition of \"ideal scenario\" ;) In production you'd probably want at least a 3-node rancher cluster for high-availability if this is a developers only access to the downstream clusters. But that setup really wouldn't differ i guess.\r\n\r\nBackups of pvcs are neat and definitely useful! Although i've had a lot of trouble with longhorn itself (very high cpu usage for keeping the disks in sync, etc), i think it is very easy to setup. (Otherwise i can recommend restic/velero for backups)",
        "Awesome @strowi!! Thanks for sharing your production experience. It's invaluable!",
        "happy to help;) ",
        " @user @strowi Have added the possibility to deploy Rancher locally. Please do not hesitate to suggest changes. https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/pull/176\r\n\r\nIt does not answer the requirement of that issue, but at least it's great to have Rancher!",
        "Solved by @user in #180."
      ]
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/cds-snc/covid-alert-server-staging-terraform/issues/25",
    "content": {
      "title": "Parameterize AWS Resources",
      "body": "In order to save costs we should start to parameterize various resources so we can scale up and down in non-prod environments more easily. \r\n\r\nThis is also work needed to be done before we implement #18 ",
      "comments": []
    },
    "codes": [
      "saving",
      "provider"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/cds-snc/covid-alert-server-staging-terraform/issues/101",
    "content": {
      "title": "Add input validation before storing to bucket",
      "body": "https://github.com/cds-snc/covid-alert-server-staging-terraform/blob/34ebe9b2a71291db6b05498c86d19bdbc8dd895f/server/aws/lambda/create_metric.js#L32\r\n\r\nJust putting this out there, based on the following assumptions:\r\n\r\n1. We have effectively no control over the input data stream.\r\n2. See #1. :)\r\n\r\nI'd recommend we do a couple of things...\r\n\r\n1. Determine a maximum event size, and /dev/null anything that exceeds that (silent fail)\r\n2. Leave the data in json encoded format. S3 is cheap, but we can also stringify later if needed.\r\n3. Determine when/where we're doing input validation on the json schema.\r\n\r\nOpen to thoughts.\r\n",
      "comments": [
        "<ignore previous> I assume this resolves it, via schema validation at the api gw level n'est pas?\r\n\r\nhttps://github.com/cds-snc/covid-alert-server-staging-terraform/blob/master/server/aws/models/metrics.json"
      ]
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/cds-snc/covid-alert-server-staging-terraform/issues/156",
    "content": {
      "title": "Investigate costs in regards to using Provisioned vs On-Demand DynamoDb",
      "body": "",
      "comments": [
        "At the moment it's not worth making this change. "
      ]
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/cds-snc/covid-alert-server-staging-terraform/issues/185",
    "content": {
      "title": "Implement Traffic Shaping on Metric Aggregation ",
      "body": "This is probably something that needs to be handled on the App side of things instead of the server side. \r\n\r\nWe could still add queues into the metric flow to manage reads and writes to the dynamodb tables but at the moment it's not worth it from a cost perspective. \r\n\r\nThe bigger issue is the limit on the metrics endpoint we have a hard limit of 600 requests a second, at the moment we are hitting about 480 at peak, if we get close 550 requests a second we will need to push a release that adds back in some jitter.",
      "comments": [
        "We've confirmed with the app team that if the phone does not receive an HTTP 200 success when it posts to `/save-metrics`, it does not mark the metrics as published and will retry with an exponential backoff.\r\n\r\nAs a result of increased metric traffic, we are now seeing cases where the hard limit of requests is reached and Lambda throttling occurs, but there has not been a loss of metrics data because of retry/backoff logic in the app."
      ]
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/openaustralia/infrastructure/issues/28",
    "content": {
      "title": "Setup Elastic IP address for theyvoteforyou instance",
      "body": "Right now we've got the DNS setup in a way that is dependent on the instances being around for a long time and maintaining their IP address.\r\n\r\nAs far as I'm aware there are two approaches we could take:\r\n* Use the Elastic Load Balancer\r\n* or use [Elastic IP Addresses](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html)\r\n\r\nI'm guessing (without looking) that having a seperate IP address for each site is going to be too expensive and as far as I'm aware we can have several domains go through a single load balancer. ",
      "comments": [
        "Hmm.. It seems that Elastic IP addresses are free. So, I'm guessing that's the way to go then...",
        "Done"
      ]
    },
    "codes": [
      "domain",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/openaustralia/infrastructure/issues/26",
    "content": {
      "title": "Estimate cost of migrating to AWS using managed db (RDS)",
      "body": "",
      "comments": [
        "Won't fix. We're already using RDS. We can see how much it costs now."
      ]
    },
    "codes": [
      "storage",
      "awareness",
      "provider"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/openaustralia/infrastructure/issues/56",
    "content": {
      "title": "Cancel DNS made easy account",
      "body": "All our domain hosting has now been removed from DNS made easy and moved to cloudflare.",
      "comments": [
        "We still have an account. I just cancelled all the services with DNS Made Easy that we pay for. So, now, we have a $9.85 credit with them."
      ]
    },
    "codes": [
      "domain",
      "saving"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/openaustralia/infrastructure/issues/90",
    "content": {
      "title": "New Relic: Migrate from \"server\" to \"infrastructure\"",
      "body": "With breathtakingly little notice (less than a month), New Relic is shutting down server monitoring and forcing users to their \"Infrastructure\" tool. *Not Happy*.\r\n \r\nInfrastructure costs money. We're already on a Pro account. Let's move and see how much it comes to. I don't think we can afford to not have server monitoring in place for any time.\r\n",
      "comments": [
        "ok thanks for the heads up. With one thing and another, looks like we have a bunch of increasing monthly running costs this quarter.\n\n\n\u2014\nKat Szuminska\n\n--- original message ---\nOn Fri, Apr 20, 2018 at 02:24 am, <notifications@github.com> Matthew Landauer wrote:\n\n\nWith breathtakingly little notice (less than a month), New Relic is shutting down server monitoring and forcing users to their \"Infrastructure\" tool. Not Happy.\n\nInfrastructure costs money. We're already on a Pro account. Let's move and see how much it comes to. I don't think we can afford to not have server monitoring in place for any time.\n\n\u2014\n\nYou are receiving this because you are subscribed to this thread.\n\nReply to this email directly, view it on GitHub, or mute the thread.\n--- end of original message ---",
        "First step: get new agent installed on all server. With 377a98d0d1d75ddb3e415e44cdcd0c8e7c51ead2 that's done with the exception of morph and cuttlefish",
        "morph installed with https://github.com/openaustralia/morph/commit/25d4d1d2e752097cc1f5ebf59e3696729d29cea8",
        "cuttlefish installed with https://github.com/openaustralia/cuttlefish/commit/4ec5e9be35a46078e1ed73ec8e35c3130ca96d8b and https://github.com/openaustralia/cuttlefish/commit/a38a030680c3a4c4580f215b733d3aa8039c7869",
        "Still to do:\r\n- [ ] set up alerts\r\n- [ ] give sensible names to the servers\r\n"
      ]
    },
    "codes": [
      "increase"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/openaustralia/infrastructure/issues/134",
    "content": {
      "title": "Disable NewRelic APM on all hosts",
      "body": "NewRelic have contacted us to let us know that our cost for APM is increasing from 50USD/mo to 750USD/mo\r\n\r\nWe aren't getting 750USD/mo of value from APM, so we need to disable this within the next few days.\r\n\r\nInfrastructure monitoring is still only 58USD/mo so we'll keep that for now.\r\n\r\nObsoletes #114 \r\nObsoletes #85 ",
      "comments": [
        "This has been completed. I've asked NewRelic to cancel our APM subscription, and I'll track that work in Front."
      ]
    },
    "codes": [
      "awareness",
      "increase",
      "provider"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/openaustralia/infrastructure/issues/142",
    "content": {
      "title": "Unused EBS volumes on AWS probably costing us some money",
      "body": "Just noticed that there are a few EBS volumes on AWS that are currently not attached to any machines. They would have been for the old machines that we decommissioned. I wonder if they\u2019re costing us much money\u2026 We should probably clean them up\u2026.",
      "comments": []
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/cds-snc/cloud-based-sensor/issues/110",
    "content": {
      "title": "Optimize cost of ConfigRules",
      "body": "# Summary\r\nCurrently the largest cost for the CBS resources are the ConfigRules.  In a scratch account with no resources, it's estimated to be between $20 to $50 per month.\r\n\r\nIf possible, this cost should be reduced.",
      "comments": [
        " or we will need to prioritize the accounts (prod only) ",
        "After more investigation, this is a fixed, low cost per account:\r\n* Pay once a month per ConfigRule enabled\r\n* Pay per configuration item check\r\n\r\nThe only way we could improve the cost would be to combine all our checks into a single ConfigRule custom Lambda.\r\n\r\nWe've also found that the current higher cost is around CloudTrail which will be addressed by:\r\n#119 "
      ]
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/cds-snc/cloud-based-sensor/issues/111",
    "content": {
      "title": "Add cost alarms for CBS resources",
      "body": "# Summary\r\nAlarms should be setup to alert us when CBS resource costs exceed a threshold.",
      "comments": [
        "After a discussion with AWS, we could setup the following billing alarms:\r\n\r\n* Each hour, compare the cost of account resources from current to previous hour.\r\n* Warning if 10% increase.\r\n* Critical if 50% increase.\r\n\r\nThis will catch early, unexpected ramp up costs.\r\n\r\nCovid Alert metrics has a metric math alarm checking for a percentage change in a given metric:\r\nhttps://github.com/cds-snc/covid-alert-metrics-terraform/blob/main/aws/cloudwatch_alarms/api_gateway.tf#L88-L131"
      ]
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/cds-snc/cloud-based-sensor/issues/127",
    "content": {
      "title": "Bug: billing alarm shows 100% change on first day of the month",
      "body": "# Summary\r\nOn the first day of the month, all billing alarms trigger showing a 100% change in billing charges over a 6 hour period.\r\n\r\nThis is because of the following calculation to get the percent change:\r\n```\r\npercent change = ABS(100 * delta/previous)\r\npercent change = ABS(100 * 0/0)\r\npercent change = 100\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/2110107/156176596-a95c102b-c453-4c83-a7fc-e8ddc62139ea.png)\r\n",
      "comments": [
        "A possible fix to this would be to add a new metric math expression that checked if `previous` charges were `0` and then to use that as the alarm trigger threshold:\r\n```\r\nIF(FLOOR(previous) == 0, 0, percent_change)\r\n```",
        "Alarm switched to anomaly detection band by #128 "
      ]
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/cds-snc/cloud-based-sensor/issues/119",
    "content": {
      "title": "Disable CloudTrail and use LandingZone Trail",
      "body": "# Summary\r\nThe CBS multi-region CloudTrail is doubling the cost of onboarding any account.  To save money, we can use the LandingZone's mutli-region trail by replicating logs from the `log-archive` S3 bucket to the `CloudBasedSensorCentral` account log archive bucket.\r\n\r\nExample of replication:\r\nhttps://github.com/cds-snc/sentinel-connectors/tree/main/aws/cloud_trail/terraform ",
      "comments": [
        "This is now done."
      ]
    },
    "codes": [
      "area",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/falldamagestudio/UE-Jenkins-BuildSystem/issues/20",
    "content": {
      "title": "Reduce impact of cold boot nodes",
      "body": "Windows nodes start slower than Linux nodes. Is there something that we can do to speed up those boot times?\r\n\r\nCan we speed up image pulls?\r\n\r\nWindows jenkins inbound-agent image is about 2.4GB zipped servercore + a little bit of extra stuff - probably about 7GB when unzipped & pulled to the target machine. Windows buildtools images are 2.4GB zipped servercore + 3.5GB zipped buildtools - in total about 15GB when unzipped & pulled to the target machine. These images are normally built by different groups of people and therefore they do not use the same exact servercore base image by default.\r\n\r\nWe need to pull both the inbound-agent image and the build tools image. Pulling both those take ~15 minutes in total. Can we reduce that time?\r\n- Is it possible to reduce the number of components in the VS buildtools - just one MSVC toolchain perhaps?\r\n- Can we get hold of/install PDBCopy without bringing Debugging Tools for Windows along?\r\n- Shall we begin building the inbound-agent image ourselves, to ensure it has the same base windows image as our build tools?\r\n\r\nCan we make caching more effective? For example, can we keep some nodes up 24/7 without incurring huge costs -- what if use ephemeral VMs?",
      "comments": [
        "Example metrics, an incremental game build job spent:\r\n- 19 min waiting;\r\n- 34 min build duration;\r\n- 34 min total from scheduled to completion.\r\n\r\nThe first 19 minutes of the run looked like this:\r\n```\r\n2021-02-25 07:45:10  Started by user Mikael Kalms\r\n2021-02-25 07:45:13  Obtained build_game_windows.groovy from git https://github.com/Kalmalyzer/UE-Jenkins-Game\r\n2021-02-25 07:45:13  Running in Durability level: MAX_SURVIVABILITY\r\n2021-02-25 07:45:16  [Pipeline] Start of Pipeline\r\n2021-02-25 07:45:20  [Pipeline] podTemplate\r\n2021-02-25 07:45:22  [Pipeline] {\r\n2021-02-25 07:45:22  [Pipeline] node\r\n2021-02-25 07:45:26  Created Pod: kubernetes default/build-game-windows-19-4bz80-3trtm-dv79f\r\n2021-02-25 07:45:27  [Warning][default/build-game-windows-19-4bz80-3trtm-dv79f][FailedScheduling] 0/1 nodes are available: 1 node(s) didn't match node selector.\r\n2021-02-25 07:45:27  [Warning][default/build-game-windows-19-4bz80-3trtm-dv79f][FailedScheduling] 0/1 nodes are available: 1 node(s) didn't match node selector.\r\n2021-02-25 07:45:32  [Normal][default/build-game-windows-19-4bz80-3trtm-dv79f][TriggeredScaleUp] pod triggered scale-up: [{https://content.googleapis.com/compute/v1/projects/kalms-ue4-jenkins-buildsystem/zones/europe-west1-b/instanceGroups/gke-jenkins-jenkins-agent-windows-nod-777d743f-grp 0->1 (max: 10)}]\r\n2021-02-25 07:45:37  Still waiting to schedule task\r\n2021-02-25 07:45:37  \u2018build-game-windows-19-4bz80-3trtm-dv79f\u2019 is offline\r\n2021-02-25 07:46:33  [Warning][default/build-game-windows-19-4bz80-3trtm-dv79f][FailedScheduling] 0/1 nodes are available: 1 node(s) didn't match node selector.\r\n2021-02-25 07:48:03  [Warning][default/build-game-windows-19-4bz80-3trtm-dv79f][FailedScheduling] 0/1 nodes are available: 1 node(s) didn't match node selector.\r\n2021-02-25 07:49:33  [Warning][default/build-game-windows-19-4bz80-3trtm-dv79f][FailedScheduling] 0/1 nodes are available: 1 node(s) didn't match node selector.\r\n2021-02-25 07:49:44  [Normal][default/build-game-windows-19-4bz80-3trtm-dv79f][Scheduled] Successfully assigned default/build-game-windows-19-4bz80-3trtm-dv79f to gke-jenkins-jenkins-agent-windows-nod-777d743f-1d08\r\n2021-02-25 07:49:52  [Normal][default/build-game-windows-19-4bz80-3trtm-dv79f][SuccessfulAttachVolume] AttachVolume.Attach succeeded for volume \"pvc-a66810b8-eb68-4a96-896e-63307fb483b9\" \r\n2021-02-25 07:50:08  [Normal][default/build-game-windows-19-4bz80-3trtm-dv79f][Pulling] Pulling image \"jenkins/inbound-agent:windowsservercore-ltsc2019\"\r\n2021-02-25 07:52:26  Created Pod: kubernetes default/build-game-windows-19-4bz80-3trtm-40xj6\r\n2021-02-25 07:52:26  [Normal][default/build-game-windows-19-4bz80-3trtm-40xj6][Scheduled] Successfully assigned default/build-game-windows-19-4bz80-3trtm-40xj6 to gke-jenkins-jenkins-agent-windows-nod-777d743f-1d08\r\n2021-02-25 07:52:37  [Normal][default/build-game-windows-19-4bz80-3trtm-40xj6][Pulling] Pulling image \"jenkins/inbound-agent:windowsservercore-ltsc2019\"\r\n2021-02-25 07:54:21  [Normal][default/build-game-windows-19-4bz80-3trtm-dv79f][Pulled] Successfully pulled image \"jenkins/inbound-agent:windowsservercore-ltsc2019\"\r\n2021-02-25 07:54:21  [Warning][default/build-game-windows-19-4bz80-3trtm-dv79f][Failed] Error: cannot find volume \"workspace-volume\" to mount into container \"jnlp\"\r\n2021-02-25 07:54:21  [Normal][default/build-game-windows-19-4bz80-3trtm-dv79f][Pulling] Pulling image \"europe-west1-docker.pkg.dev/kalms-ue4-jenkins-buildsystem/docker-build-artifacts/ue-jenkins-buildtools-windows:commit-410f8892ae266cbe4ca5e4d332b1e2518d0730f9\"\r\n2021-02-25 07:54:22  [Normal][default/build-game-windows-19-4bz80-3trtm-40xj6][Pulled] Successfully pulled image \"jenkins/inbound-agent:windowsservercore-ltsc2019\"\r\n2021-02-25 07:54:22  [Normal][default/build-game-windows-19-4bz80-3trtm-40xj6][Created] Created container jnlp\r\n2021-02-25 07:54:24  [Normal][default/build-game-windows-19-4bz80-3trtm-40xj6][Started] Started container jnlp\r\n2021-02-25 07:54:24  [Normal][default/build-game-windows-19-4bz80-3trtm-40xj6][Pulling] Pulling image \"europe-west1-docker.pkg.dev/kalms-ue4-jenkins-buildsystem/docker-build-artifacts/ue-jenkins-buildtools-windows:commit-410f8892ae266cbe4ca5e4d332b1e2518d0730f9\"\r\n2021-02-25 07:59:26  Created Pod: kubernetes default/build-game-windows-19-4bz80-3trtm-hn289\r\n2021-02-25 07:59:26  [Warning][default/build-game-windows-19-4bz80-3trtm-hn289][FailedScheduling] 0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) didn't match pod affinity/anti-affinity, 1 node(s) didn't satisfy existing pods anti-affinity rules.\r\n2021-02-25 07:59:26  [Warning][default/build-game-windows-19-4bz80-3trtm-hn289][FailedScheduling] 0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) didn't match pod affinity/anti-affinity, 1 node(s) didn't satisfy existing pods anti-affinity rules.\r\n2021-02-25 07:59:31  [Normal][default/build-game-windows-19-4bz80-3trtm-hn289][TriggeredScaleUp] pod triggered scale-up: [{https://content.googleapis.com/compute/v1/projects/kalms-ue4-jenkins-buildsystem/zones/europe-west1-b/instanceGroups/gke-jenkins-jenkins-agent-windows-nod-777d743f-grp 1->2 (max: 10)}]\r\n2021-02-25 07:59:43  [Normal][default/build-game-windows-19-4bz80-3trtm-hn289][Scheduled] Successfully assigned default/build-game-windows-19-4bz80-3trtm-hn289 to gke-jenkins-jenkins-agent-windows-nod-777d743f-1d08\r\n2021-02-25 07:59:52  [Normal][default/build-game-windows-19-4bz80-3trtm-hn289][Pulled] Container image \"jenkins/inbound-agent:windowsservercore-ltsc2019\" already present on machine\r\n2021-02-25 07:59:52  [Normal][default/build-game-windows-19-4bz80-3trtm-hn289][Created] Created container jnlp\r\n2021-02-25 07:59:53  [Normal][default/build-game-windows-19-4bz80-3trtm-hn289][Started] Started container jnlp\r\n2021-02-25 07:59:53  [Normal][default/build-game-windows-19-4bz80-3trtm-hn289][Pulling] Pulling image \"europe-west1-docker.pkg.dev/kalms-ue4-jenkins-buildsystem/docker-build-artifacts/ue-jenkins-buildtools-windows:commit-410f8892ae266cbe4ca5e4d332b1e2518d0730f9\"\r\n2021-02-25 08:04:28  [Normal][default/build-game-windows-19-4bz80-3trtm-hn289][Pulled] Successfully pulled image \"europe-west1-docker.pkg.dev/kalms-ue4-jenkins-buildsystem/docker-build-artifacts/ue-jenkins-buildtools-windows:commit-410f8892ae266cbe4ca5e4d332b1e2518d0730f9\"\r\n2021-02-25 08:04:28  [Normal][default/build-game-windows-19-4bz80-3trtm-hn289][Created] Created container ue-jenkins-buildtools-windows\r\n2021-02-25 08:04:35  [Normal][default/build-game-windows-19-4bz80-3trtm-hn289][Started] Started container ue-jenkins-buildtools-windows\r\n2021-02-25 08:04:37  Agent build-game-windows-19-4bz80-3trtm-hn289 is provisioned from template build_game_windows_19-4bz80-3trtm\r\n2021-02-25 08:04:37  ---\r\n2021-02-25 08:04:37  apiVersion: \"v1\"\r\n2021-02-25 08:04:37  kind: \"Pod\"\r\n2021-02-25 08:04:37  metadata:\r\n2021-02-25 08:04:37    annotations:\r\n2021-02-25 08:04:37      buildUrl: \"http://jenkins-controller:8080/job/build_game_windows/19/\"\r\n2021-02-25 08:04:37      runUrl: \"job/build_game_windows/19/\"\r\n2021-02-25 08:04:37    labels:\r\n2021-02-25 08:04:37      app: \"jenkins-agent\"\r\n2021-02-25 08:04:37      jenkins/jenkins-controller-jenkins-agent: \"true\"\r\n2021-02-25 08:04:37      jenkins/label-digest: \"ba1bb669db87bb10ed2bfa9aa590a7ba54c266b6\"\r\n2021-02-25 08:04:37      jenkins/label: \"build_game_windows_19-4bz80\"\r\n2021-02-25 08:04:37    name: \"build-game-windows-19-4bz80-3trtm-hn289\"\r\n2021-02-25 08:04:37  spec:\r\n2021-02-25 08:04:37    affinity:\r\n2021-02-25 08:04:37      podAntiAffinity:\r\n2021-02-25 08:04:37        requiredDuringSchedulingIgnoredDuringExecution:\r\n2021-02-25 08:04:37        - labelSelector:\r\n2021-02-25 08:04:37            matchExpressions:\r\n2021-02-25 08:04:37            - key: \"app\"\r\n2021-02-25 08:04:37              operator: \"In\"\r\n2021-02-25 08:04:37              values:\r\n2021-02-25 08:04:37              - \"jenkins-agent\"\r\n2021-02-25 08:04:37          topologyKey: \"kubernetes.io/hostname\"\r\n2021-02-25 08:04:37    containers:\r\n2021-02-25 08:04:37    - env:\r\n2021-02-25 08:04:37      - name: \"JENKINS_SECRET\"\r\n2021-02-25 08:04:37        value: \"********\"\r\n2021-02-25 08:04:37      - name: \"JENKINS_TUNNEL\"\r\n2021-02-25 08:04:37        value: \"jenkins-controller-agent:50000\"\r\n2021-02-25 08:04:37      - name: \"JENKINS_AGENT_NAME\"\r\n2021-02-25 08:04:37        value: \"build-game-windows-19-4bz80-3trtm-hn289\"\r\n2021-02-25 08:04:37      - name: \"JENKINS_NAME\"\r\n2021-02-25 08:04:37        value: \"build-game-windows-19-4bz80-3trtm-hn289\"\r\n2021-02-25 08:04:37      - name: \"JENKINS_AGENT_WORKDIR\"\r\n2021-02-25 08:04:37        value: \"C:\\\\J\"\r\n2021-02-25 08:04:37      - name: \"JENKINS_URL\"\r\n2021-02-25 08:04:37        value: \"http://jenkins-controller:8080/\"\r\n2021-02-25 08:04:37      image: \"jenkins/inbound-agent:windowsservercore-ltsc2019\"\r\n2021-02-25 08:04:37      name: \"jnlp\"\r\n2021-02-25 08:04:37      resources:\r\n2021-02-25 08:04:37        requests:\r\n2021-02-25 08:04:37          cpu: \"100m\"\r\n2021-02-25 08:04:37          memory: \"256Mi\"\r\n2021-02-25 08:04:37      volumeMounts:\r\n2021-02-25 08:04:37      - mountPath: \"C:\\\\J\"\r\n2021-02-25 08:04:37        name: \"workspace-volume\"\r\n2021-02-25 08:04:37        readOnly: false\r\n2021-02-25 08:04:37      workingDir: \"C:\\\\J\"\r\n2021-02-25 08:04:37    - args:\r\n2021-02-25 08:04:37      - \"Start-Sleep 999999\"\r\n2021-02-25 08:04:37      command:\r\n2021-02-25 08:04:37      - \"powershell\"\r\n2021-02-25 08:04:37      image: \"europe-west1-docker.pkg.dev/kalms-ue4-jenkins-buildsystem/docker-build-artifacts/ue-jenkins-buildtools-windows:commit-410f8892ae266cbe4ca5e4d332b1e2518d0730f9\"\r\n2021-02-25 08:04:37      name: \"ue-jenkins-buildtools-windows\"\r\n2021-02-25 08:04:37      volumeMounts:\r\n2021-02-25 08:04:37      - mountPath: \"C:\\\\J\"\r\n2021-02-25 08:04:37        name: \"workspace-volume\"\r\n2021-02-25 08:04:37        readOnly: false\r\n2021-02-25 08:04:37      workingDir: \"C:\\\\J\"\r\n2021-02-25 08:04:37    nodeSelector:\r\n2021-02-25 08:04:37      jenkins-agent-windows-node-pool: \"true\"\r\n2021-02-25 08:04:37    restartPolicy: \"Never\"\r\n2021-02-25 08:04:37    tolerations:\r\n2021-02-25 08:04:37    - effect: \"NoSchedule\"\r\n2021-02-25 08:04:37      key: \"jenkins-agent-windows-node-pool\"\r\n2021-02-25 08:04:37      operator: \"Equal\"\r\n2021-02-25 08:04:37      value: \"true\"\r\n2021-02-25 08:04:37    - effect: \"NoSchedule\"\r\n2021-02-25 08:04:37      key: \"node.kubernetes.io/os\"\r\n2021-02-25 08:04:37      operator: \"Equal\"\r\n2021-02-25 08:04:37      value: \"windows\"\r\n2021-02-25 08:04:37    volumes:\r\n2021-02-25 08:04:37    - name: \"workspace-volume\"\r\n2021-02-25 08:04:37      persistentVolumeClaim:\r\n2021-02-25 08:04:37        claimName: \"build-game-windows\"\r\n2021-02-25 08:04:37        readOnly: false\r\n2021-02-25 08:04:37  \r\n2021-02-25 08:04:37  Running on build-game-windows-19-4bz80-3trtm-hn289 in C:\\J\\workspace\\build_game_windows\r\n```\r\n\r\nA summary of what happended there is:\r\n\r\n- at 07:45:26, the pod was created\r\n- at 07:45:32, GKE began the scale-up process\r\n- at 07:50:08, the node was running and GKE began pulling the Jenkins agent container\r\n- at 07:54:22, the Jenkins agent container finished pulling\r\n- at 07:54:24, the node began pulling the build tools container\r\n- at 08:04:28, the build tools container finished pulling\r\n- at 08:04:37, Jenkins began doing the things it'd do near-instantly if this was a non-K8s job\r\n\r\nThere are three `Created Pod` messages. They are exactly 7 minutes apart. I'm not 100% sure, but I think there is a timeout within Jenkins that times, deletes and recreates a pod if provisioning of the pod doesn't complete within 7 minutes. There was one timeout in Jenkins' Kubernetes plugin at ~120 seconds, I found that some time ago, but this 7-minute timeout I don't know where it is. From what I can tell, this does not affect image pulls significantly; I think that the docker daemon will complete the pull operation (at least completing pull of the current fs layer) even when the pod is deleted & recreated halfway through.\r\n\r\nThere are two \"pod triggered scale-up\" messages, 14 minutes apart. I don't know whether it is GKE or Jenkins doing this -- I suspect that is GKE. It might be because of pods getting deleted/recreated makes GKE's autoscaler think there is more pressure than there is in reality.\r\n\r\nNow - what's in each of the two images?\r\n\r\nThe \"jenkins/inbound-agent:windowsservercore-ltsc2019\" image contains the Jenkins agent software. It is provided by the Jenkins CI system. Details here: https://hub.docker.com/layers/jenkins/inbound-agent/windowsservercore-ltsc2019/images/sha256-c484ee6bb3e1512dfadff764a064a66610b5b77da3f61dfb7746936c1fa09fb4?context=explore\r\n\r\nThe \"ue-jenkins-buildtools-windows\" image contains Windows Server 2019 Core, Visual Studio Build Tools, Windows SDK, and some smaller bits and pieces. We build that as part of https://github.com/falldamagestudio/UE-Jenkins-BuildTools . Here is the manifest:\r\n```\r\n{\r\n   \"schemaVersion\": 2,\r\n   \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\r\n   \"config\": {\r\n      \"mediaType\": \"application/vnd.docker.container.image.v1+json\",\r\n      \"size\": 4260,\r\n      \"digest\": \"sha256:4baac603b7c18c867e2f32acac868bc3a7a5223537d6a56b2eb9aa1e9be83622\"\r\n   },\r\n   \"layers\": [\r\n      {\r\n         \"mediaType\": \"application/vnd.docker.image.rootfs.foreign.diff.tar.gzip\",\r\n         \"size\": 1718332879,\r\n         \"digest\": \"sha256:4612f6d0b889cad0ed0292fae3a0b0c8a9e49aff6dea8eb049b2386d9b07986f\",\r\n         \"urls\": [\r\n            \"https://mcr.microsoft.com/v2/windows/servercore/blobs/sha256:4612f6d0b889cad0ed0292fae3a0b0c8a9e49aff6dea8eb049b2386d9b07986f\"\r\n         ]\r\n      },\r\n      {\r\n         \"mediaType\": \"application/vnd.docker.image.rootfs.foreign.diff.tar.gzip\",\r\n         \"size\": 720933935,\r\n         \"digest\": \"sha256:db4edcf0861ff3fdc41851a5a218965ecb2ab6cf4ec9448fb80cc4b6792fd46c\",\r\n         \"urls\": [\r\n            \"https://mcr.microsoft.com/v2/windows/servercore/blobs/sha256:db4edcf0861ff3fdc41851a5a218965ecb2ab6cf4ec9448fb80cc4b6792fd46c\"\r\n         ]\r\n      },\r\n      {\r\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\r\n         \"size\": 1098,\r\n         \"digest\": \"sha256:bb6ad75e230ef2e49ad1cae3c7e412bad862fcdb381f21a07a043e0f14bae428\"\r\n      },\r\n      {\r\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\r\n         \"size\": 940268109,\r\n         \"digest\": \"sha256:18a64b4c9b5ed8a24cf3349e5a729b6b3a0674159277c92dc8b87a976d439c2a\"\r\n      },\r\n      {\r\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\r\n         \"size\": 4920,\r\n         \"digest\": \"sha256:9b710a6555a82208bc62dedb7cb46d447082463cb6d392b2f6ac214d8c0f639d\"\r\n      },\r\n      {\r\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\r\n         \"size\": 2103155,\r\n         \"digest\": \"sha256:ddd80c318d0e19d3e894b9b8816981577931ea5e6b34f0dbf0976424afdd77a7\"\r\n      },\r\n      {\r\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\r\n         \"size\": 2511130685,\r\n         \"digest\": \"sha256:e891b7a86bed1ca60c6cd8ede698d205998797ad049cbd7b393c2d463558a2d5\"\r\n      },\r\n      {\r\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\r\n         \"size\": 469207,\r\n         \"digest\": \"sha256:21c6dff06fd7fb62eadec76f8aab4587f7a2398c244c21abbc3080589a7544f2\"\r\n      },\r\n      {\r\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\r\n         \"size\": 1334,\r\n         \"digest\": \"sha256:5c3842f7f2ed3433eb77fd895bae3c020cf546ad5c28fba55c561422b94972c6\"\r\n      }\r\n   ]\r\n}\r\n```",
        "Speeding up this on GKE is a no-go unfortunately. The pull operation is 'within' GKE itself. The only improvements we can make ourselves are to move to static/dynamic VMs (which we have over the past few months)."
      ]
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/falldamagestudio/UE-Jenkins-BuildSystem/issues/53",
    "content": {
      "title": "Convert VMs to machine images after they have been stopped for a while",
      "body": "Google has support for [machine images](https://cloud.google.com/compute/docs/machine-images) in Beta.\r\n\r\nThis allows for snapshotting an instance, with its disks and all its metadata, and storing that cheaper than having running disks. The instance can later be recreated including labels etc from the machine image.\r\n\r\n```\r\ntime gcloud beta compute machine-images create build-linux-static --source-instance build-linux-static\r\n\t-> 29s\r\ntime gcloud beta compute machine-images create build-win64-static --source-instance build-win64-static\r\n\t-> 24s\r\n\r\ntime gcloud beta compute instances create build-linux-static --source-machine-image=build-linux-static\r\ntime gcloud beta compute instances create build-win64-static --source-machine-image=build-win64-static\r\n\r\n\ttook 1 minute, goes instantly to \"started\" state (probably possible to control with various options)\r\n```\r\n\r\nWe should do a bit of math on the cost ($0.05/GB&month) for a machine image. It's the same cost as for custom images; is it the same as for snapshots? Is it only for the used portion of the disk? How does this change pricing for a build machine that is active, say, 3h/day?",
      "comments": [
        "Also -- are the above times affected by disk size? Are they affected by the amount of unique data on the disk? (There is some form of de-duplication between snapshots happening in the GCE backend)",
        "With a 600g pd-balanced disk...\r\n```\r\ntime gcloud beta compute machine-images create build-linux-static --source-instance build-linux-static\r\nwith 111G random data\r\n-> 214s\r\nwith 511G random data\r\n-> 225s\r\n\r\ntime gcloud beta compute instances create test --source-machine-image=test-with-random-511g\r\n-> 61s\r\n```\r\nNot sure why the previous test showed ~30 second image creation times and this is at ~220s.\r\n\r\nI wonder if there's a difference when using a pd-ssd?",
        "WIth a 600g pd-ssd disk...\r\n```\r\ncreate machine image from 600g pd-ssd disk with 511g random data:\r\n210s\r\n\r\ncreate 600g pd-ssd disk from machine image with 511g random data:\r\n72s\r\n```",
        "So the statistics looks like this:\r\n\r\n* Disk type probably has no impact on image creation / disk creation times\r\n* Create machine image from disk probably takes 3-4 mins in our normal situations\r\n* Create disk from machine image probably takes 70 seconds in our normal situations\r\n\r\nBased on this,\r\n* Having a second persistence step where we convert stopped VMs into machine images can be worthwhile. We could do this at, say, 1h of idleness. This will enable use of pd-ssd volumes for rarely-run jobs (since most of the time the pd-ssd doesn't exist). It also cuts away most of the cost associated with diskspace for rarely-run jobs. Expected savings: ~50% on disk costs?\r\n* Going from machine image -> running VM adds ~70s to the startup time but does not add complexity (the VM gets started automatically when it gets created)\r\n* We would need extra logic for managing the machine images (deleting them as necessary) in parallel with other ops\r\n*  Going from VM -> machine image means extra steps in Jenkins\r\n* Delays are so long that we probably need a different solution to time(out) those transitions than what we use to stop-after-idle-threshold. Something like the clean-lost-nodes worker would apply well (but that one hasn't been updated to work properly either yet)."
      ]
    },
    "codes": [
      "saving",
      "instance",
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/falldamagestudio/UE-Jenkins-BuildSystem/issues/55",
    "content": {
      "title": "Compile Windows targets on Linux",
      "body": "We should be able to get rid of the Windows OS license cost by building all on Linux. As a bonus, Linux is faster at individual file access.\r\n\r\nWe could run Visual Studio Build Tools under Wine. This should require no UE source changes. Or we could cross-compile using Clang. That probably requires UE source changes.\r\n\r\nIf we cross-build UE using Clang, will individual developers be able to use that engine build together with VS? If individual developers begin to build using Clang on their workstations as well, will they be able to debug using the VS debugger? Will there be functionality and/or performance differences between VS & Clang?",
      "comments": []
    },
    "codes": [
      "saving",
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/tesera/terraform-modules/issues/63",
    "content": {
      "title": "NAT instance",
      "body": "- similar to a bastion host, a NAT using an EC2 instance.\r\n- [x] ssh in via bastion module\r\n- update VPC module to toggle between them `nat`:`{gateway,instance}`, `nat_instance`\r\n- change min to one, but note that RDS needs two\r\n\r\nWhy: Cost. NAT Gateway cost ~$30 USD each / month (2 min)\r\n\r\nExamples:\r\n- https://github.com/terraform-community-modules/tf_aws_nat",
      "comments": []
    },
    "codes": [
      "networking",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/tesera/terraform-modules/issues/78",
    "content": {
      "title": "Ops: Billing alert",
      "body": "notify when monthly bill goes over value",
      "comments": []
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/tesera/terraform-modules/issues/141",
    "content": {
      "title": "CloudWatch Insight to decrease lambda cost",
      "body": "- push metrics https://medium.com/tensult/send-lambda-functions-usage-metrics-to-amazon-cloudwatch-for-tuning-4d5ed69341b0\r\n- https://forums.aws.amazon.com/thread.jspa?threadID=226674\r\n-  https://cloudforecast.io/blog/aws-cloudwatch-fine-tune-lambda-functions/",
      "comments": []
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Azure/sap-hana/issues/302",
    "content": {
      "title": "Addition of Load Balancer to Single Node HANA Systems",
      "body": "## Problem Statement\r\n\r\nThe current single node HANA systems built from V2 of the codebase do not have (or need) a load balancer. However, to support the scenario where a user wishes to upgrade a single node HANA system into a clustered system, we wish to add a load balancer to single node systems too.\r\n\r\nThis proposal originated from the Design Workshop, where it was stated that there is no Azure runtime cost in having a load balancer added to a single node system (or perhaps the cost is very minimal). However, adding in a load balancer into the original build, should support building out to a clustered system with negligible or no downtime.\r\n\r\n## Design Overview\r\n\r\nThis change is unlikely to warrant any change to the interface, but may require some changes to the input/output.json configuration.\r\n\r\nThe design should support both HANA 1 and HANA 2 (from design workshop notes).\r\n\r\n## Checklist\r\n\r\n- [x] V2 Documentation updated as necessary (now in #319)\r\n- [x] V2 Pipeline testing updated as necessary\r\n- [x] Differing load balancer rules for different HANA versions should be supported\r\n- [x] ~New V2 Azure resource naming should be supported (i.e. prefixes for env details, and suffixes for resource type)~ Now out of scope\r\n- [x] Ensure LB is configured with HANA VM (not RTI, which we used for testing)\r\n",
      "comments": [
        "Load Balancer and Availability Set are module that can be used for HANA HA as well as NW HA setup. \r\nHowever, this may create more dependency between the modules, which we are trying to avoid in v2.\r\nTherefore we should probably go with the option to implement this as part of module `hdb_node`.\r\n"
      ]
    },
    "codes": [
      "awareness",
      "provider"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Azure/sap-hana/issues/615",
    "content": {
      "title": "Add support for the new HANA certified VMs SKUs",
      "body": "There are new Hana supported SKUs (Azure Esv3 family and the Edsv4) however they require Ultra disk for the log volumes.\r\n\r\nOut current disk size schema does not support providing the IOPS for Ultradisk\r\nhttps://www.terraform.io/docs/providers/azurerm/r/managed_disk.html#disk_iops_read_write\r\n\r\n{\r\n                \"name\": \"log\",\r\n                \"count\": 3,\r\n                \"disk_type\": \"Premium_LRS\",\r\n                \"size_gb\": 255,\r\n                \"caching\": \"None\",\r\n                \"write_accelerator\": true,\r\n                \"mount_point\": \"/hana/log\"\r\n  }\r\n\r\ncan we add an IOPS property to the schema?\r\n\r\n",
      "comments": [
        " @user  this should be possible. Two qeustions:\r\n- Do we know if all data disks used by HANA are ultra SSD? If not, how do we identify that? The reason I'm asking that is because `disk_iops_read_write` is only settable for ultra SSD.\r\n- If we set a value for `IOPS` in `hdb_sizes.json`, will this reflect `IOPS` column in the disk configuration table in [this reference doc](https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/hana-vm-operations-storage)?\r\n\r\nThanks,\r\nNancy",
        "Ultra would primarily be for log disks because of cost reasons, I don't think we need to identify it as I believe Terraform will only use that property if the SKU is UltraSSD_LRS (https://www.terraform.io/docs/providers/azurerm/r/managed_disk.html) - but not 100% sure\r\n\r\nYes if we set the IOPS we would set the value to match the documentation",
        "Closed with https://github.com/Azure/sap-hana/releases/tag/v2.3.0-beta"
      ]
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Azure/sap-hana/issues/1038",
    "content": {
      "title": "Suggestion : Addition of 'Azure Fencing' for clusters as an alternative to SBD based fencing",
      "body": "Azure fencing agent is now available for SUSE as well as RHEL. This is easier to setup than SBD as it involves creating an RBAC role, an AD SP with role access on the Azure VM's and adding a resource type to the pacemaker cluster. Reduces cost as iSCSI VM's need not be created and failover is equally fast.\r\n\r\nWhat to do : Terraform will create a role as per link :https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-rhel-pacemaker#1-create-a-custom-role-for-the-fence-agent      and create a service principal, grant the SP role access on the VM/s. The Fencing agent resource type can be added to the cluster via an Ansible/salt playbook or be done manually at a later stage.\r\n",
      "comments": [
        "Hi @user \r\nWe do support azure fence agent for SLES (check [here](https://github.com/Azure/sap-hana/blob/f6e6b7fe322c3ad195cb5bfe5f689bbac6642ca6/deploy/ansible/roles/hana-os-clustering/tasks/cluster-Suse.yml#L147-L157)) and RHEL (check [here](https://github.com/Azure/sap-hana/blob/f6e6b7fe322c3ad195cb5bfe5f689bbac6642ca6/deploy/ansible/roles/hana-os-clustering/tasks/cluster-RedHat.yml#L59-L68)).\r\n\r\nThanks,\r\nNancy"
      ]
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/yclerc/KnowledgeGraph-Terraform-Flask-app/issues/9",
    "content": {
      "title": "test deployment with t2 micro instances",
      "body": "For cost optimization ",
      "comments": [
        "to do next",
        "done"
      ]
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/SUSE/ha-sap-terraform-deployments/issues/182",
    "content": {
      "title": "Add option to enable/disable monitoring stack in Azure",
      "body": "Right now, the monitoring stack is always deployed in Azure. This must be changed to work as in libvirt, configurable.\r\nWe should reuse the `monitoring_enabled` parameter as in libvirt,",
      "comments": [
        "I think the best solution here is to make monitoring a module. \r\nWe can start by the monitoring `module` and that should be ok",
        "> I think the best solution here is to make monitoring a module.\r\n> We can start by the monitoring `module` and that should be ok\r\n\r\nIf it's a viable solution it's OK for me. My concern is that if this would request to add a main.tf file and so on... what means not trivial changes. This requires some research.\r\n\r\nBut the final goal is clear, enable/disable the monitoring stack as wished.",
        "> If it's a viable solution it's OK for me. My concern is that if this would request to add a main.tf file and so on... what means not trivial changes. This requires some research.\r\n\r\nMy main concern is just to have a way to do this for 2.2.0 version and not in 6 months if possible :). If it's already available for libvirt (`monitoring_enabled`) why it's not possible to do the same for Azure (and the others PC providers).\r\n\r\nThe last issue we are facing is that `hanadb_exporter` requires `python3.6` and this version is not installed by default on 12-SP5 PC images, and it's not possible to install it, as this OS is not available on SCC during Build Validation. As `python3.6` will be available in SCC after release of 12-SP5, it will be easy to install it after registration.\r\nSo, as monitoring is not needed/useful for QA validation, the easiest way for workaround this is to disable it.",
        "regarding the enabled/disabled we are working on this sprint . so we should have it.\r\n\r\nRegarding the python `hanadb_exporter` I think is another issue which we might should track elsewhere and reference it here.",
        "Correct me if I'm wrong but from my understand, `monitoring_enabled` doesn't control if you want the monitoring server or not in Terraform. I mean it's just a variable used by salt to load sls files in monitoring folder. Whatever if `monitoring_enabled` is set to `true` or `false`, a monitoring server will be created by terraform. So this variable name is a little confused. WDYT?",
        "Yep Salt cannot control terraform. The monitoring enabled refer yes to provisions. We need to separate concerns between deployment and provision. We will need to provide different main.tf as inventory of use cases. This is how sumaform solve issue in a easy way. We can just do it same. In a main. Tf without monitoring module the other with ",
        "For sure the monitoring VM shouldn't be provisioning if not needed, as we have to reduce/control costs during our tests and so creating an unneeded VM is not so good ;-)",
        " @user regarding the exporter can you open a separate issue and refer to https://github.com/SUSE/ha-sap-terraform-deployments/issues/183. We need to keep track of that. I think it is a serious thing to consider on our release ",
        "Yep I full agree. We should not create the monitor vm by default. This will come this sprint. I will do a pr for libvirt also unless @user isn't already on it",
        "> @user regarding the exporter can you open a separate issue and refer to #183. We need to keep track of that. I think it is a serious thing to consider on our release\r\n\r\nOK, will do it asap!",
        "Issue #199 opened for `hanadb_exporter` issue."
      ]
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/SUSE/ha-sap-terraform-deployments/issues/482",
    "content": {
      "title": "iSCSI based SBD",
      "body": "is iSCSI target based SBD in this deployment better than using cloud-native STONITH agents?\r\n\r\nFor example:\r\nAWS - Overlay IP-based STONITH: external/ec2\r\nAzure - Fencing Agent-based STONITH: fence_azure_arm \r\nGCP - external/gcpstonith\r\nAli Cloud: fence_aliyun\r\n\r\nFor SAP deployments typically stability is key, but what is the technicalities and rationale behind NOT using cloud-native Fencing agent?\r\nIn Azure, Microsoft cautioned the downside of using its own fence agent, besides the longer failover what would be other reasons?\r\n\r\n\u201cIf you don't want to invest in one additional virtual machine, you can also use the Azure Fence agent. The downside is that a failover can take between 10 to 15 minutes if a resource stop fails or the cluster nodes cannot communicate which each other anymore.\u201d",
      "comments": [
        "Hi @user \n\nI will not say what is better or not, because all of them have pros and cons.\n\nSBD is a platform agnostic fencing mechanism, so it could be an advantage to use it in case you have hybrid environment, like running cluster on premises and in the cloud, or in different clouds, so what could help in terms of learning curve and adminstration since you could have the same standard on all clusters. On the other side, it comes with the overhead of administering an iSCSI target extra machine as there is (not yet) shared block devices on the Cloud Providers. \n\nThe Cloud Providers native fence agents uses specifics APIs for each provider and in some of them there is this limitation of long reaction times (AFAIK improvements on that regards are being done in all of them). On the other side they are actively maintained, tested and certified by the providers, so improvements are always being introduced, and you save the costs of extra machines and disks only for the fencing mechanism.\n\nSo basically you have choices and need to pick what fits better for your environment.\n\nHope it answers your question.",
        "thanks for the detailed explanation! Appreciate it"
      ]
    },
    "codes": [
      "saving",
      "instance",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/SUSE/ha-sap-terraform-deployments/issues/227",
    "content": {
      "title": "Set a default Grafana user dashboard",
      "body": "It would be nice to have the main dashboard open up automatically when browsing the monitoring server.\r\nThis is not supported by provisioning yet, but can be accomplished by performing a PUT request to the Grafana HTTP API as follows:\r\n\r\n```\r\ncurl -X PUT \\\r\n    -H 'X-Grafana-Org-Id: 1' \\\r\n    -H 'Content-Type: application/json' \\\r\n    -d '{\"homeDashboardId\": 1}' \\ \r\n    $MONITORING_IP/api/user/preferences\r\n```\r\n\r\nI guess we could use the salt http module in [salt/monitoring/init.sls](https://github.com/SUSE/ha-sap-terraform-deployments/blob/05c61526e17ba2edfe3b7f2d5630795d3a252ad9/salt/monitoring/init.sls)?",
      "comments": [
        " @user  I think personally setting a default perhaps one day grafana will make this possible somehow by provisioning. Imho I'm also ok to add this kind of feature. But it has a little cost since we are doing a post to an API that could perhaps change and fail. So imho for so little avantage. we can still wait. Call me a minimalist pessimist :grin: ",
        " @user imho retrospectively do we need this? I mean imho there is no real default dashboard, since the are several use-case and focuses (hana, netweaver, cluster) etc. \r\n\r\nImho we can close this",
        "I don't know, I think we should show the multicluster overview by default. @user what do you think about this? Do you think it's desirable?",
        " @user  imho can we close this, or we add a default. I personally don't like the `curl` maybe grafana new version has this feature.",
        " @user  I do think settting a dashboard default is not technically right.\r\n\r\nSince we do have rpm packages, we could set X dashboard default and maybe this is missing because the customer didn't install the `ha-dashboard` .. \r\n\r\nI would in favor to close it , since I don't see the advantage of putting effort to fix this.",
        "how is this tech debt? it's not even implemented! \ud83d\ude01 \r\nyeah, I guess we can close it.",
        "thx for your understanding and let focus on things which will give value.. hopefully :grin: "
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/SUSE/ha-sap-terraform-deployments/issues/583",
    "content": {
      "title": "Improve documentation with default values written in examples",
      "body": "# description:\r\n\r\nin our terraform.tfvars.example file, we should state more **explicitly** what are the `default` values for X parameter and what are the possible values.\r\n\r\nfor example:\r\n\r\nA file contains this\r\n\r\nwe should also state in the `example.file` that if the variable are commented, they are not `mandatory` to set..\r\n\r\nA 1 line comment about this could also help readability\r\n\r\n```hcl\r\n# Cost optimized scenario\r\n#scenario_type: \"cost-optimized\"\r\n\r\n# To disable the provisioning process\r\n#provisioner = \"\"\r\n\r\n# Run provisioner execution in background\r\n#background = true\r\n```\r\n\r\n\r\nshould be like:\r\n```hcl\r\n# Cost optimized scenario. Default `performance-optimized`, possible value `cost-optimized`.\r\n#scenario_type: \"cost-optimized\"\r\n\r\n# To disable the provisioning process. Default: Enabled\r\n#provisioner = \"\"\r\n\r\n# Run provisioner execution in background. Default Disabled\r\n#background = true\r\n```\r\n\r\nThis make more clear to the user and he doesn't need to check all the variables in HCL default\r\n\r\n",
      "comments": [
        "Can I also add the explanation of some those more complex variables like:\r\ndisks_type, disks_size, etc.  And how they fit together?",
        " @user @tuttipazzo \r\nThe default values and the descriptions are already available in the `variables.tf` file.\r\nI wouldn't like to start duplicating the content in both files.\r\n\r\nMaybe, we need to enforce somehow the users to check that file, and obviously improve the descriptions if they are not good enough.\r\n\r @user Just as a curiosity, why don't you like HCL syntax? I find it easy to understand and consume, even without any technical skill.\r\n",
        ">The default values and the descriptions are already available in the variables.tf file.\r\nYeah.  I didn't know the variable definitions were in another file until I started greping.  \r\n\r\n>Maybe, we need to enforce somehow the users to check that file, and obviously improve the descriptions if they are not good enough.\r\nI think a 1 line at the beginning of the example tfvars file is enough.\r\n\r\nHCL?  I think that is a typo.\r\n",
        "> > The default values and the descriptions are already available in the variables.tf file.\r\n> > Yeah.  I didn't know the variable definitions were in another file until I started greping.\r\n> \r\n> > Maybe, we need to enforce somehow the users to check that file, and obviously improve the descriptions if they are not good enough.\r\n> > I think a 1 line at the beginning of the example tfvars file is enough.\r\n> \r\n> HCL? I think that is a typo.\r\n\r\nThis is already done in `develop` branch: https://github.com/SUSE/ha-sap-terraform-deployments/blob/develop/azure/terraform.tfvars.example#L3\r\n\r\nMore information about HCL syntax:\r\nhttps://www.terraform.io/docs/configuration/syntax.html\r\nhttps://hub.packtpub.com/what-is-hcl-hashicorp-configuration-language-how-does-it-relate-to-terraform-and-why-is-it-growing-in-popularity/",
        "`Find all the available variables and definitions in the variables.tf file` is stated in every `terraform.tfvars.example`\r\nMore complex variables are explained in the `README.md` files also.\r\nIf something concrete is missing, please file a new issue."
      ]
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/SUSE/ha-sap-terraform-deployments/issues/466",
    "content": {
      "title": "Align VM size for DRBD on all providers",
      "body": "`t2.micro` is used for AWS, which is a VM with 1 vCPU and 1 GB of RAM, while `n1-standard-4` is used for GCP (4 vCPUs and 15GB of RAM) and `Standard_D2s_v3` for Azure (2 vCPUs and 8GB of RAM).\r\n\r\nCould be better to have the same kind of default VM for all providers, no?\r\n\r\nAs some power may be needed on the DRBD cluster maybe we can use `Standard_D4s_v3` for Azure, `t2.xlarge` for AWS and `n1-standard-4` for GCP? All providers should pretty much have 4 vCPUS and ~16GB of RAM.",
      "comments": [
        " @user  is the PR linked fixing this? TIA",
        " @user Do you think this is still valid to change?\r\nTo not be \"too expensive\" in the default setup, one could also use 2 vCPUs and ~8GB RAM as the default.\r\nFor production setups one has to consider the actual workload anyhow.",
        "DRBD is not that of an priority right know."
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/cn-terraform/terraform-aws-networking/issues/1",
    "content": {
      "title": "Need a env to configure/disable nat gateway",
      "body": "I would like a way to not create a natgw or create at the max one and route all traffic from private subnet to this single nat_gw, they are expensive ",
      "comments": [
        "The idea of having one nat gateway per availability zone is for redundancy in case of failure. I'll start working on a solution to match both cases the one is right now and the one you proposed",
        "Already implemented, PR coming soon.",
        "Released in version [2.0.11](https://registry.terraform.io/modules/cn-terraform/networking/aws/2.0.11)\r\n\r\nThanks to @user "
      ]
    },
    "codes": [
      "networking",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Genaker/TerraformMagentoCloud/issues/25",
    "content": {
      "title": "Attache EFS Shared Network File System to Magento instances ",
      "body": "AWS Documentation: https://docs.aws.amazon.com/efs/latest/ug/wt1-test.html\r\n\r\nAmazon EFS offers two storage classes, Standard and Infrequent Access. Standard is used for frequently accessed files and Infrequent Access is used for Infrequent access files to store files more cost-effectively. EFS IA storage class costs 85% less than the EFS Standard class and you need to pay a fee each time you read from or write to a file. EFS Standard is intended to give single-digit latencies by and large, and EFS IA is intended to give two-digit latencies overall.\r\n\r\nAmazon EFS One Zone storage classes, reducing storage costs by 47% compared to Amazon EFS Standard storage classes. As an example, in the US East (N. Virginia) Region, this allows customers to achieve an effective storage price of $0.043/GB-month, assuming you are using lifecycle management and 80% of your data is infrequently accessed. Amazon EFS is designed for eleven 9\u2019s of durability, and EFS One Zone Storage classes offer a three 9\u2019s availability SLA while maintaining the same capabilities as Standard storage classes such as elasticity, simplicity, scalability, and lifecycle management.\r\n\r\nMagento requires mounts next folders:\r\n- app/etc\r\n- pub/media\r\n- var/export\r\n- var/log (optionally) you need sync logs or use one server. \r\n\r\nConsider using a single server installation. it works much better than an Autoscaling model.\r\n\r\nImportant!: don't mount pup/static it will decrease deployment speed dramatically. \r\n\r\n![image](https://user-images.githubusercontent.com/9213670/134841715-14f10f5c-9c14-4da8-a068-cd0fd3663b9f.png)\r\n",
      "comments": []
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Genaker/TerraformMagentoCloud/issues/17",
    "content": {
      "title": "EC2 instances don't have access to internet",
      "body": "I can't follow the instructions on https://github.com/Genaker/Magento-AWS-Linux-2-Installation because none of the instances have any ability to use the commands as they don't have internet access. wget or yum update or ping doesn't work. I also cannot ssh and have to use the CLI on aws because there is no key pair.",
      "comments": [
        "I also don't understand what any of this is https://github.com/Genaker/TerraformMagentoCloud/issues/4 I find it to be important as I do have to implement an ssl cert.",
        "How do you stop the instances? Every time I try stopping it, new instances pop up and it costs money. Instead of disabling the auto scaling groups, is there a button that just chills everything out?\r\n\r\nIt seems like every single time I stop them it gets terminated and all data gets lost. How do I keep the data saved on ec2? I feel like I'm not deploying magento right. What are the other ec2 nodes for if magento is on the webnode?\r\n\r\nI was able to finally install magento by adding a allow all rule for outgoing in the security groups for the webnode.",
        "To stop the instance you need change Auto Scaling group capacity to 0. If\nyou just stop it it will create new one and maintain the same size 1\n\nOn Sat, Sep 18, 2021 at 6:59 PM polkunus ***@***.***> wrote:\n\n> How do you stop the instances? Every time I try stopping it, new instances\n> pop up and it costs money.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/Genaker/TerraformMagentoCloud/issues/17#issuecomment-922402270>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACGJNZTGTZCVXRJDC277CE3UCU7YBANCNFSM5EJZJBAA>\n> .\n>\n",
        "Thanks for the follow up. How would I ftp into the ec2 instance to modify files and upload data? There is no key pair. I feel like I'm not really doing something correctly as it seems pretty dangerous that stopping the servers wipes your data entirely and automatically. \r\n\r\nIf I followed the instructions on here and the other magento repository, are no other configurations needed to fully utilize the features of this repo of a autoscaling magento with varnish cache? And if so, is most of the system administration is interacting with the webnode?\r\n\r\nSorry for the questions, I am a novice at this. Very very lost.",
        "If doesn\u2019t have ftp. Look for sftp/ssh. It has insurance for varnish you\nneed to use target group to connect the instances.\n\nOn Sat, Sep 18, 2021 at 9:28 PM polkunus ***@***.***> wrote:\n\n> Thanks for the follow up. How would I ftp into the ec2 instance to modify\n> files and upload data? There is no key pair. I feel like I'm not really\n> doing something correctly as it seems pretty dangerous that stopping the\n> servers wipes your data entirely and automatically.\n>\n> If I followed the instructions on here and the other magento repository,\n> are no other configurations needed to fully utilize the features of this\n> repo of a autoscaling magento with varnish cache? And if so, is most of the\n> system administration is interacting with the webnode?\n>\n> Sorry for the questions, I am a novice at this. Very very lost.\n>\n> \u2014\n> You are receiving this because you commented.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/Genaker/TerraformMagentoCloud/issues/17#issuecomment-922413572>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACGJNZWKDBDAHOOHPCQKRFTUCVRHXANCNFSM5EJZJBAA>\n> .\n>\n",
        "Thanks I will look into whatever target groups are.\r\n\r\nAs for connecting ssh, I still don't understand how that'd work as none of the instances have keynames and from my experience you can only assign keypairs (pems) when creating the instance.",
        "I don\u2019t remember exactly how it works. Probably you need add key in the AWS\naccount and to the Terraform code.\n\nOn Sat, Sep 18, 2021 at 9:36 PM polkunus ***@***.***> wrote:\n\n> Thanks I will look into whatever target groups are.\n>\n> As for connecting ssh, I still don't understand how that'd work as none of\n> the instances have keynames and from my experience you can only assign\n> keynames (pems) when creating the instance.\n>\n> \u2014\n> You are receiving this because you commented.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/Genaker/TerraformMagentoCloud/issues/17#issuecomment-922414076>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACGJNZWDKTMHX245N53IMU3UCVSGVANCNFSM5EJZJBAA>\n> .\n>\n",
        "Ok I'll try to figure that out. Thanks again.\r\n\r\nI have an ecommerce website that is nearly dormant most of the month with 2-5 visitors a day but about 2 days of the month has around 35k clicks each in about an hour time, is there any way i can further downsize the application, possibly even make the instances types smaller on the off days without screwing things up?",
        "I don\u2019t think this solution can help you. It is really complex and\nexpensive: you need separate DB server, Redis, ElasticSearch server. The\nbest is to use this approach:\n\nhttps://link.medium.com/TXGHNfMEFjb\n\n\nOn Sat, Sep 18, 2021 at 9:51 PM polkunus ***@***.***> wrote:\n\n> Ok I'll try to figure that out. Thanks again.\n>\n> I have an ecommerce website that is nearly dormant most of the month with\n> 2-5 visitors a day but about 2 days of the month has around 35k clicks each\n> in about an hour time, is there any way i can further downsize the\n> application, possibly even make the instances types smaller on the off days\n> without screwing things up?\n>\n> \u2014\n> You are receiving this because you commented.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/Genaker/TerraformMagentoCloud/issues/17#issuecomment-922415009>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACGJNZSJ2JJCY2BTIIZIKYDUCVT4JANCNFSM5EJZJBAA>\n> .\n>\n",
        "Oh great, really appreciate you putting me in the right direction. I don't my sparing the expense for a solution that can provide the numbers during the active peaks of 35k concurrent users, but like the idea of active scalability so I only pay for what I use when the site is dormant. Is there a reason why you suggest this over the amazon outside of its complexity?",
        "Better price and performance.\n\nOn Tue, Sep 21, 2021 at 4:36 AM polkunus ***@***.***> wrote:\n\n> Oh great, really appreciate you putting me in the right direction. I don't\n> my sparing the expense for a solution that can provide the numbers during\n> the active peaks of 35k concurrent users, but like the idea of active\n> scalability so I only pay for what I use when the site is dormant. Is there\n> a reason why you suggest this over the amazon outside of its complexity?\n>\n> \u2014\n> You are receiving this because you commented.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/Genaker/TerraformMagentoCloud/issues/17#issuecomment-923896223>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACGJNZWNDNO3UTD73ZP3GV3UDBU3FANCNFSM5EJZJBAA>\n> .\n>\n",
        "Really cool. Thanks. I set it up, but I don't understand how to implement a load balancer and caching, its not in the tutorial.",
        "This is an open source project, please provide/contribute some information\nhow it works what you fixed etc.\n\nUnfortunately Varnish is outdated technology you can remove it ;)\n\nYou need just 1 load balancer with admin and web. Sometimes even Magento\nadmin node is not required.\n\nYou need setup target group with web end admin nodes as a target:\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html\n\n\nOn Tue, Sep 21, 2021 at 6:36 PM polkunus ***@***.***> wrote:\n\n> Really cool. Thanks. I set it up, but I don't understand how to implement\n> a load balancer and caching, its not in the tutorial.\n>\n> \u2014\n> You are receiving this because you commented.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/Genaker/TerraformMagentoCloud/issues/17#issuecomment-924511218>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACGJNZTXAI3QDNUDWKPQMEDUDEXK3ANCNFSM5EJZJBAA>\n> .\n>\n"
      ]
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/slalom/dataops-infra/issues/19",
    "content": {
      "title": "Feature Request: Infra Catalog usage alerts or cost-based alerts",
      "body": "User Story:\r\nAs a user of the Infrastructure Catalog, I want to have the peace of mind that if my spending or cloud usage exceeds a specific dollar amount, I will be notified via email and be more likely to avoid any large unexpected cloud charges.",
      "comments": []
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/slalom/dataops-infra/issues/18",
    "content": {
      "title": "Feature Request: Infra Catalog \"Self-Destruct\" timer",
      "body": "User Story: \r\nAs a tester and user of the Infrastructure Catalog, I want to know that resources I spin up in Terraform will be automatically deleted after a specific number of days, and that I won't accidentally incur ongoing costs if I forget to delete or otherwise fail to delete the test infrastructure.\r\n\r\nPossible solution = \"Cloud Nuke\": https://github.com/gruntwork-io/cloud-nuke",
      "comments": [
        "Related: https://github.com/gruntwork-io/cloud-nuke/issues/88"
      ]
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/npalm/terraform-aws-gitlab-runner/issues/246",
    "content": {
      "title": "Support for multiple docker machines within a single runner instance",
      "body": "I'm currently doing small research in order to migrate my current GitLab runner setup (done with CloudFormation, modified and updated by hand via SSH) to Terraform. I'm currently using a setup with configuration like this:\r\n\r\n```toml\r\nconcurrent = 20\r\ncheck_interval = 0\r\n\r\n[session_server]\r\n  session_timeout = 3600\r\n\r\n[[runners]]\r\n  name = \"team-a-runner\"\r\n  url = \"https://gitlab.(redacted)/\"\r\n  token = \"(team-a token redacted)\"\r\n  executor = \"docker+machine\"\r\n  [runners.docker]\r\n    tls_verify = false\r\n    image = \"alpine:latest\"\r\n    privileged = true\r\n    disable_entrypoint_overwrite = false\r\n    oom_kill_disable = false\r\n    disable_cache = true\r\n    volumes = [\"/certs/client\", \"/cache\"]\r\n    shm_size = 0\r\n  [runners.cache]\r\n    Type = \"s3\"\r\n    Shared = true\r\n    [runners.cache.s3]\r\n      ServerAddress = \"s3.amazonaws.com\"\r\n      BucketName = \"(redacted)\"\r\n      BucketLocation = \"eu-central-1\"\r\n  [runners.machine]\r\n    IdleCount = 0\r\n    IdleTime = 1200\r\n    MaxBuilds = 100\r\n    MachineDriver = \"amazonec2\"\r\n    MachineName = \"gitlab-docker-machine-%s\"\r\n    MachineOptions = [\"engine-storage-driver=overlay2\", \"amazonec2-instance-type=m5.large\", \"amazonec2-region=eu-central-1\", \"amazonec2-vpc-id=(redacted)\", \"amazonec2-security-group=(redacted)\", \"amazonec2-use-private-address=true\", \"amazonec2-iam-instance-profile=(redacted)\"]\r\n    OffPeakPeriods = [\"* * 0-9,17-23 * * mon-fri *\", \"* * * * * sat,sun *\"]\r\n    OffPeakTimezone = \"Etc/UTC\"\r\n    OffPeakIdleCount = 0\r\n    OffPeakIdleTime = 300\r\n\r\n[[runners]]\r\n  name = \"team-b-runner\"\r\n  url = \"https://gitlab.(redacted)/\"\r\n  token = \"(team-b token redacted)\"\r\n  executor = \"docker+machine\"\r\n  (same as above)\r\n\r\n[[runners]]\r\n  name = \"team-c-runner\"\r\n  url = \"https://gitlab.(redacted)/\"\r\n  token = \"(team-c token redacted)\"\r\n  executor = \"docker+machine\"\r\n  (same as above)\r\n\r\n[[runners]]\r\n  name = \"team-d-runner\"\r\n  url = \"https://gitlab.(redacted)/\"\r\n  token = \"(team-d token redacted)\"\r\n  executor = \"docker+machine\"\r\n  (same as above)\r\n\r\n[[runners]]\r\n  name = \"team-e-runner\"\r\n  url = \"https://gitlab.(redacted)/\"\r\n  token = \"(team-e token redacted)\"\r\n  executor = \"docker+machine\"\r\n  (same as above)\r\n```\r\n\r\nI defined multiple `[[runners]]` section in order to register separate runners for each Gitlab group (I can't register global runner in our company). A lot of improvements can be made to this setup, yet it's a few times cheaper than defining each runner group separately. So I have a question regarding registering more than one group of `docker-machine` instances inside a single GL runner, as above. Is it a bad practice to register multiple runners? \r\n\r\nI know it's doable within Terraform, as we may simply loop over instance types and generate more `[[runners]]` sections. If not, why not define 12 different groups that are configured with 12 different EC2 instance types and create our own \"spot fleet\" within a single runner? ",
      "comments": [
        "In my opinion it is not a bad practice at all to have multiple runners, we have as well. We have deployed the module a few times. To match different use cases, some of them ar dedicated to a team.\r\n\r\nWith Terrafomr 13 it should also be possible to loop over modules, so would it be an option to deploy the module one time for each time, the overad is in that case that you have 1 small ec2 instances for the agent running per team. ",
        " @user To try to avoid such cost overhead mentioned by you, I created proposal PR which should enable some cool stuff, like adding multiple different docker-machine configurations that run on spot. I'll check it tomorrow in practice on my side, please take a look and tell me what you think \ud83d\ude09"
      ]
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/npalm/terraform-aws-gitlab-runner/issues/245",
    "content": {
      "title": "Instance type recommendations",
      "body": "I would be interested in recommendations for preferred instance types for different scenarios.\r\n\r\nUp until now I have been using the general purpose (m4/m5) instances, currently 2xlarge, for compiling, testing and deployment jobs, but have recently found the spot capacity to be quite limited and the disk performance could be better.\r\n\r\nI am now considering switching to c5.4xlarge for better CPUs and unrestricted EBS IO  but they come at a significantly higher cost. Or alternatively the SSD volumes of the i3.large may work better at a lower cost as long as the slower CPU clock speed doesn't make a significant difference.\r\n\r\nHas anyone spent time optimising instance types and have any good recommendations in this area?",
      "comments": [
        "We are using m4.xlarge instances to build Java projects and haven't found any problems so far. Also we didn't discover any problems to buy the spot instances.\r\n\r\nFor terraform jobs we do not use spot instances as the terraform state might be damaged if a terraform job is cancelled in between.",
        "> We are using m4.xlarge instances to build Java projects and haven't found any problems so far. Also we didn't discover any problems to buy the spot instances.\r\n> \r\n> For terraform jobs we do not use spot instances as the terraform state might be damaged if a terraform job is cancelled in between.\r\n\r\nThanks for you reply @user  We used m4.xlarge for a while too but I think AWS were phasing them out in favor of the m5 types so we switched. We had a problem with a compatibility issue with m5 and NodeJS npx command which required a workaround.\r\nWe also have had some issue with terraform jobs being cancelled while running when spot capacity was low. We have had to build a self-service tool to remove the lock from the DynamoDb table when the job is cancelled before the lock is released because it was becoming a fairly common issue! Do you deploy this terraform module multiple times, with some using spot and some plain on-demand?",
        " @user Yeah, we deploy the module twice: terraform runner and non terraform runner. The first uses on demand with 0 idle instances and the latter uses spot instances with some put on idle.\n\nThis way we make sure not to damage the terraform state file."
      ]
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/npalm/terraform-aws-gitlab-runner/issues/259",
    "content": {
      "title": "Add variable to disable detailed monitoring on the runner instance",
      "body": "I noticed there is a variable `runners_monitoring` (default: false) available which can enable/disable detailed Cloudwatch monitoring on the docker-machine instances. \r\n\r\nHowever, when checking the Terraform plan, the `module.runner.aws_autoscaling_group.gitlab_runner_instance` receives `EC2 detailed monitoring` enabled by default and I don't need detailed monitoring (not spinning up a critical stack for me). So I would like to disable that (for the cost).  \r\n\r\n![image](https://user-images.githubusercontent.com/1352979/95268689-1e088e00-0838-11eb-8159-f488aa8be92a.png)\r\n\r\n\r\n\r\nSeems like detailed monitoring (`enable_monitoring`) is enabled by default on `aws_launch_configuration`: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/launch_configuration\r\n\r\nIs there any way to merge or override the `template_gitlab_runner` [locals of the module](https://github.com/npalm/terraform-aws-gitlab-runner/blob/develop/main.tf#L55-L75) in my own terraform code? Or should a variable be added to control detailed monitoring of the gitlab runner instance? ",
      "comments": [
        "Seems monitoring is by default enabled in the auto launch configuration (https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/launch_configuration#enable_monitoring). The auto scaling group is managing the gitlab agent, the runners are managed via docker machine and should have by default monitoring disabled. \r\n\r\nTo disable monitoring for the instance running in the scaling group we have to create an extra variable. Feel free to propose a PR to introduce more flexibility in the module."
      ]
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/npalm/terraform-aws-gitlab-runner/issues/277",
    "content": {
      "title": "'docker-machine: cannot execute binary file' error on startup",
      "body": "Hello,\r\n\r\nI have just updated to the latest release of the module (4.21.0), previously on 4.20.0.\r\n\r\nThe user data script is failing to execute to completion.\r\nI think the script ends when it gets to [template/gitlab-runner.tpl#L39](https://github.com/npalm/terraform-aws-gitlab-runner/blob/2ccd3d8d6b5cb609c32c98d7b262c1530d638401/template/gitlab-runner.tpl#L39).\r\n\r\nThe log shows:\r\n\r\n```\r\nuser-data: + curl --fail --retry 6 -L https://gitlab-docker-machine-downloads.s3.amazonaws.com/v0.16.2-gitlab.10/docker-machine-Linux-aarch64\r\nuser-data: % Total % Received % Xferd Average Speed Time Time Time Current\r\nuser-data: Dload Upload Total Spent Left Speed\r\nuser-data: #015  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0#015  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0#015 14 41.5M   14 6223k    0     0  5039k      0  0:00:08  0:00:01  0:00:07 5035k#015 61 41.5M   61 25.5M    0     0  11.5M      0  0:00:03  0:00:02  0:00:01 11.5M#015100 41.5M  100 41.5M    0     0  14.0M      0  0:00:02  0:00:02 --:--:-- 14.0M\r\nuser-data: + chmod +x /tmp/docker-machine\r\nuser-data: + mv /tmp/docker-machine /usr/local/bin/docker-machine\r\nuser-data: + ln -s /usr/local/bin/docker-machine /usr/bin/docker-machine\r\nuser-data: + docker-machine --version\r\nuser-data: /var/lib/cloud/instance/scripts/part-001: line 182: /usr/local/bin/docker-machine: cannot execute binary file\r\ncloud-init: Jan 18 16:34:37 cloud-init[2399]: util.py[WARNING]: Failed running /var/lib/cloud/instance/scripts/part-001 [126]\r\ncloud-init: Jan 18 16:34:37 cloud-init[2399]: cc_scripts_user.py[WARNING]: Failed to run module scripts-user (scripts in /var/lib/cloud/instance/scripts)\r\ncloud-init[2399]: util.py[WARNING]: Running module scripts-user (<module 'cloudinit.config.cc_scripts_user' from '/usr/lib/python2.7/site-packages/cloudinit/config/cc_scripts_user.pyc'>) failed\r\ncloud-init: ci-info: no authorized ssh keys fingerprints found for user ec2-user.\r\n```\r\n\r\nI have been looking at this a while and not sure what to try to fix it. Do you have any idea what the problem could be?",
      "comments": [
        "I expect the docker-machine version is not compatible with the runner type as it appears to be compiled for ARM. How can I find a listing of docker-machine download URL's?",
        "Found a listing of docker-machine builds [here](https://gitlab-docker-machine-downloads.s3.amazonaws.com/master/index.html).\r\n\r\nAdded an input to the **terraform-aws-gitlab-runner** module\r\n```\r\ndocker_machine_download_url = \"https://gitlab-docker-machine-downloads.s3.amazonaws.com/v0.16.2-gitlab.10/docker-machine-Linux-x86_64\"\r\n```\r\n\r\nIt now works with my instance types.",
        "I have updated the defaults to the gitlab matained one. This since this version is still maintained. Did not checkec impact for other architectures then the one in the default example. ",
        "The issue here is that the default runner agent machine type is a `t3.micro` x86 VM, but the docker-machine version in the defaults is an AArch64 one, hence this falling over if you \"just\" update without reading and understanding the changes made.\r\n\r\nI guess that the default for one of these needs to be changed: either update the `t3.micro` default to an equivalent AArch64 machine type or update the default docker machine url to the x86 version.\r\n\r\nTo be honest, as the AArch64 VMs are (supposed to be) cheaper than the x86 ones and it's the new hotness, maybe it's time to update the default machine type for the runner agent?",
        "Hey same here with latest version `4.22.0` + `Gitlab runner 13.8.0` from a fresh setup.\r\nThe runner was not autoRegister (failed initScript) but the machine `t3.micro` was still running with green lights (on AWS).\r\n\r\nAdded  ` docker_machine_download_url = \"https://gitlab-docker-machine-downloads.s3.amazonaws.com/v0.16.2-gitlab.10/docker-machine-Linux-x86_64\"` to my module config and it works now.\r\n\r\nPerhpas it could be great to put it as default or like @user suggests update the default machine type for the runner agent.\r\n\r\nWe could reopen this maybe."
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/npalm/terraform-aws-gitlab-runner/issues/124",
    "content": {
      "title": "Pass an EIP to the EC2 instance created",
      "body": "In some cases when using spot, I'd like to be able to provide an EIP and reuse them in case the spot is terminated. \r\n\r\nThis is useful to prevent changing inbound rules that uses the runner's public IP as a source in their inbound rule.",
      "comments": [
        " @user feel free to propose a PR. I use the runners in a private subnet, so no public ones are attached.",
        "I will work on a PR. The issue with having them in a private subnet is that the NAT Gateway costs can become prohibitive",
        "related issue #92 ",
        "I've started working on this issue: https://github.com/roock/terraform-aws-gitlab-runner/commit/c204e4ba3a0427eedb1386a214bda31d1c287306\r\n~i t is working when enabling the use of EIP, but for disabling the feature I need a way to reference a non existing object (aws_eip is only create if the flag is set)~\r\nseems to work fine, tested with runner on public subnet with and without flag enabled @user what do you think?",
        " @user do not hard code region and please do rebase from upstream and open PR",
        " @user ups, thx for the hint",
        "Assignment of EIP to the Runner server itself was added in #161 and #165. Not sure if it is possible to add support for EIPs for the docker-machine servers though.",
        " @user seems not supported by the aws docker machine driver https://docs.docker.com/machine/drivers/aws/",
        "A possibility would be to use the user_data to assign an EIP to the docker-machine instances.",
        "Is it really necessary to expose the runners? You could also access your machines via AWS Console (SSM access). Seems to be easier to configure and less risky.\r\n\r\nIf we are talking about the agent: There should be an inbound rule which allows traffic from your Gitlab instance only. And the communicatin agent <-> runner should be safeguarded through the module itself using security groups.\r\n\r\nWe should be fine without an EIP at all or do I miss something? Any other use case?"
      ]
    },
    "codes": [
      "networking",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/npalm/terraform-aws-gitlab-runner/issues/222",
    "content": {
      "title": "Add option to request spot-instance with on-demand price",
      "body": "When leaving spot-price empty, the price defaults to on-demand price, no need to come up with arbitrary numbers.\r\n\r\nhttps://aws.amazon.com/blogs/compute/new-amazon-ec2-spot-pricing/",
      "comments": [
        " @user The module is only taking care of creating the infrastructure. Creating the ec2 instances is handled by the aws provider for docker machine. So as for I can see there is no option to support this feature in this module. It is the gitlab agent that request ec2 instances via docker machine. What is your view?",
        "Perhaps if docker-machine's `--amazonec2-spot-price` flag is left undefined.  \r\nNot sure what's the default for it though.  ",
        "This is an interesting topic, taking a look at the source code for the [AWS driver `docker-machine` ](https://github.com/docker/machine/blob/master/drivers/amazonec2/amazonec2.go), the default value for defaultSpotPrice is set to $0.5. This piece of code has not been recently updated though.\r\n\r\n```\r\nconst (\r\n\tdriverName                  = \"amazonec2\"\r\n\tipRange                     = \"0.0.0.0/0\"\r\n\tmachineSecurityGroupName    = \"docker-machine\"\r\n\tdefaultAmiId                = \"ami-c60b90d1\"\r\n\tdefaultRegion               = \"us-east-1\"\r\n\tdefaultInstanceType         = \"t2.micro\"\r\n\tdefaultDeviceName           = \"/dev/sda1\"\r\n\tdefaultRootSize             = 16\r\n\tdefaultVolumeType           = \"gp2\"\r\n\tdefaultZone                 = \"a\"\r\n\tdefaultSecurityGroup        = machineSecurityGroupName\r\n\tdefaultSSHPort              = 22\r\n\tdefaultSSHUser              = \"ubuntu\"\r\n\tdefaultSpotPrice            = \"0.50\" <<-----------------------\r\n\tdefaultBlockDurationMinutes = 0\r\n)\r\n```\r\nI think it may be worth raising an issue on their side so we might have a developer taking an action and adopting the default sane AWS approach for matching the on-demand EC2 price instead of an arbitrary `0.5`\r\n\r\n[AWS - API_RequestSpotInstances](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RequestSpotInstances.html)\r\n\r\n> SpotPrice - The maximum price per hour that you are willing to pay for a Spot Instance. The default is the On-Demand price.\r\n> \r\n>     Type: String\r\n> \r\n>     Required: No",
        "You can work around it by setting the price yourself, something like\r\n\r\n```terraform\r\ndata \"aws_pricing_product\" \"on_demand_job_instance\" {\r\n  provider     = aws.ap-south-1\r\n  service_code = \"AmazonEC2\"\r\n\r\n  filters {\r\n    field = \"location\"\r\n    value = data.aws_region.current.description\r\n  }\r\n\r\n  filters {\r\n    field = \"operatingSystem\"\r\n    value = \"Linux\"\r\n  }\r\n\r\n  filters {\r\n    field = \"instanceType\"\r\n    value = var.gitlab_task_instance_type\r\n  }\r\n\r\n  filters {\r\n    field = \"capacitystatus\"\r\n    value = \"Used\"\r\n  }\r\n\r\n  filters {\r\n    field = \"tenancy\"\r\n    value = \"Shared\"\r\n  }\r\n\r\n  filters {\r\n    field = \"preInstalledSw\"\r\n    value = \"NA\"\r\n  }\r\n\r\n  filters {\r\n    field = \"licenseModel\"\r\n    value = \"No License required\"\r\n  }\r\n}\r\n```\r\n\r\nthen get the value with `tonumber(values(values(jsondecode(data.aws_pricing_product.on_demand_job_instance.result).terms.OnDemand)[0].priceDimensions)[0].pricePerUnit.USD)`; of course that's annoying and you'll have to be updating this if price changes but it's better than some default 0.50",
        "I think there is no need to do anything, right? You specify the maximum price you are willing to pay. If the current spot price is less, you get charged the lower price. If the current spot price is higher, you will not get a machine.",
        ">I think there is no need to do anything, right? You specify the maximum price you are willing to pay. If the current spot price is less, you get charged the lower price. If the current spot price is higher, you will not get a machine.\r\n\r\nThe runner forces you to provide a value otherwise it defaults to $0.50. This default isn't very useful, it's just a random number that doesn't reflect instance type(s) you've chosen: if you're using large instance type, you're never going to get it for $0.50.\r\n\r\nSo you're forced to pick a number at the moment and the most common scenario is to accept up to the on-demand price: as you say, the spot price is usually much less and the user ends up paying the lower price.\r\n\r\nWhat's annoying to do is to look up this price for every instance type you need (something my workaround above does) and keep track of the price changing over time.\r\n\r\nBasically, in order to accept any price up to on-demand which is what most people want, we have to specify no value and let AWS deal with it \u2013 and the runner doesn't let you do this, forcing you to pick a number up front and keep it updated. Most people don't have a hard-limit of say $2, they are willing to pay whatever it costs to get a machine but if they can pay less with spot, that's what they want.",
        "Got it. It was always a one time effort to determine the on-demand price as we never changed the instance type.\n\nYeah, would be nice to have the \"correct\" default value.",
        "Ok, I tried this approach. Filtering the location with `aws_region.name` is not working for Europe as \"EU Frankfurt\" is expected and not \"Europe (Frankfurt)\". Any idea?",
        "> Ok, I tried this approach. Filtering the location with `aws_region.name` is not working for Europe as \"EU Frankfurt\" is expected and not \"Europe (Frankfurt)\". Any idea?\r\n\r\nI'm looking at #391 and maybe you misunderstood what I was trying to convey above.\r\n\r\nThe whole lookup of current on-demand price stuff is just a workaround. What we actually want is to _not_ set any value at all for the pricing which means exactly what we want: use up to on-demand price. AWS handles this. See this comment again: https://github.com/npalm/terraform-aws-gitlab-runner/issues/222#issuecomment-641138387\r\n\r\nThe AWS API allows to leave the field empty and that's what we should be doing in this case: _not_ looking up the price at current time ourselves.",
        " @user Damn, of course. This is nothing more than quick hack. Didn't see that we can leave the price empty. Let me try this.",
        "Thanks for putting me back on track. Things can be so easy... Just finished the work.",
        ":tada: This issue has been resolved in version 4.35.0 :tada:\n\nThe release is available on [GitHub release](https://github.com/npalm/terraform-aws-gitlab-runner/releases/tag/4.35.0)\n\nYour **[semantic-release](https://github.com/semantic-release/semantic-release)** bot :package::rocket:"
      ]
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/npalm/terraform-aws-gitlab-runner/issues/270",
    "content": {
      "title": "Support for Docker Hub registry mirror for rate-limited pulls",
      "body": "While running some automation jobs in bulk on my Gitlab projects, I got hit today by the infamous new [Docker Hub Rate Limiting](https://www.docker.com/increase-rate-limits)...\r\n\r\n```\r\nWaiting for services to be up and running...\r\nPulling docker image docker:latest ...\r\nERROR: Preparation failed: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit (docker.go:142:0s)\r\n``` \r\n\r\nThere are some solutions to this problem:\r\n* **Authenticate to docker hub**. Anonymously you get 100 container image requests per 6 hours, with an authenticated account you get 200 container image requests per 6 hours. \r\n* **Pay for Docker Hub**. There's a [Pro account](https://www.docker.com/pricing) already for only $5/month. Requires you to authenticate. For a company, this is the easiest solution and makes the most sense to pay for such a critical service. In my case, it's a personal project for fun so worth looking for other (cheaper) ways. \r\n* **Cache on GitLab container registry**: tag and push the docker hub image you use (e.g. `php:8`) to your Gitlab repository container registry to \"cache\", and simply pull from that one to decrease pulls from docker hub. Bit tedious... \r\n* Use the free [**Gitlab Dependency Proxy**](https://docs.gitlab.com/ee/user/packages/dependency_proxy/) \r\n  * Requires you to add a `docker login` statement and to prefix every docker image with `$CI_DEPENDENCY_PROXY_GROUP_IMAGE_PREFIX`. Currently, I tried this but get a 500 error back on `docker pull`...\r\n* **Self-Hosted Docker Registry As Pull-Through Cache**:  [Docker Hub Registry Mirror](https://docs.gitlab.com/runner/configuration/speed_up_job_execution.html#docker-hub-registry-mirror) and let the runner use this as a registry as a pull-through cache. A similar thing can be done with JFrog Artifactory/Container Registry. Because distribution uses HEAD manifest requests to check if cached content has changed, it will not count towards the rate limit. Note that initial requests for Hub images that are not yet in cache will be counted.\r\n* Google and AWS have both launched a solution: https://cloud.google.com/container-registry/docs/pulling-cached-images and https://aws.amazon.com/blogs/containers/advice-for-customers-dealing-with-docker-hub-rate-limits-and-a-coming-soon-announcement/ and https://aws.amazon.com/blogs/aws/amazon-ecr-public-a-new-public-container-registry/\r\n\r\n\r\nDoes anyone have experience with any of these solutions? Did anyone try the self-hosted Docker registry mirror? Would it make sense to spin it up in this repo, or provide config options in this repo to add it? ",
      "comments": [
        "Hey, \r\nit could be great to setup a Docker registry proxy and configure the runner to point on it. \r\n**But sure +1 for this killer feature.**\r\n\r\nSo far, I have my own workaround, I have a repository of all my base docker images (`my-org/docker-env`) and I use this simple `.gitlab-ci.yaml` script to build the images and publish them to the Gitlab registry.\r\n\r\n```yaml\r\nstages:\r\n    - build\r\n\r\n# Define variables\r\nvariables:\r\n    CONTAINER_IMAGE: $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_NAME\r\n    DOCKER_HOST: tcp://docker:2375\r\n    DOCKER_DRIVER: overlay2\r\n\r\n# Build docker image\r\nbuild-image:\r\n    stage: build\r\n    image: docker:1.12-dind\r\n    services:\r\n        - docker:dind\r\n    script:\r\n        - cat /etc/resolv.conf\r\n        - cat /etc/hosts\r\n        - echo $CI_REGISTRY\r\n        - docker info\r\n        - docker login -u gitlab-ci-token -p $CI_BUILD_TOKEN $CI_REGISTRY\r\n        - docker build -t $CONTAINER_IMAGE .\r\n        - docker push $CONTAINER_IMAGE\r\n \r\n```\r\n\r\nI build one image per branch:\r\n\r\n![image](https://user-images.githubusercontent.com/4228646/103525158-12e8ec80-4e7f-11eb-8252-113756d0720a.png)\r\n\r\nAnd publish the branch name as a tag for the image repository registry.\r\n\r\nFinally use my base images in my other projects in `.gitlab-ci.yml` or `Dockerfile`. \r\n\r\nFor example:\r\n\r\n`registry.gitlab.com/my-org/docker-env:node14-nx`\r\n\r\n\r\nSource: https://about.gitlab.com/blog/2020/10/30/mitigating-the-impact-of-docker-hub-pull-requests-limits ",
        "We use a proxy like artifactory, post of the builds points directly to the mirror. But on the runners we configure on system level the mirror for docker via `--registry-mirror`. Not ideal but it works all quite well",
        "Ran into this issue today, looking to setup pointing at the ECR. I was hoping it'd be as easy as passing IAM permissions to either the runner, or the machine, but I haven't had any luck with that, so I'm investigating installing ecr credential helper.\r\n\r\nHow are you pointing directly at the mirror?\r\n\r\nI'd even be willing to setup authentication, but I'm at a loss at the easiest way to do this.",
        " @user Could you please share your solution? Where to put the \"--registry-mirror\" option?",
        " @user I ran into the same issue.\r\nI'm using setup which is similar to [examples/runner-default](https://github.com/npalm/terraform-aws-gitlab-runner/tree/develop/examples/runner-default)\r\n\r\nmy solution is to add following variable to `main.tf`\r\n```\r\ndocker_machine_options = [\"engine-registry-mirror=https://mirror.gcr.io\"]\r\n```\r\n\r\nThis uses Google Cloud Platform mirror.\r\nAs a result, module will add parameter to _MachineOptions_ block in `config.toml`\r\nThis configuration is described in GitLab Runner Documentation [Autoscale config section](https://docs.gitlab.com/runner/configuration/autoscale.html#distributed-container-registry-mirroring).\r\n",
        " @user Yeah, that's the configuration I am using too.",
        "Launching a Docker registry mirror is out of scope. But making the configuration explicit would be nice. See #400 ",
        ":tada: This issue has been resolved in version 4.36.0 :tada:\n\nThe release is available on [GitHub release](https://github.com/npalm/terraform-aws-gitlab-runner/releases/tag/4.36.0)\n\nYour **[semantic-release](https://github.com/semantic-release/semantic-release)** bot :package::rocket:"
      ]
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/concourse/hush-house/issues/26",
    "content": {
      "title": "metrics: set up alerting",
      "body": "Hey,\r\n\r\nDespite the fact that we already have metrics being collected by [Prometheus](https://prometheus.io/), and dashboards being displayed from Grafana, we're still in the need of having alerting set up in order to not need to constantly look at dashboards to know when things go wrong.\r\n\r\nFrom my understanding, we have at least two choices here:\r\n\r\n- go with [`grafana` alerts](https://grafana.com/docs/alerting/rules/) (like we do for [Wings](https://wings.pivotal.io)), or\r\n- go with [`alertmanager`](https://prometheus.io/docs/alerting/alertmanager/), which would be notified by Prometheus when a particular query goes beyond a certain level.\r\n\r\nWhile the second case seems interesting to me from the standpoint that we can decouple the visualization from the alerting, the second case allows us to visualize how our thresholds look like, as well as consume multiple data sources (e.g., not only Prometheus but [Stackdriver](https://cloud.google.com/stackdriver/docs/) as well).\r\n\r\n**Acceptance criteria**:\r\n\r\n- Can we use Prometheus for all alerting needs? \r\n- have the full alerting system working: having the observed metrics going up, we really get paged.\r\n\r\nThanks!\r\n",
      "comments": [
        "We'll probably want alerts for Datadog SLIs/SLOs",
        "Chore: Bump Grafana to 6.0",
        "note: the domain doesn't redirect http requests to https",
        "It seems possible to use Prometheus with stackdriver, though the integration has some limitations and is in beta ([see here](https://cloud.google.com/monitoring/kubernetes-engine/prometheus)). Based on one [medium blog post](https://medium.com/google-cloud/prometheus-and-stackdriver-fb8f7524ece0), seems possible though!\r\n\r\nOur current plan is to setup alerts with Grafana and create another long-term issue to implement alerts with Prometheus. We don't want to tackle prometheus now because we have a lot of learning to do about it. We'd rather get hush-house production-ready sooner rather than later, which means using Grafana for alerts in the short-term.\r\n\r\nThoughts?\r\ncc @user @cirocosta @user ",
        "sounds goood \ud83d\ude01  thankss!",
        " @user - I agree with this. We have other potential avenues to dive into Prometheus and hush-house doesn't need to be held back because of it. ",
        "### EOD\r\n\r\n- Added two alerts using Grafana. One for high disk usage and a second for high number of containers\r\n- We setup PagerDuty integration with hush-house's Grafana and for the previous two alerts\r\n\r\n**Next Steps:**\r\n\r\n- [x] Think about what kind of alerts we want for hush-house and add them using Grafana.\r\n- ~~Figure out why Grafana's HTTP is not redirecting to HTTPS and make it redirect.~~",
        "Let's make a new issue around diving into what suite of metrics we can provide on hush house outside of what we currently offer on Wings. ",
        "Hey,\r\n\r @user  and I were looking at Grafana performing the redirect, and it turns out that Grafana itself doesn't do that (differently from Concourse, which does redirect automatically), meaning that we'd need something else to perform the redirects (like `nginx`).\r\n\r\nWe'll leave this to another issue in order to not be blocked on this (something that the other metrics dashboard doesn't provide anyway).\r\n\r\nThanks!"
      ]
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/poseidon/typhoon/issues/128",
    "content": {
      "title": "ASG's for Controllers",
      "body": "Is there some reason that the AWS controllers can't be in an auto scaling group?  \r\n\r\nI ask because I'd like to convert the Classic Load Balancers to NLB's to allow for the passthru of websockets at the very least, and the only way that I can see to associate the instances to the targetgroup is via an ASG.\r\n\r\nIt seems sensible that the controllers should be able to heal themselves if they get blown away, I'm just not sure if any problems might manifest themselves because of this.\r\n\r\nIf I get it working I'll feed back a PR.\r\n\r\nCheers,\r\nPaul.",
      "comments": [
        "If you review the history, you'll find there is a reason #35. The use of controller instances is intentional.\r\n\r\nThe etcd cluster backing a Kubernetes cluster is stateful. Full stop. No getting around it. Typhoon clusters run etcd peers across \"controllers\". Controllers are considered stateful because of the etcd data they contain, its not appropriate to use an ASG as a result, and the number of controllers should be selected upfront. If an etcd quorum loss occurs, instances are left intact so you can follow the official etcd [recovery](https://coreos.com/etcd/docs/latest/op-guide/recovery.html) procedures.\r\n\r\nAt the foundational level, etcd doesn't endorse running etcd in ASGs. In naive implementations, replacement means data loss. A number of clever projects/attempts have been made over the years and some non-CoreOS projects do it anyway with varying degrees of safety. But officially, recovery is something best left to an admin.\r\n\r\nThere were alternative Kubernetes designs, such as running dedicated etcd instances to allow Kubernetes controllers to be in ASGs. This just moves the problem. The etcd instances become the stateful ones, can't be in an ASG, and their number must be selected upfront. Except now the architecture is more complex, you pay for VMs that just run the etcd process, and recovery is still the same. Plus if you need to scale the control plane you usually also need to scale the etcd data plan so it doesn't really solve the scale problem. If you're interested in that design, you can look at Tectonic where we chose it for enterprisey reasons.\r\n\r\nOverall, I understand the instinct is to put everything in auto-scaling groups (AWS) or managed instance groups (GCP) and hope it auto-heals. But here I think its harmful.",
        "Switching from ELBs to NLBs is a separate desire and has merit. I agree that part is a good goal.",
        "Opening #130 ",
        "OK.  I'll investigate how to work this around for the controllers.  The workers a pretty easy."
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/poseidon/typhoon/issues/90",
    "content": {
      "title": "why bootkube?",
      "body": "From readme, it says in-place upgrades is non-goal. Then why bootkube or self hosted Kubernetes is needed? Why not just use systemd+containers to avoid complexity, and achieve the minimalism goal more?\r\n\r\nKubernetes control plane is designed to allow maintenance windows. With HA setup, the maintenance window can even be minimized. The worker plane is tricker, but it has nothing to do with bootkube anyway. I do not think in-place upgrades will bring anything significant anyway to typhoon's goal. So I wont worry about the future plan to put bootkube in here now.\r\n",
      "comments": [
        "Another value of self hosted is to be able to re-config the control plane easily in the post-installation phase through k8s tooling (kubectl).\r\n\r\nBut I found very little of value of it in practice. After I settle down with a set of k8s configurations I am happy about in production, I seldom want to change it in any signifiant way. Every time I want to change configuration is due to a upgrade. Then I would just regenerate all configurations of control plane, and re-deploy anyway.\r\n\r\nI would like to hear other advantages of self hosted or bootkube you found in practice. Thanks.",
        "In the last 2+ year years maintaining Matchbox reference clusters (of the static (i.e. systemd+containers), self-hosted, and rktnetes varieties) and until recently Tectonic, I have a few comments (static and rktnetes variants were killed off over time).\r\n\r\nTyphoon uses bootkube for reasons beyond just self-hosted. First, bootkube is maintained as a kubernetes-incubator project. That means it gets contributions from various folks with different focuses, its considered in security disclosures and discussions upstream, and it establishes common ground with similar projects. Second, bootkube runs automated conformance tests and CNCF certification tests so its easy to spot breakages (Typhoon doesn't pay $$$ for that infra). Finally, its mutually beneficial for Typhoon to leverage the community's work on bootkube as a low level piece and likewise it has contributed to bootkube's direction.\r\n\r\nI'll break down your other points:\r\n\r\n* Yes, holistic in-place upgrades are a non-goal (speaking in terms of the whole infrastructure, architecture, cluster design, components, etc.). That doesn't preclude being able to update manifests. Self-hosted only concerns manifest updates (a piece of the puzzle). Its true you can sometimes update those pieces in-place, but Typhoon makes no guarantees. In normal operations, reprovisioning is advised.\r\n* At a few key points in the last few years, it has been handy to reconfigure components with `kubectl` instead of the usual reprovisioning process. The kube-dns vulnerabilities came to mind. If nothing else, it gives end users the ability to blast out certain critical changes immediately if a proper reprovision would take too long.\r\n* Typhoon uses bootkube, but doesn't strictly care that the cluster is self-hosted. There are a few failure scenarios you can encounter in self-hosted that cannot occur in static clusters and Typhoon may in future move in a more static direction. However, its way more mature than it was when I was running the first versions - those failure scenarios are adequately dealt with and there isn't an immediate reason to shift the architecture.\r\n\r\nOverall, I'm not making an argument for self-hosted here. Typhoon uses bootkube mainly to leverage its maturity, community involvement, testing infrastructure, and mind-share. To the end user, the fact that a few control plane components are run as self-hosted pods is just an internal implementation detail. To the end user, they `terraform apply` and get a cluster, that's it.\r\n\r\nNote: In future, I'd prefer we reserve issues for concrete actionable tasks or code issues and use the provided IRC for discussions like this.\r\n",
        ">  In future, I'd prefer we reserve issues for concrete actionable tasks or code issues and use the provided IRC for discussions like this.\r\n\r\nquick reply: probably there should be a mailing list for discussion like this one. irc or other instant messaging is not good for searching, sharing or long term discussion I think.\r\n\r\nI will reply on other points in the next couple of days.",
        "I think the answer above describes the current reasons for using bootkube, which was the original question.\r\n\r\nI'm closing for now, but lemme know if there is something I missed."
      ]
    },
    "codes": [
      "cluster",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/poseidon/typhoon/issues/118",
    "content": {
      "title": "Integrate digitalocean-cloud-controller-manager for exposing Services with an external IP. ",
      "body": "\r\n\r\nAt the moment, kubernetes Services can't really expose an IP outside the cluster because DigitalOcean is not a supported cloud provider.\r\n\r\nIt would be lovely to be able to integrate this:https://github.com/digitalocean/digitalocean-cloud-controller-manager into typhoon so we could use their LoadBalancer and expose services to an external IP.\r\n",
      "comments": [
        "So, I've been having problems with the firewall that is automatically created with the cluster. Using the digitalocean-cloud-controller-manager fails to connect the nodes to the loadbalancer when this firewall is enabled on the nodes.\r\n\r\nTherefor it is not possible to create a service outside digitalocean with this firewall. I've tried countless of times using the digitalocean-cloud-controller-manager and it wont' work unless I take out the firewall.",
        "At the moment, Digital Ocean's offering provides features you can get in cleaner ways already. If your root aim is to expose services, you can use the Nginx Ingress controller addon for TCP, UDP, and HTTP/HTTPS services. https://github.com/poseidon/typhoon/blob/master/docs/addons/ingress.md#digital-ocean. This avoids the need for a component to hold cloud-provider credentials and avoids the need to allocate a load balancer per service (avoid type LoadBalancer).\r\n\r\nIn the future, if the controller manager adds support for block storage attachment / detachment (or another compelling feature), we'll evaluate it and consider including it in an opt-in, addon way.",
        "Its worth noting that the Digital Ocean platform is the only Typhoon platform that doesn't setup an Ingress Load Balancer on behalf of users (hence the Daemonset and CNAME approach used in docs). Originally this was done to keep DO dirt cheap since its popular among developers. We may need to re-evaluate that. ",
        " @user Thanks for your reply but then how exactly would that allow me to deploy multiple services that use the port 80 for my workers FQDN?\r\n\r\nIf I understand your reply correctly, you want me to create CNAME for app1.nemo.mysite.com that will link to nemo-workers.mysite.com ? If all of my workers have the nemo-workers.mysite.com as their FQDN, then how would they be able to accommodate for more than 1 service?\r\n\r\nI understood that with a load balancer you get a EXTERNAL_IP that can be used per service, how would this work with your proposed solution if the EXTERNAL_IP is technically the same for all of the workers.\r\n\r\nWith a loadbalancer I noticed that it redirected the traffic to the exact port the service was located at. So it was something like port 31480. It worked without the firewall, however it was a $20 dollar a month or 3 cents an hour per service, which adds up for every service I want exposed.\r\n",
        "Ingress supports SNI, so multiple sites should not be an issue. Create an Ingress resource for each application and fill out the `host` section, as usual. https://kubernetes.io/docs/concepts/services-networking/ingress/#the-ingress-resource\r\n\r\nOn the infrastructure side, your sole responsibility is to ensure packets destined for app1, app2, etc. reach an Ingress controller pod. On Digital Ocean, the super cheap way is to deploy the Nginx Ingress controller as a DaemonSet across workers as [described](https://github.com/poseidon/typhoon/blob/master/docs/addons/ingress.md#digital-ocean) and make use of the DNS record `cluster-workers.domain.com` which resolves to a worker (and thus an Ingress controller). Then, \"for each application, add a CNAME record resolving to the worker(s) DNS record\".\r\n\r\n```\r\napp1.domain.com -> CNAME cluster-workers.domain.com\r\napp2.domain.com -> CNAME cluster-workers.domain.com\r\napp3.somedomain.com -> CNAME cluster-workers.domain.com\r\n```\r\n\r\nOn other platforms (AWS, Google), a load balancer is created which has a single IP and you'd do something similar, creating an A record for each app, that resolves to the shared load balancer IP. In future we could add a load balancer in the DO cluster to be a little nicer than round-robin DNS, but I'm not sure the gain is worth spending $20/month extra given the DO clusters tend to be used for dev.\r\n\r\nEDIT: I've double checked, there is no trouble running multiple HTTPS sites on DO clusters. ~You shouldn't have issues with multiple HTTP sites either.~\r\nCorrection: SNI is part of TLS/SSL so you may have difficulty multiplexing multiple HTTP sites. I don't really run any non-HTTPS sites to have tested how Nginx Ingress behaves in that case. In any case, the Ingress addon is just a supported example, if its not right for you you're welcome to take another approach.",
        "Was this helpful or have you chosen to take another route to setup Ingress?",
        "Closing this out. Digital Ocean with the Ingress addon is working as intended. That's the recommended way to expose services.",
        "Hi, I don't know if this the right place to ask but I'm having trouble setting up my DNS using the nginx ingress.\r\n\r\n`*.domain.com -> CNAME cluster-workers.domain.com` is working just fine, but I'm having difficulties settings up the DNS for the root domain. `domain.com` gets `Could not resolve host` on curl",
        "I solved it by adding `@ -> A xxx.xxx.xxx.xxx`. Where the IP is set same with the one on `cluster-workers.domain.com`. Thanks anyways"
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/poseidon/typhoon/issues/94",
    "content": {
      "title": "VMware vSphere module?",
      "body": "Hello,\r\n\r\nI am about to start to write a VMware vSphere module for Typhoon.\r\n\r\nI just would like to know if you planned to support this platform, if you have already started this module, and if have any suggestions or recommendations?\r\n\r\nThank you.",
      "comments": [
        "I'm not planning for Typhoon to add or support a vSphere module in the near future.\r\n\r\nWriting a module isn't the issue, its the maintenance, validation, testing, and support that goes into each platform, for Typhoon releases, for the foreseeable future. Typhoon clusters are run continuously on each platform, with workloads (simmer tests, not just \"does it launch\").  Platforms get independently tested and certified for conformance. Addons are verified to play nicely and provide a good admin experience. My bar is that I personally use and can recommend each aspect of Typhoon.\r\n\r\nWith VMWare, I'm not sure those goals can be achieved right now. VMWare vSphere is high-cost, which would make it difficult to offer the same quality guarantees and maintain it effectively. It requires ecosystem knowledge that should be maintained with involvement from VMWare. And its barrier to entry and lock-in mean a smaller audience is available to help in its upkeep (i.e. I don't know individuals running vSphere of their own choosing, its only within organizations). Those are just the tradeoffs, vSphere clearly has pros for many folks.\r\n\r\nIn my opinion, products are best maintained by those who both understand them deeply and love using them daily - Typhoon trying to offer vSphere right now, I think it'd miss the mark and the expectations of VMWare fans (Typhoon is non-commercial, not targeting every case, so I can be frank). VMWare seems to have its own Kubernetes offering (no idea if its good/bad), so it gets some love from VMWare engineers (or at least contractually obligated attention). Or you can build your own module and run it for a while, I do encourage it.\r\n\r\nFor now, you can try treating VMWare like bare-metal machines and use the bare-metal platform. Some folks have reported success - though I'll caution that's not how Typhoon gets validated.",
        "Thanks a lot for your fast reply. This totally make sense.\r\nI close the thread, and gonna use your suggestion.",
        " @user Have you made any progress on getting Typhoon to work on vmware? I am also interested in working on this. I have a 3-node vsphere cluster that I'd like to try boot-kube on, but typhoon would be better, keeping in mind that it would not be supported, etc..",
        "Hello. Yes, it's working now.\r\n\r\nI wrote a Terraform module (called Cyclone ;)) which is using Typhoon as module as it is (I wanted to avoid any fork to track the next releases easier), to deploy the K8s cluster based on \"machines\". These machines can be VM and/or PM, and are respectively provided by VMware vSphere provider and the an IPMI module. In the point of view of Typhoon, it's just machine which are booting on PXE.\r\n\r\nJust as a preview, the `vsphere.tf`:\r\n```\r\ndata \"vsphere_datacenter\" \"dc\" {\r\n  name = \"${var.vsphere_datacenter}\"\r\n}\r\n\r\ndata \"vsphere_datastore\" \"datastore\" {\r\n  name          = \"${var.vsphere_datastore}\"\r\n  datacenter_id = \"${data.vsphere_datacenter.dc.id}\"\r\n}\r\n\r\ndata \"vsphere_resource_pool\" \"pool\" {\r\n  name          = \"${var.vsphere_resource_pool}\"\r\n  datacenter_id = \"${data.vsphere_datacenter.dc.id}\"\r\n}\r\n\r\ndata \"vsphere_network\" \"network\" {\r\n  name          = \"${var.vsphere_network}\"\r\n  datacenter_id = \"${data.vsphere_datacenter.dc.id}\"\r\n}\r\n\r\nresource \"vsphere_virtual_machine\" \"vm_controllers\" {\r\n  count                      = \"${length(data.null_data_source.vm_controllers.*.inputs)}\"\r\n  name                       = \"${lookup(data.null_data_source.vm_controllers.*.inputs[count.index], \"name\")}\"\r\n  resource_pool_id           = \"${data.vsphere_resource_pool.pool.id}\"\r\n  datastore_id               = \"${data.vsphere_datastore.datastore.id}\"\r\n  folder                     = \"${var.vsphere_folder}\"\r\n  guest_id                   = \"${var.vsphere_guest_id}\"\r\n  num_cpus                   = \"${var.vsphere_controllers_num_cpus}\"\r\n  memory                     = \"${var.vsphere_controllers_memory}\"\r\n  wait_for_guest_net_timeout = 0\r\n\r\n  network_interface {\r\n    network_id = \"${data.vsphere_network.network.id}\"\r\n  }\r\n\r\n  disk {\r\n    label = \"${var.vsphere_disk_label}\"\r\n    size  = \"${var.vsphere_controllers_disk_size}\"\r\n  }\r\n}\r\n\r\nresource \"vsphere_virtual_machine\" \"vm_workers\" {\r\n  count                      = \"${length(data.null_data_source.vm_workers.*.inputs)}\"\r\n  name                       = \"${lookup(data.null_data_source.vm_workers.*.inputs[count.index], \"name\")}\"\r\n  resource_pool_id           = \"${data.vsphere_resource_pool.pool.id}\"\r\n  datastore_id               = \"${data.vsphere_datastore.datastore.id}\"\r\n  folder                     = \"${var.vsphere_folder}\"\r\n  guest_id                   = \"${var.vsphere_guest_id}\"\r\n  num_cpus                   = \"${var.vsphere_workers_num_cpus}\"\r\n  memory                     = \"${var.vsphere_workers_memory}\"\r\n  wait_for_guest_net_timeout = 0\r\n\r\n  network_interface {\r\n    network_id = \"${data.vsphere_network.network.id}\"\r\n  }\r\n\r\n  disk {\r\n    label = \"${var.vsphere_disk_label}\"\r\n    size  = \"${var.vsphere_workers_disk_size}\"\r\n  }\r\n}\r\n\r\nresource \"null_resource\" \"vm_tools\" {\r\n  count = \"${length(concat(data.null_data_source.vm_controllers.*.inputs,\r\n                           data.null_data_source.vm_workers.*.inputs))}\"\r\n\r\n  depends_on = [\"null_resource.wait-online\"]\r\n\r\n  connection {\r\n    type    = \"ssh\"\r\n    user    = \"core\"\r\n    timeout = \"10m\"\r\n    host = \"${element(concat(data.null_data_source.vm_controllers.*.inputs.domain,\r\n                              data.null_data_source.vm_workers.*.inputs.domain), count.index)}\"\r\n  }\r\n\r\n  provisioner \"remote-exec\" {\r\n    inline = [\r\n      \"docker run -d --restart=always --net=host -v /run/systemd:/run/systemd --name open-vm-tools godmodelabs/open-vm-tools 1>&2 2>/dev/null\",\r\n    ]\r\n  }\r\n}\r\n```\r\nAnd the `typhoon.tf`:\r\n```\r\nmodule \"typhoon\" {\r\n  source = \"git::https://github.com/poseidon/typhoon//bare-metal/container-linux/kubernetes?ref=v1.9.2\"\r\n\r\n  container_linux_channel = \"${var.container_linux_channel}\"\r\n  container_linux_version = \"${var.container_linux_version}\"\r\n  matchbox_http_endpoint  = \"${var.matchbox_http_endpoint}\"\r\n  ssh_authorized_key      = \"${var.ssh_authorized_key}\"\r\n  cluster_name            = \"${var.cluster_name}\"\r\n  k8s_domain_name         = \"${var.cluster_name}.${var.cluster_domain_name}\"\r\n  asset_dir               = \"${local.asset_dir}\"\r\n  pod_cidr                = \"${var.pod_cidr}\"\r\n  service_cidr            = \"${var.service_cidr}\"\r\n  controller_names        = \"${concat(data.null_data_source.pm_controllers.*.inputs.name, data.null_data_source.vm_controllers.*.inputs.name)}\"\r\n  controller_macs         = \"${concat(data.null_data_source.pm_controllers.*.inputs.mac_address, vsphere_virtual_machine.vm_controllers.*.network_interface.0.mac_address)}\"\r\n  controller_domains      = \"${concat(data.null_data_source.pm_controllers.*.inputs.domain, data.null_data_source.vm_controllers.*.inputs.domain)}\"\r\n  controller_networkds    = \"${concat(data.null_data_source.pm_controllers.*.inputs.profile_render, data.null_data_source.vm_controllers.*.inputs.profile_render)}\"\r\n  worker_names            = \"${concat(data.null_data_source.pm_workers.*.inputs.name, data.null_data_source.vm_workers.*.inputs.name)}\"\r\n  worker_macs             = \"${concat(data.null_data_source.pm_workers.*.inputs.mac_address, vsphere_virtual_machine.vm_workers.*.network_interface.0.mac_address)}\"\r\n  worker_domains          = \"${concat(data.null_data_source.pm_workers.*.inputs.domain, data.null_data_source.vm_workers.*.inputs.domain)}\"\r\n  worker_networkds        = \"${concat(data.null_data_source.pm_workers.*.inputs.profile_render,data.null_data_source.vm_workers.*.inputs.profile_render)}\"\r\n}\r\n```\r\n\r\n\r\nLet me clean my repo, and I will push my module code on Github.\r\nI'll let you know here.",
        "just mentioning that my vsphere fork is still WIP here: https://github.com/bkcsfi/typhoon/tree/feature-vsphere\r\n\r\nStill need to a) update docs, b) switch to full dns name based setup, c) support vsphere volumes (requires upstream support for --cloud-config option for kubelet)\r\n\r\n"
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/poseidon/typhoon/issues/232",
    "content": {
      "title": "Typhoon does not start bootkube ",
      "body": "\r\n\r\n## Bug\r\nmodule.aws-tempest.module.workers.aws_lb_listener.ingress-https: Creating...\r\n  arn:                               \"\" => \"<computed>\"\r\n  default_action.#:                  \"\" => \"1\"\r\n  default_action.0.target_group_arn: \"\" => \"arn:aws:elasticloadbalancing:us-west-2:121741712844:targetgroup/tempest-workers-https/f3d1961f202e7f56\"\r\n  default_action.0.type:             \"\" => \"forward\"\r\n  load_balancer_arn:                 \"\" => \"arn:aws:elasticloadbalancing:us-west-2:121741712844:loadbalancer/net/tempest-ingress/ec3b402b109abd48\"\r\n  port:                              \"\" => \"443\"\r\n  protocol:                          \"\" => \"TCP\"\r\n  ssl_policy:                        \"\" => \"<computed>\"\r\nmodule.aws-tempest.module.workers.aws_lb_listener.ingress-https: Creation complete after 1s (ID: arn:aws:elasticloadbalancing:us-west-2:...ress/ec3b402b109abd48/ef245d91e5b4973e)\r\nmodule.aws-tempest.module.workers.aws_lb_listener.ingress-http: Creation complete after 1s (ID: arn:aws:elasticloadbalancing:us-west-2:...ress/ec3b402b109abd48/6f0ae73b4c501b94)\r\nmodule.aws-tempest.aws_route53_record.apiserver: Still creating... (20s elapsed)\r\nmodule.aws-tempest.aws_route53_record.apiserver: Still creating... (30s elapsed)\r\nmodule.aws-tempest.aws_route53_record.apiserver: Still creating... (40s elapsed)\r\nmodule.aws-tempest.aws_route53_record.apiserver: Still creating... (50s elapsed)\r\nmodule.aws-tempest.aws_route53_record.apiserver: Still creating... (1m0s elapsed)\r\nmodule.aws-tempest.aws_route53_record.apiserver: Still creating... (1m10s elapsed)\r\nmodule.aws-tempest.aws_route53_record.apiserver: Still creating... (1m20s elapsed)\r\nmodule.aws-tempest.aws_route53_record.apiserver: Creation complete after 1m28s (ID: Z202GOHHUL8IHS_tempest.was.example.com._A)\r\nmodule.aws-tempest.null_resource.bootkube-start: Creating...\r\nmodule.aws-tempest.null_resource.bootkube-start: Provisioning with 'file'...\r\nmodule.aws-tempest.null_resource.bootkube-start: Provisioning with 'remote-exec'...\r\nmodule.aws-tempest.null_resource.bootkube-start (remote-exec): Connecting to remote host via SSH...\r\nmodule.aws-tempest.null_resource.bootkube-start (remote-exec):   Host: 54.245.19.133\r\nmodule.aws-tempest.null_resource.bootkube-start (remote-exec):   User: core\r\nmodule.aws-tempest.null_resource.bootkube-start (remote-exec):   Password: false\r\nmodule.aws-tempest.null_resource.bootkube-start (remote-exec):   Private key: false\r\nmodule.aws-tempest.null_resource.bootkube-start (remote-exec):   SSH Agent: true\r\nmodule.aws-tempest.null_resource.bootkube-start (remote-exec):   Checking Host Key: false\r\nmodule.aws-tempest.null_resource.bootkube-start (remote-exec): Connected!\r\nmodule.aws-tempest.null_resource.bootkube-start: Still creating... (10s elapsed)\r\nmodule.aws-tempest.null_resource.bootkube-start: Still creating... (20s elapsed)\r\nmodule.aws-tempest.null_resource.bootkube-start: Still creating... (30s elapsed)\r\nmodule.aws-tempest.null_resource.bootkube-start: Still creating... (40s elapsed)\r\nmodule.aws-tempest.null_resource.bootkube-start: Still creating... (50s elapsed)\r\nmodule.aws-tempest.null_resource.bootkube-start: Still creating... (1m0s elapsed)\r\n\r\n...\r\n\r\nmodule.aws-tempest.null_resource.bootkube-start: Still creating... (18m50s elapsed)\r\nmodule.aws-tempest.null_resource.bootkube-start: Still creating... (19m0s elapsed)\r\nmodule.aws-tempest.null_resource.bootkube-start: Still creating... (19m10s elapsed)\r\nmodule.aws-tempest.null_resource.bootkube-start: Still creating... (19m20s elapsed)\r\nmodule.aws-tempest.null_resource.bootkube-start: Still creating... (19m30s elapsed)\r\nmodule.aws-tempest.null_resource.bootkube-start: Still creating... (19m40s elapsed)\r\nmodule.aws-tempest.null_resource.bootkube-start: Still creating... (19m50s elapsed)\r\nmodule.aws-tempest.null_resource.bootkube-start: Still creating... (20m0s elapsed)\r\nmodule.aws-tempest.null_resource.bootkube-start: Still creating... (20m10s elapsed)\r\nmodule.aws-tempest.null_resource.bootkube-start: Still creating... (20m20s elapsed)\r\nmodule.aws-tempest.null_resource.bootkube-start (remote-exec): Job for bootkube.service failed because the control process exited with error code.\r\nmodule.aws-tempest.null_resource.bootkube-start (remote-exec): See \"systemctl status bootkube.service\" and \"journalctl -xe\" for details.\r\n\r\nError: Error applying plan:\r\n\r\n1 error(s) occurred:\r\n\r\n* module.aws-tempest.null_resource.bootkube-start: error executing \"/tmp/terraform_1907365135.sh\": Process exited with status 1\r\n\r\nTerraform does not automatically rollback in the face of errors.\r\nInstead, your Terraform state file has been partially updated with\r\nany resources that successfully completed. Please address the error\r\nabove and apply again to incrementally change your infrastructure.\r\n### Environment\r\n\r\n* Platform: aws\r\n* OS: CoreOS, AWS Linux \r\n* Terraform: `terraform version`\r\ncore@ip-10-0-0-250 ~/clusters $ **terraform version** \r\nTerraform v0.11.7\r\n+ provider.aws v1.13.0\r\n+ provider.ct (unversioned)\r\n+ provider.local v1.1.0\r\n+ provider.null v1.0.0\r\n+ provider.template v1.0.0\r\n+ provider.tls v1.1.0\r\n\ta) made sure no '~' was present in resource paths\r\n\tb) core@ip-10-0-0-250 ~/clusters $ **journalctl -u bootkube**\r\n-- Logs begin at Wed 2018-05-30 20:10:23 UTC, end at Wed 2018-05-30 23:10:27 UTC. --\r\n-- No entries --\r\ncore@ip-10-0-0-250 ~/clusters $ **systemctl status bootkube.service**\r\nUnit bootkube.service could not be found.\r\n\r\n* Plugins: Provider plugin versions\r\n* Ref: Git SHA (if applicable)\r\n\r\nfinally, **journalctl -xe**\r\nMay 30 22:32:22 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]: I0530 22:32:22.270292   771 libcurl_http_fetcher.cc:164] Setting up curl options for HTTPS\r\nMay 30 22:32:22 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]: I0530 22:32:22.270382   771 libcurl_http_fetcher.cc:427] Setting up timeout source: 1 seconds.\r\nMay 30 22:32:22 ip-10-0-0-250.us-west-2.compute.internal locksmithd[809]: LastCheckedTime=1527716865 Progress=0 CurrentOperation=\"UPDATE_STATUS_CHECKING_FOR_UPDATE\" NewVersion=0.0.0 NewSize=0\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]: I0530 22:32:23.529968   771 libcurl_http_fetcher.cc:240] HTTP response code: 200\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]: I0530 22:32:23.532069   771 libcurl_http_fetcher.cc:297] Transfer completed (200), 287 bytes downloaded\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]: I0530 22:32:23.532233   771 omaha_request_action.cc:592] Omaha request response: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]: <response protocol=\"3.0\" server=\"update.core-os.net\">\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]:  <daystart elapsed_seconds=\"0\"></daystart>\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]:  <app appid=\"e96281a6-d1af-4bde-9a0a-97b76e56dc57\" status=\"ok\">\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]:   <updatecheck status=\"noupdate\">\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]:    <urls></urls>\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]:   </updatecheck>\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]:  </app>\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]: </response>\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]: I0530 22:32:23.532907   771 omaha_request_action.cc:386] No update.\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]: I0530 22:32:23.533053   771 action_processor.cc:82] ActionProcessor::ActionComplete: finished OmahaRequestAction, starting OmahaResponseHandlerAction\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]: I0530 22:32:23.533161   771 omaha_response_handler_action.cc:36] There are no updates. Aborting.\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]: I0530 22:32:23.533277   771 action_processor.cc:68] ActionProcessor::ActionComplete: OmahaResponseHandlerAction action failed. Aborting processing.\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]: I0530 22:32:23.533385   771 action_processor.cc:73] ActionProcessor::ActionComplete: finished last action of type OmahaResponseHandlerAction\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]: I0530 22:32:23.533490   771 update_attempter.cc:290] Processing Done.\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]: I0530 22:32:23.533671   771 update_attempter.cc:326] No update.\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal update_engine[771]: I0530 22:32:23.533793   771 update_check_scheduler.cc:74] Next update check in 44m37s\r\nMay 30 22:32:23 ip-10-0-0-250.us-west-2.compute.internal locksmithd[809]: LastCheckedTime=1527719543 Progress=0 CurrentOperation=\"UPDATE_STATUS_IDLE\" NewVersion=0.0.0 NewSize=0\r\nMay 30 22:40:28 ip-10-0-0-250.us-west-2.compute.internal systemd-timesyncd[724]: Network configuration changed, trying to establish connection.\r\nMay 30 22:40:28 ip-10-0-0-250.us-west-2.compute.internal systemd-timesyncd[724]: Synchronized to time server 138.236.128.112:123 (0.coreos.pool.ntp.org).\r\n### Problem\r\n\r\nI'm trying to run Typhoon in AWS, setting up my launch point in AWS on a dedicated server in a public subnet. In the dozen attempts I've made it appears bookkube never starts. The behavior looks very similar to the following problem description:https://github.com/poseidon/typhoon/issues/167 \r\n\r\n1. t2.micro for workers -- could this be a problem?\r\n2. I must use  \"terraform import aws_default_vpc.default vpc-XXXXXXXX\" as my account predates default VPCs but I have tried specifying the default with this command, and I have also tried specifying another VPC in my account. The change makes no difference. \r\n3. I've tried running the install from both an AWS Linux installation and (what I believe to be) the same CoreOS ami (ami-574f362f) that Typhoon uses for the controller and workers. I assume either should work. \r\n4. I've reviewed this post (https://github.com/poseidon/typhoon/issues/167) and I believe I've addressed the issues mentioned there. \r\n5. I  have the asset direct listed as the same directory as I am launching terraform from. \r\n\r\nAny input would be greatly appreciated!!\r\n-Adrian\r\n\r\nHere is some of the (previously) requested output:\r\n\r\nHere is my providers.tf file -- only I truncated the public key\r\ncat providers.tf \r\n\r\nprovider \"aws\" {\r\n  version = \"~> 1.13.0\"\r\n  alias   = \"default\"\r\n\r\n  region                  = \"us-west-2\"\r\n  shared_credentials_file = \"/home/user/.aws/credentials\"\r\n}\r\n\r\nresource \"aws_default_vpc\" \"default\" {\r\n    tags {\r\n        Name = \"Default VPC\"\r\n    }\r\n}\r\n\r\nprovider \"local\" {\r\n  version = \"~> 1.0\"\r\n  alias = \"default\"\r\n}\r\n\r\nprovider \"null\" {\r\n  version = \"~> 1.0\"\r\n  alias = \"default\"\r\n}\r\n\r\nprovider \"template\" {\r\n  version = \"~> 1.0\"\r\n  alias = \"default\"\r\n}\r\n\r\nprovider \"tls\" {\r\n  version = \"~> 1.0\"\r\n  alias = \"default\"\r\n}\r\n\r\nmodule \"aws-tempest\" {\r\n  source = \"git::https://github.com/poseidon/typhoon//aws/container-linux/kubernetes?ref=v1.10.3\"\r\n\r\n  providers = {\r\n    aws = \"aws.default\"\r\n    local = \"local.default\"\r\n    null = \"null.default\"\r\n    template = \"template.default\"\r\n    tls = \"tls.default\"\r\n  }\r\n\r\n  # AWS\r\n  cluster_name = \"tempest\"\r\n  dns_zone     = \"was.example.com\"\r\n  dns_zone_id  = \"Z202GOHHUL8IHS\"\r\n\r\n  # configuration\r\n  ssh_authorized_key = \"ssh-rsa AAAAB3NzaC1yc2EAAAADA...core@ip-10-0-0-250.us-west-2.compute.internal\"\r\n  asset_dir          = \"/home/core/clusters\"\r\n\r\n  # optional\r\n  worker_count = 2\r\n  worker_type  = \"t2.micro\"\r\n}\r\n### Desired Behavior\r\n\r\nDetermine why bootkube fails to start. \r\n\r\n### Steps to Reproduce\r\n\r\nProvide clear steps to reproduce the issue unless already covered.\r\n\r\n## Feature Request\r\n\r\n### Feature\r\n\r\nDescribe the feature and what problem it solves.\r\n\r\n### Tradeoffs\r\n\r\nWhat are the pros and cons of this feature? How will it be exercised and maintained?\r\n",
      "comments": [
        "One last point: I am following the steps described in this web page to perform the installation. \r\n\r\nhttps://typhoon.psdn.io/cl/aws/#variables\r\n\r\nAre there are other prep steps / downloads / services not listed here? ",
        "The default controller EC2 instance type is `t2.small` because it is the smallest/cheapest that functions.\r\n\r\nI'm not entirely sure what you're up to with your VPC and image comment. Each Typhoon cluster creates its own VPC, there is no use of the default VPC and you don't get to pick your own VPC. You also just set `os_image` to something like `coreos-stable`, there's no picking AMIs by hand.\r\n\r\nCan you cleanup the report a bit? Probably don't need full system logs for this, just `journalctl -u bootkube.service`. Also, seeing no logs means you're probably on the wrong node. The one-time bootstrap is done on the 0th controller. That stuff should get you started.",
        "Oh, a few corrections.\r\n\r\n* You do show you're only using `t2.micro` for workers. Those should be kinda ok. You should likely be able to bootstrap, even though your cluster will be kinda little and maybe not super practical.\r\n* And I see your 3rd point is just about what host OS you're using - yeah any of those are fine. Any environment where you can run Terraform should be ok (laptop, server, CoreOS, macOS, etc.)\r\n* \"I have the asset direct listed as the same directory as I am launching terraform from\" <- I haven't tried that particular organization of files, but should be unrelated\r\n* Yes, https://typhoon.psdn.io/cl/aws/ is the official tutorial. It shows all steps from start to finish.\r\n\r\nBasically, let's get the `bootkube.service` logs. Everything else is just guessing in the dark. Now that I see you're running `terraform` from an EC2 instance as well, I think you're trying to journalctl on that same box (missed this bc most folks run terraform from their laptop). You wanna ssh to the 0th controller **in the cluster**. I think that's why you see nothing.",
        "Thank you. I'll up the size to small just to make sure that is not an issue. \r\n\r\nI'll set up a new cluster today and get back to you with the requested logs. \r\n\r\nLast thing - if I do NOT run ...\r\nterraform import aws_default_vpc.default vpc-XXXXXXX\r\n... and have the following associated entry in the providers.tf file ...\r\nresource \"aws_default_vpc\" \"default\" {\r\n    tags {\r\n        Name = \"Default VPC\"\r\n    }\r\n}\r\n... the install will fail with a 'failed to find default VPC'. So maybe not Typhoon but something in Terraform? No idea. But I'll keep setting it as it's no trouble, but I did want to make sure that was not the cause of some indeterminate issue. \r\n-Adrian\r\n",
        "**New cluster created. Same error:**\r\nmodule.aws-tempest.null_resource.bootkube-start: Still creating... (20m20s elapsed)\r\nmodule.aws-tempest.null_resource.bootkube-start (remote-exec): Job for bootkube.service failed because the control process exited with error code.\r\nmodule.aws-tempest.null_resource.bootkube-start (remote-exec): See \"systemctl status bootkube.service\" and \"journalctl -xe\" for details.\r\n\r\nError: Error applying plan:\r\n\r\n1 error(s) occurred:\r\n\r\n* module.aws-tempest.null_resource.bootkube-start: error executing \"/tmp/terraform_110858210.sh\": Process exited with status 1\r\n\r\nTerraform does not automatically rollback in the face of errors.\r\nInstead, your Terraform state file has been partially updated with\r\nany resources that successfully completed. Please address the error\r\nabove and apply again to incrementally change your infrastructure.\r\n\r\n**Log into the controller node:** \r\nLast login: Thu May 31 17:26:23 UTC 2018 from 34.220.43.233 on pts/0\r\nContainer Linux by CoreOS stable (1745.4.0)\r\nUpdate Strategy: No Reboots\r\nFailed Units: 1\r\n  bootkube.service\r\ncore@ip-10-0-4-150 ~ $ **systemctl status bootkube.service**\r\n\u25cf bootkube.service - Bootstrap a Kubernetes cluster\r\n   Loaded: loaded (/etc/systemd/system/bootkube.service; disabled; vendor preset: disabled)\r\n   Active: failed (Result: exit-code) since Thu 2018-05-31 17:46:43 UTC; 11min ago\r\n  Process: 1434 ExecStart=/opt/bootkube/bootkube-start (code=exited, status=1/FAILURE)\r\n Main PID: 1434 (code=exited, status=1/FAILURE)\r\n\r\nMay 31 17:46:43 ip-10-0-4-150 bootkube-start[1434]: [ 1495.548116] bootkube[6]: W0531 17:46:43.222525       6 create.go:57] Unable to determine api-server readiness: API Server http status: 0\r\nMay 31 17:46:43 ip-10-0-4-150 bootkube-start[1434]: [ 1495.551940] bootkube[6]: W0531 17:46:43.227117       6 create.go:57] Unable to determine api-server readiness: API Server http status: 0\r\nMay 31 17:46:43 ip-10-0-4-150 bootkube-start[1434]: [ 1495.552822] bootkube[6]: E0531 17:46:43.228001       6 create.go:66] API Server is not ready: timed out waiting for the condition\r\nMay 31 17:46:43 ip-10-0-4-150 bootkube-start[1434]: [ 1495.553627] bootkube[6]: Error: API Server is not ready: timed out waiting for the condition\r\nMay 31 17:46:43 ip-10-0-4-150 bootkube-start[1434]: [ 1495.554457] bootkube[6]: Tearing down temporary bootstrap control plane...\r\nMay 31 17:46:43 ip-10-0-4-150 bootkube-start[1434]: [ 1495.556423] bootkube[6]: Error: API Server is not ready: timed out waiting for the condition\r\nMay 31 17:46:43 ip-10-0-4-150 bootkube-start[1434]: [ 1495.557318] bootkube[6]: Error: API Server is not ready: timed out waiting for the condition\r\nMay 31 17:46:43 ip-10-0-4-150 systemd[1]: bootkube.service: Main process exited, code=exited, status=1/FAILURE\r\nMay 31 17:46:43 ip-10-0-4-150 systemd[1]: bootkube.service: Failed with result 'exit-code'.\r\nMay 31 17:46:43 ip-10-0-4-150 systemd[1]: Failed to start Bootstrap a Kubernetes cluster.\r\n\r\nAnd the **journalctl -xe command results:** (And forgive me for the verbosity  ... if there is a proper way to format these logs so they are not so ugly please let me know):\r\nMay 31 18:04:24 ip-10-0-4-150 etcd-wrapper[4781]: 2018-05-31 18:04:24.707213 W | pkg/netutil: failed resolving host tempest-etcd0.was.example.com:2380 (lookup tempest-etcd0.was.example.com on 10.0.0.2:53: no such host); retrying in 1s\r\nMay 31 18:04:24 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:24.752030     911 eviction_manager.go:247] eviction manager: failed to get get summary stats: failed to get node info: node \"ip-10-0-4-150\" not found\r\nMay 31 18:04:24 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:24.787061     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:452: Failed to list *v1.Service: Get https://tempest.was.example.com:443/api/v1/services?limit=500&resourceVersion=0: dial tcp: lookup tempest.was.\r\nMay 31 18:04:24 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:24.789366     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://tempest.was.example.com:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-0-4-150&limit=500&reso\r\nMay 31 18:04:24 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:24.789589     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:461: Failed to list *v1.Node: Get https://tempest.was.example.com:443/api/v1/nodes?fieldSelector=metadata.name%3Dip-10-0-4-150&limit=500&resourceVe\r\nMay 31 18:04:25 ip-10-0-4-150 kubelet-wrapper[911]: I0531 18:04:25.337269     911 kubelet_node_status.go:271] Setting node annotation to enable volume controller attach/detach\r\nMay 31 18:04:25 ip-10-0-4-150 kubelet-wrapper[911]: I0531 18:04:25.340613     911 kubelet_node_status.go:82] Attempting to register node ip-10-0-4-150\r\nMay 31 18:04:25 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:25.342543     911 kubelet_node_status.go:106] Unable to register node \"ip-10-0-4-150\" with API server: Post https://tempest.was.example.com:443/api/v1/nodes: dial tcp: lookup tempest.was.example.com on 10.0.0.2:53: no such h\r\nMay 31 18:04:25 ip-10-0-4-150 kubelet-wrapper[911]: W0531 18:04:25.550963     911 cni.go:171] Unable to update cni config: No networks found in /etc/kubernetes/cni/net.d\r\nMay 31 18:04:25 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:25.551344     911 kubelet.go:2130] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized\r\nMay 31 18:04:25 ip-10-0-4-150 etcd-wrapper[4781]: 2018-05-31 18:04:25.709688 W | pkg/netutil: failed resolving host tempest-etcd0.was.example.com:2380 (lookup tempest-etcd0.was.example.com on 10.0.0.2:53: no such host); retrying in 1s\r\nMay 31 18:04:25 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:25.789886     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://tempest.was.example.com:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-0-4-150&limit=500&reso\r\nMay 31 18:04:25 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:25.790533     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:452: Failed to list *v1.Service: Get https://tempest.was.example.com:443/api/v1/services?limit=500&resourceVersion=0: dial tcp: lookup tempest.was.\r\nMay 31 18:04:25 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:25.792472     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:461: Failed to list *v1.Node: Get https://tempest.was.example.com:443/api/v1/nodes?fieldSelector=metadata.name%3Dip-10-0-4-150&limit=500&resourceVe\r\nMay 31 18:04:26 ip-10-0-4-150 etcd-wrapper[4781]: 2018-05-31 18:04:26.712118 W | pkg/netutil: failed resolving host tempest-etcd0.was.example.com:2380 (lookup tempest-etcd0.was.example.com on 10.0.0.2:53: no such host); retrying in 1s\r\nMay 31 18:04:26 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:26.792696     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:452: Failed to list *v1.Service: Get https://tempest.was.example.com:443/api/v1/services?limit=500&resourceVersion=0: dial tcp: lookup tempest.was.\r\nMay 31 18:04:26 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:26.793356     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://tempest.was.example.com:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-0-4-150&limit=500&reso\r\nMay 31 18:04:26 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:26.795441     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:461: Failed to list *v1.Node: Get https://tempest.was.example.com:443/api/v1/nodes?fieldSelector=metadata.name%3Dip-10-0-4-150&limit=500&resourceVe\r\nMay 31 18:04:27 ip-10-0-4-150 etcd-wrapper[4781]: 2018-05-31 18:04:27.714474 W | pkg/netutil: failed resolving host tempest-etcd0.was.example.com:2380 (lookup tempest-etcd0.was.example.com on 10.0.0.2:53: no such host); retrying in 1s\r\nMay 31 18:04:27 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:27.795455     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://tempest.was.example.com:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-0-4-150&limit=500&reso\r\nMay 31 18:04:27 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:27.796108     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:452: Failed to list *v1.Service: Get https://tempest.was.example.com:443/api/v1/services?limit=500&resourceVersion=0: dial tcp: lookup tempest.was.\r\nMay 31 18:04:27 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:27.798266     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:461: Failed to list *v1.Node: Get https://tempest.was.example.com:443/api/v1/nodes?fieldSelector=metadata.name%3Dip-10-0-4-150&limit=500&resourceVe\r\nMay 31 18:04:28 ip-10-0-4-150 etcd-wrapper[4781]: 2018-05-31 18:04:28.716621 W | pkg/netutil: failed resolving host tempest-etcd0.was.example.com:2380 (lookup tempest-etcd0.was.example.com on 10.0.0.2:53: no such host); retrying in 1s\r\nMay 31 18:04:28 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:28.797996     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:452: Failed to list *v1.Service: Get https://tempest.was.example.com:443/api/v1/services?limit=500&resourceVersion=0: dial tcp: lookup tempest.was.\r\nMay 31 18:04:28 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:28.798669     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://tempest.was.example.com:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-0-4-150&limit=500&reso\r\nMay 31 18:04:28 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:28.800398     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:461: Failed to list *v1.Node: Get https://tempest.was.example.com:443/api/v1/nodes?fieldSelector=metadata.name%3Dip-10-0-4-150&limit=500&resourceVe\r\nMay 31 18:04:28 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:28.943578     911 event.go:209] Unable to write event: 'Patch https://tempest.was.example.com:443/api/v1/namespaces/default/events/ip-10-0-4-150.1533ca87a00f4568: dial tcp: lookup tempest.was.example.com on 10.0.0.2:53: no s\r\nMay 31 18:04:29 ip-10-0-4-150 etcd-wrapper[4781]: 2018-05-31 18:04:29.718872 W | pkg/netutil: failed resolving host tempest-etcd0.was.example.com:2380 (lookup tempest-etcd0.was.example.com on 10.0.0.2:53: no such host); retrying in 1s\r\nMay 31 18:04:29 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:29.800647     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://tempest.was.example.com:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-0-4-150&limit=500&reso\r\nMay 31 18:04:29 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:29.801322     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:452: Failed to list *v1.Service: Get https://tempest.was.example.com:443/api/v1/services?limit=500&resourceVersion=0: dial tcp: lookup tempest.was.\r\nMay 31 18:04:29 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:29.803220     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:461: Failed to list *v1.Node: Get https://tempest.was.example.com:443/api/v1/nodes?fieldSelector=metadata.name%3Dip-10-0-4-150&limit=500&resourceVe\r\nMay 31 18:04:30 ip-10-0-4-150 kubelet-wrapper[911]: W0531 18:04:30.552529     911 cni.go:171] Unable to update cni config: No networks found in /etc/kubernetes/cni/net.d\r\nMay 31 18:04:30 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:30.552891     911 kubelet.go:2130] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized\r\nMay 31 18:04:30 ip-10-0-4-150 etcd-wrapper[4781]: 2018-05-31 18:04:30.721050 W | pkg/netutil: failed resolving host tempest-etcd0.was.example.com:2380 (lookup tempest-etcd0.was.example.com on 10.0.0.2:53: no such host); retrying in 1s\r\nMay 31 18:04:30 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:30.803433     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:452: Failed to list *v1.Service: Get https://tempest.was.example.com:443/api/v1/services?limit=500&resourceVersion=0: dial tcp: lookup tempest.was.\r\nMay 31 18:04:30 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:30.804140     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://tempest.was.example.com:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-0-4-150&limit=500&reso\r\nMay 31 18:04:30 ip-10-0-4-150 kubelet-wrapper[911]: E0531 18:04:30.806881     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:461: Failed to list *v1.Node: Get https://tempest.was.example.com:443/api/v1/nodes?fieldSelector=metadata.name%3Dip-10-0-4-150&limit=500&resourceVe\r\nMay 31 18:04:31 ip-10-0-4-150 etcd-wrapper[4781]: 2018-05-31 18:04:31.649633 E | pkg/netutil: could not resolve host tempest-etcd0.was.example.com:2380\r\nMay 31 18:04:31 ip-10-0-4-150 etcd-wrapper[4781]: 2018-05-31 18:04:31.650403 C | etcdmain: failed to resolve https://tempest-etcd0.was.example.com:2380 to match --initial-cluster=etcd0=https://tempest-etcd0.was.example.com:2380 (failed to resolve \"https://tempest-etcd0.was.example.com:23\r\nMay 31 18:04:31 ip-10-0-4-150 systemd[1]: etcd-member.service: Main process exited, code=exited, status=1/FAILURE\r\nMay 31 18:04:31 ip-10-0-4-150 systemd[1]: etcd-member.service: Failed with result 'exit-code'.\r\nMay 31 18:04:31 ip-10-0-4-150 systemd[1]: Failed to start etcd (System Application Container).\r\n-- Subject: Unit etcd-member.service has failed\r\n\r\n----\r\nFinal thoughts: Unable to tell if this was a service that failed to start or a networking error. I did check Route53 and installer creates the A record (tempest-etcd0.was.example.com), and the A record alias (tempest.was.example.com), SOA (was.example.com) and the NS records correctly. And ports 2379-2380 are allowed in the security group config, so it seems that the networking bits are correct. \r\n\r\nThoughts? Anything other data I can provide? \r\n\r\n",
        "You can place logs in a gist or wrap in triple ticks (```).\r\n\r\nGeneral advice: If bootkube.service can't reach apiserver, look at some lower-level components. `journalctl -u etcd-member` and `journalctl -u kubelet`. Also, in the AWS console, look at the apiserver's NLB and see if it lists the instances at healthy, otherwise bootkube will never be able to reach it.\r\n\r\nAdvice for your case: Its definitely a problem that apiserver can't resolve the DNS record for etcd: `failed resolving host tempest-etcd0.was.example.com:2380`. Can you check that the A record was created, it points to the controller, and you can resolve it yourself (ssh to the controller and `dig`). Check the etcd logs too. Something is weird here.\r\n\r\nAlso look at how your Terraform definition may differ from the tutorial or make some assumptions. Typhoon mostly eliminates possible ways of misconfiguring things and failures and needing to debug aren't a normal thing (just for context). I deployed an AWS cluster yesterday too.",
        "In regard to the default VPC thing, I'm not familiar with that. Typhoon shouldn't be using the default VPC anywhere. I'd be curious if it is trying to somewhere. Heck, if I could delete my default VPC, I would, but AWS doesn't seem to allow it.",
        "The route53 info on was.example.com is as follows (sole hosted zone in this environment):\r\n\r\nwas.example.com. | SOA | ns-1536.awsdns-00.co.uk. awsdns-hostmaster.ama ...\r\ntempest-etcd0.was.example.com. | A | 10.0.4.150\r\ntempest.was.example.com. | A | \u00a0ALIAS tempest-apiserver-a8adaa16494a272a.elb.us-west-2.amazonaws.com. (z18d5fsroun65g) | \u00a0\r\n\r\nThis looks to me like it points to the controller but I am not an expert here. Also to note the \"Alias Hosted Zone ID: Z18D5FSROUN65G\" -- this seems to be associated with both the tempest-apiserver and tempest-ingress -- as these are public facing maybe z18d5fsroun65g was created for them as part of the install process for the load balancers.  It is not the same as the the hosted dns_zone_id  = \"Z202GOHHUL8IHS\" but I think this as it should be.  \r\n\r\n>dig tempest-etcd0.was.example.com\r\n\r\n; <<>> DiG 9.10.2-P4 <<>> tempest-etcd0.was.example.com\r\n;; global options: +cmd\r\n;; Got answer:\r\n;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 5779\r\n;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1\r\n\r\n;; OPT PSEUDOSECTION:\r\n; EDNS: version: 0, flags:; udp: 4096\r\n;; QUESTION SECTION:\r\n;tempest-etcd0.was.example.com.\tIN\tA\r\n\r\n;; AUTHORITY SECTION:\r\nexample.com.\t\t32\tIN\tSOA\tsns.dns.icann.org. noc.dns.icann.org. 2018050804 7200 3600 1209600 3600\r\n\r\n;; Query time: 0 msec\r\n;; SERVER: 10.0.0.2#53(10.0.0.2)\r\n;; WHEN: Thu May 31 22:16:14 UTC 2018\r\n;; MSG SIZE  rcvd: 115\r\n----\r\nWhat is server 10.0.0.2? OK - now we are onto something.\r\n\r\n**journalctl -u kubelet** is showing something very wrong -- it is a very long list of this same error message:\r\nMay 31 22:16:58 ip-10-0-4-150 kubelet-wrapper[911]: E0531 22:16:58.819005     911 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:452: Failed to list *v1.Service: Get https://tempest.was.example.com:443/api/v1/services?limit=500&resourceVersion=0: dial tcp: lookup tempest.was.example.com on 10.0.0.2:53: no such host\r\n\r\nWhich is correct as **10.0.0.2 is not associated with any IP, public or private, in my environment.**  Where would it get that IP? Workers (10.0.13.93, 10.0.47.10) and controller (10.0.4.150). My 'launchpads' for setting up the clusters are (10.0.0.250, 10.0.0.172).  I have not allocated an EIP with this address. Any idea why it would be looking for this IP?  \r\n\r\nNot sure how to fix this or what would cause it. \r\n\r\nMy Terraform template is cut directly from the tutorial page, and I have added/changed the following elements:\r\nregion                  = \"us-west-2\"\r\nshared_credentials_file = \"/home/user/.aws/credentials\r\ndns_zone     = \"was.example.com\"\r\ndns_zone_id  = \"Z202GOHHUL8IHS\"\r\nssh_authorized_key\r\nasset_dir\r\nresource \"aws_default_vpc\" \"default\" {\r\n    tags {\r\n        Name = \"Default VPC\"\r\n    }\r\n}\r\nNothing else has been changed. \r\n\r\nThanks! ",
        "I have a successful build of the cluster.  \r\n\r\nI noticed that the cluster did not associate itself with the private hosted domain so I did two things: \r\n> Removed all references to default VPC.\r\n> Manually associated the VPC with the hosted zone just after the new VPC was created. \r\n\r\nIt's possible the default VPC fail was a one time spurious thing. No idea why the association did not occur on the last couple builds but manual association did the trick. \r\n\r\nClosing this report. ",
        "I can't stop Windows update services by using the above method. I stop the services in the properties option.\r\nhttps://www.hptechnicalsupportphonenumbersusa.com/hp-customer-care/"
      ]
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/poseidon/typhoon/issues/336",
    "content": {
      "title": "Custom worker instance groups not referenced in GCE LB ingress",
      "body": "## Bug\r\n\r\n### Environment\r\n\r\n* Platform: google-cloud\r\n* OS: container-linux / stable (1855.5.0)\r\n* Typhoon modules: v1.12.2\r\n* Terraform: v0.11.10\r\n+ Terraform provider.ct: v0.3.0\r\n+ Terraform provider.google: v1.19.1\r\n+ Terraform provider.local: v1.1.0\r\n+ Terraform provider.null: v1.0.0\r\n+ Terraform provider.template: v1.0.0\r\n+ Terraform provider.tls: v1.2.0\r\n\r\n### Problem\r\n\r\nI've setup two worker pools for GCE following the Typhoon documentation and leave out a specific worker configuration in the `variables.tf`, but Teraform is going to create a dedicated worker instance named \"$CLUSTER_PREFIX-worker-$ID\". I did not noticed this behavior prior v1.12.2. If I delete this node afterwards it is recreated some time later by \"../InstanceGroupManagers/xxx-worker-group\". And now the real problem; This single instance is in its own \"Instance group\" named \" $CLUSTER_PREFIX-worker-group\" where both ingresses (http/https) are pointing to. Also the health check of the load balancer has a problem with this instance it shows a unhealthy state that leads to the problem that the public nginx-ingress isn't reachable.\t\r\n\r\n***terraform plan**\r\n```shell\r\n...\r\ngoogle_compute_instance_template.worker: Refreshing state... (ID: xxx-small-static-worker-20181103093325568000000003)\r\ngoogle_compute_instance_template.worker: Refreshing state... (ID: xxx-small-preemptible-worker-20181103093325414000000002)\r\ngoogle_compute_region_instance_group_manager.workers: Refreshing state... (ID: xxx-small-static-worker-group)\r\ngoogle_compute_region_instance_group_manager.workers: Refreshing state... (ID: xxx-worker-group)\r\n...\r\n```\r\n\r\n**variables.tf**\r\n```hcl\r\nmodule \"google-cloud\" {\r\n  source = \"git::https://github.com/poseidon/typhoon//google-cloud/container-linux/kubernetes?ref=v1.12.2\"\r\n\r\n  providers = {\r\n    google   = \"google.default\"\r\n    local    = \"local.default\"\r\n    null     = \"null.default\"\r\n    template = \"template.default\"\r\n    tls      = \"tls.default\"\r\n  }\r\n\r\n  # Google Cloud\r\n  cluster_name  = \"${var.gcloud_cluster_name}\"\r\n  region        = \"${var.gcloud_region}\"\r\n  dns_zone      = \"${var.gcloud_dns_zone}\"\r\n  dns_zone_name = \"${var.gcloud_dns_zone_name}\"\r\n\r\n  # General configuration\r\n  cluster_domain_suffix = \"xxx.local\"\r\n  ssh_authorized_key = \"${var.gcloud_ssh_authorized_key}\"\r\n  asset_dir          = xxx/.secrets/clusters/xxx\"\r\n\r\n  # Nodes\r\n  os_image           = \"coreos-stable\"\r\n  controller_count   = 2\r\n  controller_type    = \"n1-standard-1\"\r\n}\r\n```\r\n\r\n**worker-pool.tf**\r\n```hcl\r\nmodule \"worker-pool-small-static\" {\r\n  source = \"git::https://github.com/poseidon/typhoon//google-cloud/container-linux/kubernetes/workers?ref=v1.12.2\"\r\n\r\n  providers = {\r\n    google = \"google.default\"\r\n  }\r\n\r\n  # Google Cloud\r\n  region       = \"${var.gcloud_region}\"\r\n  network      = \"${module.google-cloud.network_name}\"\r\n  cluster_name = \"${var.gcloud_cluster_name}\"\r\n\r\n  # Configuration\r\n  name               = \"${var.gcloud_cluster_name}-small-static\"\r\n  kubeconfig         = \"${module.google-cloud.kubeconfig}\"\r\n  ssh_authorized_key = \"${var.gcloud_ssh_authorized_key}\"\r\n\r\n  # Optional\r\n  count        = 1\r\n  machine_type = \"n1-standard-1\"\r\n  os_image     = \"coreos-stable\"\r\n  preemptible  = false\r\n}\r\n\r\nmodule \"worker-pool-small-preemptible\" {\r\n  source = \"git::https://github.com/poseidon/typhoon//google-cloud/container-linux/kubernetes/workers?ref=v1.12.2\"\r\n\r\n  providers = {\r\n    google = \"google.default\"\r\n  }\r\n\r\n  # Google Cloud\r\n  region       = \"${var.gcloud_region}\"\r\n  network      = \"${module.google-cloud.network_name}\"\r\n  cluster_name = \"${var.gcloud_cluster_name}\"\r\n\r\n  # Configuration\r\n  name               = \"${var.gcloud_cluster_name}-small-preemptible\"\r\n  kubeconfig         = \"${module.google-cloud.kubeconfig}\"\r\n  ssh_authorized_key = \"${var.gcloud_ssh_authorized_key}\"\r\n\r\n  # Optional\r\n  count        = 2\r\n  machine_type = \"n1-standard-1\"\r\n  os_image     = \"coreos-stable\"\r\n  preemptible  = true\r\n}\r\n```\r\n\r\n### Desired Behavior\r\n\r\nI would expect that all \"custom\" workers from the pools are in the generic instance group \"$CLUSTER_PREFIX-worker-group\" or at least referenced in the load balancer backend configuration. It was completely fine prior v1.12.2 that the additional worker wasn't created. Maybe I'm not using the worker pool correctly or the documentation is outdated. I also tried to define \"worker_count = 0\" in `variables.tf` but this didn't change anything.\r\nLet me know if more informations are required.\r\n",
      "comments": [
        "Typhoon GCP clusters consist of a desired number of controllers and a managed instance group of workers (default `worker_count` is 1) that are homogeneous (e.g. workers have the same type, preemptibility, etc.). For convenience, each cluster comes with load balancer forwarding rules forwarding to workers running a healthy Ingress Controller. Apply an Ingress controller (see [nginx-ingress](https://typhoon.psdn.io/addons/ingress/#google-cloud) addon) to use the feature, otherwise its perfectly fine and expected there won't be ingress backends. Nothing about this design has changed in recent releases and Typhoon clusters on AWS and Azure provide the same model.\r\n\r\n[Workers pools](https://typhoon.psdn.io/advanced/worker-pools/) (advanced) are for more skilled users who wish to attach additional (different!) managed instance groups (or ASGs on AWS or SS's on Azure) of workers to a cluster for certain workloads (e.g. a mix of `non-preemptible` and `preemptible`, different machine types, GPUs, etc.). Attached worker pools aren't part of the included ingress backend service (its not generally plausible to do so, especially consistently on different clouds). Depending on the complexity of attached worker pools, if you want to use the included ingress load balancer, you'll have to think about where you want ingress controllers to be run.\r\n\r\nLooking at your usage, first, there's no reason for that `static-small` worker pool. Its no different from the workers in the cluster's default managed instance group of workers (same machine_type, region, os_image, etc.) Delete it and instead just set `worker_count` to desired. So then you'll have a cluster with a mix of non-premeptible and prepemtible (via the worker pool) workers.\r\n\r\nNext, you'd typically just need to ensure some ingress controllers run on the non-preemptible workers (either run enough you're sure there will be some on non-premeptible workers, use a daemonset, or select with node selectors or taints). If you decide you also want to serve ingress traffic from worker pools (e.g. your premptible workers), you should create your own load balancer instead of using the convenient included one. Each GCP worker pool has an output [variable](https://github.com/poseidon/typhoon/blob/master/google-cloud/container-linux/kubernetes/workers/outputs.tf#L1) `instance_group` intended to allow creating load balancers with any combination of worker pools. \r\n\r\n",
        "Hey, thank you for your detailed answer.\r\n\r\nBoth worker pools were just tests. I know that the simple one makes no sense in this configuration. Some time ago I evaluated Typhoon version 1.12.1 with exact the same configurations, but probably some other dependencies versions of Terraform plugins and the single worker wasn\u2018t created at all and the pool worker were in the loadbalancer instance group after installing the nginx-ingress addon. So I\u2019m just wondering what happens.\r\n\r\nIn conclusion, a easy solution for my problem is to add every custom worker instance group to the included load balancer setup and live with the single worker that is created by default (if it\u2019s not possible to stop the automatic worker creation by Typhoon). ",
        "I can only suspect there's some confusion in the observation then.\r\n\r\nTyphoon clusters have a managed instance group of workers. If there were some issue with workers creating, there would be no nodes for typical workloads (without pools). That would be a big honking clear issue. Conformance wouldn't pass. Most apps wouldn't work. I'd be getting a ton of pages (I run clusters for each Typhoon release myself). We can spin up a v1.12.1 to verify this isn't the case. I can't imagine a plugin combination that could cause that unless GCP shipped a very broken plugin and quickly fixed it (I believe I've used every successive one though).\r\n\r\nAnd attached worker pool nodes aren't included in the ingress's instance group. No part of Typhoon does that. Perhaps someone on your team added the instance group to the backend service via the UI? Or perhaps you're thinking of a [v1.10.5](https://github.com/poseidon/typhoon/blob/master/CHANGES.md#v1105) or earlier cluster, which the last change to the ingress architecture.\r\n\r\nWhile you could manually add other instance groups to the backend service, be aware that on the next `terraform apply` (perhaps you wish to adjust the worker count), that change will be lost. Notice how Google's [google_compute_backend_service](https://www.terraform.io/docs/providers/google/r/compute_backend_service.html) uses a backend object, not a list. And there is no \"attachment-style\" object either. Hence, Typhoon includes an Ingress LB for the default workers or gives you the output to go create your own. Further improvements would need improvements from Google provider.",
        "Alright thanks again. Probably someone else here deleted the managed default worker instance group and added the custom ones to the lb backend. And for whatever reasons Terraform didn\u2019t recognize a change. Guess this issue can be marked as solved.\r\nJust a short additional question: Is there any disadvantage to let every default worker be preemtible? I can\u2018t find one, especially if the cluster configuration will drain this node before it\u2019s termination. Will Typhoon take care that there is at least one worker node running or start a new one shortly after the last worker is gone?",
        "> Is there any disadvantage to let every default worker be preemtible?\r\n\r\nWell yes. Every worker may be preempted at the same time and not replaced for an arbitrary amount of time (though very unlikely). Typhoon is not involved here. Managed instance groups are a Google Cloud concept. They aim to maintain a desired number of nodes. With `premeptible=true`, you're agreeing to pay a lower price, but Google doesn't guarantee they will have capacity for you (e.g. if they're overloaded). Be sure you're cool with the risk. https://cloud.google.com/compute/docs/instances/preemptible"
      ]
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/poseidon/typhoon/issues/392",
    "content": {
      "title": "Bootstrapping in existing vpc",
      "body": "An option to create the cluster in existing vpc along with existing external Ips for NAT gateway, or existing NAT gateway.",
      "comments": [
        "Existing VPC support was intentionally not added.\r\n\r\nOn providers with virtual network concepts, each cluster creates its own network (AWS VPC, GCP network, Azure network) that defines IP addressing, subnetting, gateways, routes, subnets, or firewall rules (AWS security groups, Azure security rules). Users may define resources to add or extend the VPC/network (e.g. add security groups), but the network is managed under the purview of the cluster.\r\n\r\nWhen cross-cluster traffic is required, clusters' VPCs/networks should be peered or tunneled explicitly. Design for multiple clouds from day one (e.g. what are you going to do when the other cluster isn't on AWS?). Avoid relying on provider-specific cross-traffic mechanisms (like sharing the same VPC) that lead to lock-in.\r\n\r\nA few years ago, customer asks required me to design CoreOS Tectonic clusters to allow existing AWS VPCs. Its a troublesome design.\r\n\r\n* Existing VPCs introduce a lot of complexity. Its error-prone. Passing in a pre-existing network/VPC resource creates multiple modes of use, one path creating the VPC, one path for bring-your-own VPC (in various states). An existing VPC would have to be configured to avoid conflicts or collisions with resources created in the module, but those assumptions can't be enforced well. There's also the complexity of build-up - if a partially configured existing VPC were provided, to what degree should missing required resources be added vs assumed.\r\n\r\n* Existing VPCs reduce the ease of use and make a module harder to reason about - the module should create a self-contained set of managed, cohesive resources needed for and owned by the cluster. One cluster per VPC. Regardless of cloud.\r\n* Existing VPCs fail the \"can we write good documentation\" litmus test. I've had to write the docs on it before, and its a mess to explain and worse to support.\r\n* Existing VPC support limits our ability to make cluster virtual network architectural improvements. It changes responsibility for the VPC from managed by the module to a mix that leads to breaking changes, painful migration docs, or upset users in the future.\r\n* Finally, asks for existing VPC only come from AWS users. Take that as you will, but its a design hint. Consider the thought experiment, would we want to extend this design to other platforms? In my mind, its not a good pattern. AWS can't be special.\r\n\r\nI'm aware plenty of organizations have historical VPC designs (e.g. everything in one big VPC) that require shared VPCs. But those are out of scope here, this project is able to draw the line, enforce the right practices, and focus on stated goals.\r\n\r\nIf you really need to share an AWS VPC, you might look at some prior work that former colleagues and I did in Tectonic or other approaches.",
        " @user One main reason for using existing vpc is cost-effectiveness. As aws with last reinvent paved a foundation for sharing vpc by making it a free mechanism for same AZ communication (https://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html).\r\n\r\nThis vanished most of the data-transfer charges right away in cross-cluster communications also, as well as simplify infra.\r\n\r\nIn fact in the current scenario, the way to move forward for our org is to create all k8 clusters in the same vpc. \r\n\r\nThough it seems anti-pattern, it has its other advantages as well. "
      ]
    },
    "codes": [
      "networking",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/poseidon/typhoon/issues/475",
    "content": {
      "title": "Feature request: GCP: provide toggle to disable IPv6 FW rule",
      "body": "Global forwarding rules are not cheap on GCP. In my case, they account for just a bit less than half the cost for a 3 n1-standard-1 premptible nodes cluster.\r\n\r\nAn extra Ipv6 address and forwarding rule was introduced in https://github.com/poseidon/typhoon/commit/5be5b261e2a6a4c7628b80a2ae3deb626ba7a945 Would you accept a PR that introduce a variable/toggle to optionally not create those resources ? ",
      "comments": [
        "https://twitter.com/dghubble/status/1074186919087468544\r\n\r\nI'm keenly aware GCP charges this way and I agree its annoying. I pay for my own clusters, so I get it. But I designed it this way because I believe IPv6 adoption is important enough. I have ensured only 5 global forwarding rules are created - since GCP charges for the first batch of 5, its the same cost for folks with one cluster in their project.\r\n\r\nAs for additional variables to disable IPv6, I'd prefer not to for several reasons. First, my aim is to go in the opposite direction so that Ingress is exposed via an IPv4 and IPv6 address on every platform, consistently and simply out of the box. I don't want to create a situation where someone chooses between IPv6 adoption and cutting costs. I disagree with GCP's decision to charge separately for IPv6 rules and dislike the idea of adding complexity here to accomodate their decisions (e.g. Google Cloud has the resources to be encouraging IPv6 adoption here, but instead the asks bubble up to this project to add/document special casing and possibly turn off IPv6). Finally, if any forwarding rules were to be made optional, it should be the IPv4 ones over the very long term.",
        "Understood, fair enough."
      ]
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/poseidon/typhoon/issues/474",
    "content": {
      "title": "Consecutive \"destroy\" fails with 'variable \"XX\" is nil, but no error was reported'",
      "body": "Hi,\r\nWhen I try to use the module and issue a terraform destroy twice, the 2nd time it fails with \r\n\r\n```\r\nError: Error applying plan:\r\n\r\n4 error(s) occurred:\r\n\r\n* module.google-cloud-jordan.module.workers.output.instance_group: variable \"workers\" is nil, but no error was reported\r\n* module.google-cloud-jordan.module.bootkube.output.kubeconfig-admin: variable \"kubeconfig-admin\" is nil, but no error was reported\r\n* module.google-cloud-jordan.module.bootkube.output.kubeconfig-kubelet: variable \"kubeconfig-kubelet\" is nil, but no error was reported\r\n* module.google-cloud-jordan.module.workers.output.target_pool: variable \"workers\" is nil, but no error was reported\r\n```\r\nIt looks like a common TF issue (https://github.com/hashicorp/terraform/issues/17862) but there was a work around that be great.",
      "comments": [
        "I'm not really sure what you're running into or how we'd address your situation here.\r\n\r\nI will say, in practice, I never seem to need `terraform destroy`. I think in pretty much every scenario, it is better and clearer to destroy resources by removing cluster declarations from Terraform configuration, `terraform plan`, and `terraform apply` as usual. This flow keeps the same plan/apply flow as during creation, aligns well with keeping declarations in version control, and is compatible with automation if you choose to automate applies. That's why the [docs](https://typhoon.psdn.io/topics/maintenance/#cloud) on removing clusters recommend this too.\r\n\r\nBut I don't have much more help to offer.",
        "I am toying with Typhoon K8s and I also create a whole lot of other resources. At the end of the day, I like to terraform destroy everything to cut cost. Typhoon resources are deleted but I sometimes have to run \"terraform destroy\" twice for other reasons and I got the mentioned error message on the second run.\r\n\r\nAnyway, I think it's a bug in TF and I might suggest a workaround here if I find one not too ugly.",
        "Yeah, currently I know GCP and Azure clusters require two `apply` runs to remove (or `destroy` I suppose). I keep track of known upstream issues like this on each cloud in [errata](https://github.com/poseidon/typhoon/wiki/Errata). GCP its just related to cleanup timeout for the network resource (provider doesn't allow setting this), whereas Azure / azurerm provider is less reliable and sometimes just requires deleting the resource group out of band (its alpha).",
        "Just going to keep this tracked in errata. There are a bunch of changes going on with providers for a few months and I'm hoping things get better once we all get over the v0.12 hill. "
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/poseidon/typhoon/issues/489",
    "content": {
      "title": "Add support for Packet",
      "body": "We adapted Typhoon to run on Packet in [Lokomotive](https://github.com/kinvolk/lokomotive-kubernetes). In order to do this, we had to make a number of changes and include additional add-ons (e.g. MetalLB). Would Packet support be something that would be accepted in Typhoon if we did the work to prepare that and get it in?",
      "comments": [
        "I've looked into [Packet.net](https://www.packet.com/) before I'm not interested in it yet. There are a number of factors:\r\n\r\n* Packet: Use of Packet still has some warts / missing bits. Like, in your fork you've had to choose AWS Route53 for records since Packet has none. The multi-cluster balancing setup doesn't have a route toward stable, it relies on RR DNS of an external provider (contrast with bare-metal where the sky is the limit for how advanced users wanna get or DO (beta) where RR is used, but mainly due to cost cutting, LBs are present). Or firewalling just being done via the hosts. One could work around these issues, but there are also a variety of hosting platforms to choose from. I'm optimistic for Packet in the long run and really like their service, but I'm content to wait for the right time. \r\n* Supportability: I use and run Typhoon clusters on each supported platform. But I don't run clusters on Packet. They're too costly without the direct support of the provider. I also worry that same cost calculus skews the potential user base of a Packet variant, favoring enterprise interests. Basically, until I can justify a personal need for Typhoon on Packet, it shouldn't be added to the distro bc that directly affects quality and decision making. \r\n* Impl: I'd prefer to just implement new platforms afresh to know it inside and out, when the time is right. I think the minute decisions play a big role and initial working variants are straight forward enough. For example, I'd probably prefer direct integration via Matchbox and some other choices if I explored it.\r\n\r\nSo I'm currently not aiming to add support for Packet. I'm glad you guys are using it though. Packet guys are great!"
      ]
    },
    "codes": [
      "cluster",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/poseidon/typhoon/issues/558",
    "content": {
      "title": "Worker pool and Load Balancers",
      "body": "It's more a note and an open question than a real bug or feature request.\r\n\r\nSo I am experimenting with node labels and worker pools:\r\n```tf\r\nmodule \"google_cloud\" {\r\n  source = \"git::https://github.com/poseidon/typhoon//google-cloud/container-linux/kubernetes?ref=f82266ac8c436bc451690179673f092c2a351a48\"\r\n  ...\r\n  # optional\r\n  worker_count       = 0\r\n}\r\n\r\nmodule \"google_cloud_worker_pool\" {\r\n  source = \"git::https://github.com/poseidon/typhoon//google-cloud/container-linux/kubernetes/workers?ref=f82266ac8c436bc451690179673f092c2a351a48\"\r\n  ...\r\n  worker_count = 3\r\n  node_labels = [\"node_type=default\"]\r\n}\r\n```\r\n\r\nWorks fine, all my nodes join the cluster properly and labels are applied.\r\n\r\nBut the LBs (HTTP(s) and TCP) don't work because they only have the default/main worker pool as target pool, and in my setup its size is 0. So I am kinda paying for Global FW rules that have no use and I can't delete them because they will get created again in the next `terraform apply`.\r\n\r\nIf there is no way to do the right thing, maybe Typhoon should not create all these LB for me, or make it optional. What do you think ?",
      "comments": [
        "I've answered a similar question before https://github.com/poseidon/typhoon/issues/336#issuecomment-435608091. Here's the summary:\r\n\r\nTyphoon clusters consist of a desired number of controllers and a managed instance group / ASG / VMSS of workers (default worker_count is 1) that are homogeneous (e.g. workers have the same type, preemptibility, etc.). Clusters come with load balancing components allowing default workers to run an Ingress Controller (such as the nginx-ingress addon) and multiplex HTTP/HTTPS Ingress traffic. Except DigitalOcean (geared more for devs) where you must add your own load balancer ([as shown](https://typhoon.psdn.io/architecture/digitalocean/#custom-load-balancer).\r\n\r\nTCP/UDP service load balancing is supported for most platforms and shown in [tutorials](https://typhoon.psdn.io/architecture/google-cloud/#tcpudp-services).\r\n\r\nWorkers pools (advanced) are for users who wish to attach additional managed instance groups (or ASGs or VMSS's) of workers to a cluster for certain special workloads (e.g. a mix of non-preemptible and preemptible, different machine types, GPUs, etc.). Attached worker pools aren't part of the included ingress backend service (its not generally practical to do so, especially consistently on different clouds). If you must do more custom things, there is an `instance_group` output variable.\r\n\r\nFor you'd I say:\r\n\r\n* To use an Ingress controller, ensure you have at least one (default) worker in the cluster itself. \r\n* Avoid possibly confusing node-labels, like attaching a worker pool, but labeling its nodes \"default\". I'd think of worker pools a way of augmenting the default workers, but up to you.\r\n\r\nTyphoon clusters come ready to load balance Ingress traffic (while allowing a choice of Ingress Controller) and I don't anticipate any change there or option to deviate. Even DigitalOcean, before it becomes stable, should come with load balancing (it alone may have an opt-out). It keeps the project pragmatic but simple, with only a few well-chosen customization knobs on a platform.\r\n\r\n\r\n\r\n\r\n"
      ]
    },
    "codes": [
      "networking",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/poseidon/typhoon/issues/579",
    "content": {
      "title": "[Feature RequesT] Add VPC endpoint to S3",
      "body": "### Feature\r\n\r\nAdd support for a VPC endpoint to S3 https://docs.aws.amazon.com/en_pv/vpc/latest/userguide/vpc-endpoints-s3.html to reduce costs accessing S3 from the cluster\r\n\r\nThe implementation of this feature is simple https://github.com/aledbf/typhoon/commit/0210e5372b4d8e0d4ae1a86412652003e7a5829b\r\n\r\n### Tradeoffs\r\n\r\nThe commit I posted assumes a VPC endpoint in the same region.\r\n\r\n\r\n",
      "comments": [
        "tldr: I'd rather just add an output for the route table id so folks can add these for AWS services as needed.\r\n\r\n#### Mini AWS commentary\r\n\r\n_Shakes fist at AWS, not you_\r\n\r\nSo let's say one has an S3 bucket `foo`in `us-west-2` accessed by an app on Kubernetes. For resiliency, that app might run from any cloud or on-prem, from any region (although perhaps one favors `us-west-2`). So the app accesses the bucket by one uniform DNS name [supported by AWS for S3](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region) (pick one).\r\n\r\n* s3.us-west-2.amazonaws.com\r\n* s3.dualstack.us-west-2.amazonaws.com\r\n* account-id.s3-control.us-west-2.amazonaws.com\r\n* account-id.s3-control.dualstack.us-west-2.amazonaws.com\r\n\r\nAll good. Now a nice cloud provider might try to improve their service experience when accessed from their own cloud by resolving these names differently for you. GCP makes choices like this automatic. AWS instead invents \"VPC Endpoints\". Perhaps its a feature that you get manual control. But the con is users get the joy of manually adding a VPC endpoint for every AWS service they might wish to access, for every name it might be accessed by. Apps on a cluster might access `s3.us-west-2.amazonaws.com` and `s3.us-east-1.amazonaws.com` or other AWS services besides S3 too. We'd want to add them all? App details are bleeding into the cluster configuration. And apparently VPC endpoints don't work for the dualstack names. And have their own notions of policy too. Oh, AWS.\r\n\r\n#### Conclusion\r\n\r\nI think the right course is to enable folks to augment their clusters with VPC endpoint(s) for the service(s) they use, if they feel the network cost savings justify it. Adding an output of the route table id could offer that. That avoids Typhoon getting involved in choosing which AWS services and endpoints to create VPC endpoints for. There may also be some special reason to favor the public service endpoints in certain cases I'm unaware of (which could go to show why AWS took this add each endpoint approach).\r\n\r\nWould you be game to add the output and an example that adds it to the route table of a cluster after this section https://typhoon.psdn.io/architecture/aws/#firewalls?",
        "As mentioned, I'd take a route table id output to allow folks to add endpoints to their clusters.\r\n\r\nGoing this issue about having AWS builtin endpoints."
      ]
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/poseidon/typhoon/issues/703",
    "content": {
      "title": "Flatcar Linux Azure billing / licensing confusion",
      "body": "Raising some concerns about the Flatcar Linux Azure Marketplace image.\r\n\r\nOn Azure, [CoreOS](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/CoreOS.CoreOS?tab=PlansAndPrice) Container Linux has always been offered as a free image. It has no license accept step and no additional cost.\r\n\r\nThe [Flatcar Linux](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/kinvolk.flatcar-container-linux?tab=PlansAndPrice) offering is listed as BYOL (\"bring your own license\") and does not list additional charges (but also doesn't say free). The BYOL link doesn't go anywhere and Flatcar's [Azure docs](https://docs.flatcar-linux.org/os/booting-on-azure/) don't provide details about license acceptance or charges / pricing.\r\n\r\n### License\r\n\r\nThe slightly different Flatcar Linux offering requires users accept Kinvolks terms to use the Marketplace image.\r\n\r\n```\r\n$ terraform apply\r\nError message: 'Legal terms have not been accepted for this item on this subscription\r\n```\r\n\r\nRemedied by the following (suggested in Typhoon docs).\r\n\r\n```\r\naz vm image terms show --publish kinvolk --offer flatcar-container-linux --plan stable\r\naz vm image terms accept --publish kinvolk --offer flatcar-container-linux --plan stable\r\n```\r\n\r\nMy best attempts at investigating show you can use the `az` CLI to get a JSON blob that links to a personalized license that just contains a link to https://kinvolk.io/legal/open-source/.\r\n\r\n### Cost\r\n\r\nAccepting the Marketplace terms enables a subscription (its shown deeply burried in the Azure Marketplace console if you click on juuust the right things) which may have costs associated with it. Since trialing of Flatcar Linux Azure began, a new separate Azure Marketplace invoice line item (separate from Azure Service) has appeared.\r\n\r\nAzure's own detailed invoice is broken and the generic invoice PDF provides no details about even the name of the subscription, so its been difficult to confirm whether this is indeed coming from Flatcar Linux Azure. Its possible its something else.\r\n\r\n![Screenshot from 2020-04-11 21-13-58](https://user-images.githubusercontent.com/2253428/79060446-debb0500-7c39-11ea-98d9-0a2c4774c65d.png)\r\n\r\n### Actions\r\n\r\nThere are several problems with this situation:\r\n\r\n* Is Flatcar Linux Azure Marketplace image cost free or not? This should be clear.\r\n* What really is the Flatcar Linux BYOL policy? Is this step needed vs CoreOS?\r\n* Azure's Dashboard's UX is becoming more of a liability\r\n\r\nI've reached out to Kinvolk directly to gather some details.",
      "comments": [
        "Thanks @user  We're looking into this.",
        "I filed a support ticket with Azure on April 17 to look into the charge I saw on my side, since their invoices haven't provided details.",
        "We have a usage-based offering set at $0/hr coming in the next days that should resolve this from our side. I'll post here when that's available.\r\nWe'll then deprecate the BYOB offering and remove it some point in the future. ",
        " @user sound good :+1: \r\n\r\nAzure support replied to say they're looking into the unattributed invoice I received. Could be some fees on their end. Alternately, I'll know in 9 days if cancelling the offering removed the charge for the new billing cycle.",
        "It's live now. You can find the \"free\" offer here: https://azuremarketplace.microsoft.com/en-us/marketplace/apps/kinvolk.flatcar-container-linux-free\r\nHope that works for ya.",
        "Alright, I tried out the new free offering and added it in #729. Updated the docs for the az command needed to accept the terms that are the same (pointing to https://kinvolk.io/legal/open-source/). I think its more clear now this is $0.\r\n\r\nIn that time, no new charges have appeared so that checks out. The original charges issue I had is still being investigated by Azure, but its sounding more like a bug on their side that somehow began when I tried the byol subscription and may be no fault of Kinvolks.",
        "Thanks for looking into this @blixtra! "
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/poseidon/typhoon/issues/735",
    "content": {
      "title": "Container Registry Outages / recommendations",
      "body": "Status: Quay.io image registry [incident](https://status.quay.io/incidents/kw2627bsdwd9) is resolved.\r\n\r\nContainer image registry outages merit some comments. Typhoon relies on gcr.io for Kubernetes hosted images and quay.io for etcd, CNI (calico & flannel), and Kubelet images. There are two image usage categories: 1) on-host images in system units or static pods (etcd, kubelet) and 2) [in-cluster images](https://github.com/poseidon/terraform-render-bootstrap/blob/master/variables.tf#L68) (e.g. CoreDNS, calico-node).\r\n\r\n### Advice\r\n\r\nDuring a registry outage or breach, first look at the affected images to anticipate impact. For the Quay outage, we know existing worker nodes are mostly unaffected (already have Kubelet and Calico/Flannel) and controller nodes are mostly unaffected (they're repaired, not replaced). The impact is mostly on _new_ worker nodes during scaling events. You should naturally arrive at one of two strategies:\r\n\r\n**Passive** (recommended): Observe the above and look to preserve existing nodes, pause any scale-downs or other events you had planned. Do not panic and delete nodes. You're looking to weather the outage with your existing nodes. This is the approach I **recommend** and that I took. Limit changes. Its the least risk operationally.\r\n\r\n**Active**: If you must intervene to fix new nodes that can't fetch images, there are some options.\r\n\r\nIn-cluster images are managed on the cluster. To patch one (say to use the Calico Dockerhub miror), you'd edit (e.g. kubectl edit calico-node -n kube-system).\r\n\r\nFor on-host images, remember nodes are provisioned once immutably. On an existing cluster, `terraform apply` manages infrastructure, it does not mutate state (e.g. desired image) on existing nodes. For a small number of affected worker nodes, you can edit the Kubelet systemd unit to refer to a different image on another registry. For larger scale, you can edit the Kubelet image in the CLC or FCC module source and apply. Bear in mind this will update the AWS ASG, GCP instance-group, or Azure VMSS and involve replacing existing (possibly good) nodes.\r\n\r\n### Internal Impact\r\n\r\nTyphoon clusters run across supported platforms and power our own internal infrastructure. The Quay outage did not affect those clusters, except one. The passive strategy was taken of just letting clusters continue operating.\r\n\r\nThe exception was a GCP cluster that runs 100% preemptible instances for cost-saving reasons and hosts our [docs](https://typhoon.psdn.io/). Nodes are preempted within 24 hours, so replacement nodes experienced fetch issues. [Best practices](https://typhoon.psdn.io/topics/maintenance/#best-practices) are followed. All services on the cluster were failed over to a different cluster in about 20 minutes (docs site down from May 19, 11:57 - 12:15 PST). No effort was made to fiddle with changing images.\r\n\r\n### Enhancements\r\n\r\nSome enhancements I'd like to add in the future, to be tracked here.\r\n\r\n- [x] Tweak the on-host units to be patchable with a snippet and systemd dropin to be a bit nicer (you can actually do this already, but its a super large snippet). Doing it via a snippet still sends the signal that its not blessed to be overriding those images long term, but it would be nicer in ahttps://github.com/poseidon/typhoon/pull/749 pinch. ([#737](https://github.com/poseidon/typhoon/pull/737))\r\n- [x] [Kubelet](https://github.com/poseidon/kubelet) is published to `quay.io/poseidon/kubelet`. I know not everyone has spare registries lying around or can build and publish an image in a crisis. I may publish official copies to a backup registry, as Calico does (note, it cannot be gcr.io, they charge by bandwidth so I don't use it for things hosted to the public). ([#749](https://github.com/poseidon/typhoon/pull/749))\r\n\r\nFinally, :hugs: to the Quay team.",
      "comments": [
        "Another note. Some will ask about hosting images internally. If you're seeking better uptime, I don't recommend it - organizations that can actually host an image registry well are few and far between. Incidents should reinforce that.\r\n\r\nI won't have more to say on that in the short term. Its possible to do by carrying patches, but in the best of cases you'll still have to worry about image skew (you drift from what Typhoon tests), policy problems (internal access controls), and your own engineers causing drift (\"we're just gonna tweak this one thing in the kube-apiserver image...\").",
        "Addressed by https://github.com/poseidon/typhoon/pull/737 and https://github.com/poseidon/typhoon/pull/749"
      ]
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/poseidon/typhoon/issues/369",
    "content": {
      "title": "Support for aws ec2 fleet",
      "body": "\r\n## Feature Request\r\n\r\nSupport for aws ec2 fleet (https://aws.amazon.com/about-aws/whats-new/2018/04/introducing-amazon-ec2-fleet/) or spot-fleet which allows having the different type of spot instances.\r\n\r\nEc2 fleet has an advantage as they can be attached with autoscaling group.\r\n",
      "comments": [
        "Today, Typhoon [worker pools](https://typhoon.psdn.io/advanced/worker-pools/) provide the ability to add multiple auto-scaling groups of instances in a cluster and [spot](https://typhoon.psdn.io/cl/aws/#spot) is an option for both clusters and worker pools. So the ability to create clusters with a variety of combinations of instances works well already.\r\n\r\nThis request would involve changing the internal implementation from launch configurations to launch templates and an ASG of on-demand or spot instances to an EC2 fleet and with little or no change to the actual exposed set of variables. While the fleet is new and has ways of having a single pool with a mix of types, spot, etc. those are not amenable to being exposed. So the internal implementation could evolve this way, but it wouldn't provide new abilities.",
        " @user I believe it would provide a very required ability. To illustrate assume my service required 8 r4.large instances and I am planning to run the same as spot instance.\r\n\r\nNow to have a nearly constant uptime I have below options\r\n1. Simply use spot/ec2 fleet with different spot instance weights (e.g. r4.large as 1, r4.xlarge 2, m4.large 0.5) etc..\r\nAws spot fleet will automatically create 2 m4 large if it cannot find r4.large in spot pool.\r\n\r\n2. Create 3 different worker pools with 3 different instance type. Make r4 large as default 8 and others 0. Add some cluster autoscaler logic to recursively scale up other worker groups if no spot instance available in default the pool.\r\n\r\nAlso, the addition of a new type of instance(e.g. r3.large or r4.2xlarge) would be much easier and faster by specifying the same in existing fleet rather than creating a new worker pool. \r\n\r\nIn my current provisioned kubernetes, I am using spot fleet for fix workloads. This is a good cost saving and scaling is also very easy. For automated scale up and scale down I am relying on an on-demand worker pool.\r\n\r\nThe dirty way to achieve this is to create a spot worker pool as the donor, create a spot fleet and copy launch configuration scripts from the donor. Then scale down donor worker pool to zero.\r\n\r\nI use this script for same https://gist.github.com/zytek/d1e49e97c0ff1dcb965ab712351b035c\r\nWould be nice if same could be integrated better.",
        "Yes, a fleet abstraction of a pool that automatically uses a mix of instance types + on-demand vs spot would be an iteration of convenience.\r\n\r\nI point out the matter of implementation, we can already anticipate how it would look. [ec2_fleet](https://www.terraform.io/docs/providers/aws/r/ec2_fleet.html) and associated templates use inline block overrides, they do not lend themselves well to usage within a module that cleanly exposes simple options to users. That is the issue that make this unpalatable. Fleet could eventually be used internally, but it would not be able to let users to define a blend of overrides.\r\n\r\nThe fleet abstraction becomes a differentiator in cases a workload is sensitive to node fluctuations, where its really meaningful the fleet can automatically compensate for an unsatisfiable instance with a different type/durability. Consider though, the (admittedly more plain) worker pools (or a single type and durability) provide much of the same value in practice and address the majority of use-cases.\r\n  * Plain clusters come with a single ASG of workers\r\n  * Clusteres needing worker variety can attach pool(s) of desired types/durability\r\n  * A mix of on-demand, spot, and different worker pools provide quite sufficient insulation against spot interuptions or temporary zonal failures (they're regional). I'm able to comfortably use 100% spot for the last few years, while richer, more risk averse users may spread across types + say 20/80 on-demand to spot ration.\r\n\r\nSo its not to say AWS EC2 fleet isn't useful, but rather that its features aren't cleanly consumable through modules and worker pools address the majority of use cases in practice.\r\n\r\n\r\n\r\n\r\n\r\n",
        "Typhoon worker pools already achieve the use cases we care about, in a more general and composable way across platforms. I'm going to close this, as I don't plan to introduce ec2_fleet. ",
        "I'm not sure if I understand conversation here correctly. Is that just issues on exposing all EC2 fleet options cleanly?\r\n\r\nWe have currently only 1 option for using spot instance: spot_price.\r\n\r\nIf we opt in using ec2_fleet (or launch template and auto scaling group with mixed instance policy)\r\nWe need to expose following options:\r\n\r\n* instance_type and weighted_capacity\r\n* ",
        "Sorry I commented while writing it. There are many options, but most of them looks simple to expose.\r\n\r\nThe only problematic one is instance type override. Should we expose this as variable that takes list of objects?\r\n\r\non_demand_allocation_strategy\r\non_demand_base_capacity\r\non_demand_percentage_above_base_capacity\r\nspot_allocation_strategy\r\nspot_instance_pools\r\nspot_max_price\r\ninstance_type, weighted_capacity\r\n\r\nhttps://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/autoscaling_group#launch_template_specification"
      ]
    },
    "codes": [
      "saving",
      "feature",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/fortinet/azure-templates/issues/22",
    "content": {
      "title": "invalid license - please allow evaluation license",
      "body": "I've spent the past couple days [unsuccessfully] trying to get an FGT VM working in azure for evaluation.\r\n\r\nCan I request there be an evaluation license option for these azure templates?  as the eval license only allows 1cpu and 1gb ram and no https, it would have to be used with a B1s VM and set the external interface to allow http.\r\n\r\ncurrently I can \"deploy to azure\", and then change the vm size to B1s, and the appliance boots but console says invalid license as 'Vcpu exceed 0'.\r\n\r\nIt would make things so much easier for those working with an Azure visual studio subscription or trial azure subscription... those subscriptions don't allow the azure credit to be used on non-ms services from the marketplace.  Making these templates eval license compatible would also be a great resource for those studying for NSE exams.",
      "comments": [
        "Hi,\n\nEither you use PAYG license which means you will incur a cost from the Azure Marketplace for usage. Or you can send an email to azure@fortinet.com and request a trial license for the BYOL licensing model. \n\nHope this will help you in your training for NSE certification. \n\nRegards,\n\nJoeri",
        "Awesome! I will try that.\r\n\r\n\r\nJeremy Finney | Senior Systems Engineer\r\nE: ***@***.***<mailto:%7BE-mail%7D> P: 04 2600 600\r\nW: concepts.co.nz<http://concepts.co.nz/> A: Level 4, 15 Willeston Street, Central, Wellington, New Zealand, 6011\r\n\r\nFrom: Joeri ***@***.***>\r\nSent: Friday, 20 August 2021 8:53 am\r\nTo: fortinet/azure-templates ***@***.***>\r\nCc: Jeremy Finney ***@***.***>; Author ***@***.***>\r\nSubject: Re: [fortinet/azure-templates] invalid license - please allow evaluation license (#22)\r\n\r\n\r\nHi,\r\n\r\nEither you use PAYG license which means you will incur a cost from the Azure Marketplace for usage. Or you can send an email to ***@***.******@***.***> and request a trial license for the BYOL licensing model.\r\n\r\nHope this will help you in your training for NSE certification.\r\n\r\nRegards,\r\n\r\nJoeri\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/fortinet/azure-templates/issues/22#issuecomment-902236734>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AVIVTZN7ASSW5IHIKW44RADT5VVJNANCNFSM5COYCCPQ>.\r\nTriage notifications on the go with GitHub Mobile for iOS<https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android<https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>.\r\n",
        "Closing the issue for now. Feel free to reopen or create a new one in case you need more information."
      ]
    },
    "codes": [
      "awareness",
      "feature",
      "increase"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/appbricks/cloud-inceptor/issues/2",
    "content": {
      "title": "Update bastion instance to not use Elastic IPs",
      "body": "When an Elastic IP is allocated it incurs costs regardless of whether the instance it is attached to is running. Since the VPN service discovery is automated we should be able to use dynamically allocated IPs when instances resume. ",
      "comments": []
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/terraform-google-modules/terraform-google-vault/issues/134",
    "content": {
      "title": "Growing Metrics Volume Cost",
      "body": "### TL;DR\r\n\r\nUsage of the module incures a constantly growing cost of Metric Volume SKU.\r\n\r\n### Expected behavior\r\n\r\nI'd expect the costs to stay at constant level unless the cluster is scaled up or down.\r\n\r\n### Observed behavior\r\n\r\nAs seen on the attached screenshot, there is a constantly growing cost of Metric Volume SKU. There was no growth in Vault usage during this period. On November 24th I took away the _Monitoring Metric Writer_ role from Vault SA and that is when the Metric Volume usage dropped to 0.\r\n\r\nIs it expected for the Metric Volume usage to grow constantly like this? It seems strange to me that the metrics would cost more than the VM Vault is running on. What are those metrics needed for? What harm could it do if I disabled them on production environment?\r\n\r\n![image](https://user-images.githubusercontent.com/94702944/144055650-4d92d729-8a4d-4f68-a24c-68cffca566dd.png)\r\n\r\n\r\n### Terraform Configuration\r\n\r\n```hcl\r\nmodule \"vault\" {\r\n  source     = \"terraform-google-modules/vault/google\"\r\n  version    = \"~>6.0.0\"\r\n  project_id = var.project_id\r\n  region     = var.region\r\n\r\n  user_startup_script = \"sudo apt-get update && sudo apt-get install jq -y\"\r\n\r\n  load_balancing_scheme = \"INTERNAL\"\r\n  vault_version         = \"1.8.5\"\r\n\r\n  network                   = module.common-vpc.network_self_link\r\n  network_subnet_cidr_range = var.vault-subnetwork-range\r\n\r\n  kms_keyring    = \"\"\r\n  kms_crypto_key = \"\"\r\n\r\n  domain = \"\"\r\n\r\n  allow_public_egress = true\r\n\r\n  ssh_allowed_cidrs = [\r\n    \"\"\r\n  ]\r\n\r\n  storage_bucket_class    = \"REGIONAL\"\r\n  storage_bucket_location = var.region\r\n\r\n  vault_allowed_cidrs = [\"\"]\r\n\r\n  tls_ca_subject      = {\r\n    \"common_name\" : \"\",\r\n    \"country\" : \"\",\r\n    \"locality\" : \"\",\r\n    \"organization\" : \"\",\r\n    \"organizational_unit\" : \"\",\r\n    \"postal_code\" : \"\",\r\n    \"province\" : \"\",\r\n    \"street_address\" : [\r\n      \"\"\r\n    ]\r\n  }\r\n  tls_cn              = \"\"\r\n  tls_dns_names       = [\r\n    \"\"\r\n  ]\r\n  tls_save_ca_to_disk = false\r\n\r\n}\r\n\r\nmodule \"common-vpc\" {\r\n  source     = \"terraform-google-modules/network/google\"\r\n  version    = \"3.2.0\"\r\n  project_id = var.project_id\r\n\r\n  network_name            = \"\"\r\n  auto_create_subnetworks = false\r\n\r\n  subnets = [\r\n    {\r\n      subnet_name           = \"\"\r\n      subnet_ip             = \"\"\r\n      subnet_region         = \"\"\r\n      subnet_private_access = \"true\"\r\n    }\r\n  ]\r\n}\r\n```\r\n```\r\n\r\n\r\n### Terraform Version\r\n\r\n```sh\r\nTerraform v1.0.11\r\non linux_amd64\r\n+ provider registry.terraform.io/hashicorp/google v3.90.1\r\n+ provider registry.terraform.io/hashicorp/google-beta v4.1.0\r\n+ provider registry.terraform.io/hashicorp/local v2.1.0\r\n+ provider registry.terraform.io/hashicorp/null v3.1.0\r\n+ provider registry.terraform.io/hashicorp/random v3.1.0\r\n+ provider registry.terraform.io/hashicorp/template v2.2.0\r\n+ provider registry.terraform.io/hashicorp/tls v2.1.1\r\n```\r\n\r\n\r\n### Additional information\r\n\r\nUsing the metrics explorer I've been able to track down the growing metric to be `custom.googleapis.com/statsd/gauge`\r\n![image](https://user-images.githubusercontent.com/94702944/144071426-3f03c14f-2297-4eac-a683-79926430db3e.png)\r\n\r\n",
      "comments": [
        "This issue is stale because it has been open 60 days with no activity. Remove stale label or comment or this will be closed in 7 days"
      ]
    },
    "codes": [
      "cluster",
      "awareness",
      "increase"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/iacsecurity/tool-compare/issues/12",
    "content": {
      "title": "Add table listing license/cost of tools",
      "body": "",
      "comments": []
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/lancekuo/terraform-docker-swarm/issues/7",
    "content": {
      "title": "Have a volume_tags in aws_instance",
      "body": "For a better budget/cost perspective view, it is a great enhancement.",
      "comments": []
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/XenitAB/terraform-modules/issues/171",
    "content": {
      "title": "Add cost assesment from terrafrom at PR",
      "body": "There are tools like https://www.infracost.io/ to assess how much extra cost a terraform change will apply.\nI think this would be a great feature in our terraform environment, maybe not so much in the modules repo since we don't actually deploy anything from here. But I think it's a good place to follow up on it.\n\nSadly infracost dosen't support azure right now due to Azure don't have a cost api.\nhttps://github.com/infracost/infracost/issues/64\nhttps://github.com/MicrosoftDocs/feedback/issues/523\n\nThere are other tools in this space, terraform cloud also provides this feature but it's not open-source and you have to pay for the feature.\n\nThere are probably other options but infracost looks like a good one. They also provide ready github actions.\n\n",
      "comments": [
        "This is currently not of interest. We might want it in the future but if that is the case we should open it up again."
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/EtchUK/Etch.OrchardCore.SiteBoilerplate/issues/31",
    "content": {
      "title": "Terraform - Add created SQL database to elastic pool",
      "body": "Update the terraform script to ensure SQL database is added to an elastic pool.",
      "comments": [
        "Also the `edition` essentially the tier/cost of the database should be set by default to something small, since the default is _very_ expensive..."
      ]
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/rancher/quickstart/issues/146",
    "content": {
      "title": "Azure quickstart fails with errors regarding etcd",
      "body": "Hi,\r\n\r\nI tried running the azure quickstart. I created a service principal and configured the .tfvars file accordingly.\r\nHowever, when I call `terraform apply --auto-approve` it fails with the following error message (some names in paths are x'ed out).\r\n\r\n```\r\nError: \r\n============= RKE outputs ==============\r\ntime=\"2020-12-11T11:52:43+01:00\" level=info msg=\"Deleting RKE cluster...\"\r\ntime=\"2020-12-11T11:52:43+01:00\" level=info msg=\"Tearing down Kubernetes cluster\"\r\ntime=\"2020-12-11T11:52:43+01:00\" level=info msg=\"[dialer] Setup tunnel for host [20.52.52.111]\"\r\ntime=\"2020-12-11T11:53:04+01:00\" level=warning msg=\"Failed to set up SSH tunneling for host [20.52.52.111]: Can't retrieve Docker Info: error during connect: Get \\\"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/info\\\": Failed to dial ssh using address [20.52.52.111:22]: dial tcp 20.52.52.111:22: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.\"\r\ntime=\"2020-12-11T11:53:04+01:00\" level=warning msg=\"Removing host [20.52.52.111] from node lists\"\r\ntime=\"2020-12-11T11:56:12+01:00\" level=info msg=\"[rke_provider] rke cluster changed arguments: map[cluster_name:true kubernetes_version:true nodes:true]\"\r\ntime=\"2020-12-11T11:56:12+01:00\" level=info msg=\"Creating RKE cluster...\"\r\ntime=\"2020-12-11T11:56:12+01:00\" level=info msg=\"Initiating Kubernetes cluster\"\r\ntime=\"2020-12-11T11:56:12+01:00\" level=info msg=\"[dialer] Setup tunnel for host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:56:13+01:00\" level=info msg=\"Checking if container [cluster-state-deployer] is running on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:56:13+01:00\" level=info msg=\"Pulling image [rancher/rke-tools:v0.1.66] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:56:21+01:00\" level=info msg=\"Image [rancher/rke-tools:v0.1.66] exists on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:56:43+01:00\" level=info msg=\"Starting container [cluster-state-deployer] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:56:44+01:00\" level=info msg=\"[state] Successfully started [cluster-state-deployer] container on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:56:44+01:00\" level=info msg=\"[certificates] Generating CA kubernetes certificates\"\r\ntime=\"2020-12-11T11:56:44+01:00\" level=info msg=\"[certificates] Generating Kubernetes API server aggregation layer requestheader client CA certificates\"\r\ntime=\"2020-12-11T11:56:44+01:00\" level=info msg=\"[certificates] GenerateServingCertificate is disabled, checking if there are unused kubelet certificates\"\r\ntime=\"2020-12-11T11:56:44+01:00\" level=info msg=\"[certificates] Generating Kubernetes API server certificates\"\r\ntime=\"2020-12-11T11:56:44+01:00\" level=info msg=\"[certificates] Generating Service account token key\"\r\ntime=\"2020-12-11T11:56:44+01:00\" level=info msg=\"[certificates] Generating Kube Controller certificates\"\r\ntime=\"2020-12-11T11:56:44+01:00\" level=info msg=\"[certificates] Generating Kube Scheduler certificates\"\r\ntime=\"2020-12-11T11:56:44+01:00\" level=info msg=\"[certificates] Generating Kube Proxy certificates\"\r\ntime=\"2020-12-11T11:56:44+01:00\" level=info msg=\"[certificates] Generating Node certificate\"\r\ntime=\"2020-12-11T11:56:44+01:00\" level=info msg=\"[certificates] Generating admin certificates and kubeconfig\"\r\ntime=\"2020-12-11T11:56:45+01:00\" level=info msg=\"[certificates] Generating Kubernetes API server proxy client certificates\"\r\ntime=\"2020-12-11T11:56:45+01:00\" level=info msg=\"[certificates] Generating kube-etcd-10-0-0-4 certificate and key\"\r\ntime=\"2020-12-11T11:56:45+01:00\" level=info msg=\"Successfully Deployed state file at [C:\\\\Dev\\\\cst\\\\xxxx\\\\xxxx\\\\xxxx\\\\xxxx-rancher\\\\quickstart\\\\azure\\\\terraform-provider-rke-tmp-169182102/cluster.rkestate]\"\r\ntime=\"2020-12-11T11:56:45+01:00\" level=info msg=\"Building Kubernetes cluster\"\r\ntime=\"2020-12-11T11:56:45+01:00\" level=info msg=\"[dialer] Setup tunnel for host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:56:46+01:00\" level=info msg=\"[network] Deploying port listener containers\"\r\ntime=\"2020-12-11T11:56:46+01:00\" level=info msg=\"Image [rancher/rke-tools:v0.1.66] exists on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:56:49+01:00\" level=info msg=\"Starting container [rke-etcd-port-listener] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:56:50+01:00\" level=info msg=\"[network] Successfully started [rke-etcd-port-listener] container on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:56:50+01:00\" level=info msg=\"Image [rancher/rke-tools:v0.1.66] exists on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:56:53+01:00\" level=info msg=\"Starting container [rke-cp-port-listener] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:56:53+01:00\" level=info msg=\"[network] Successfully started [rke-cp-port-listener] container on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:56:53+01:00\" level=info msg=\"Image [rancher/rke-tools:v0.1.66] exists on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:56:56+01:00\" level=info msg=\"Starting container [rke-worker-port-listener] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:56:57+01:00\" level=info msg=\"[network] Successfully started [rke-worker-port-listener] container on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:56:57+01:00\" level=info msg=\"[network] Port listener containers deployed successfully\"\r\ntime=\"2020-12-11T11:56:57+01:00\" level=info msg=\"[network] Running control plane -> etcd port checks\"\r\ntime=\"2020-12-11T11:56:57+01:00\" level=info msg=\"Image [rancher/rke-tools:v0.1.66] exists on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:57:00+01:00\" level=info msg=\"Starting container [rke-port-checker] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:57:01+01:00\" level=info msg=\"[network] Successfully started [rke-port-checker] container on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:57:01+01:00\" level=info msg=\"Removing container [rke-port-checker] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:57:01+01:00\" level=info msg=\"[network] Running control plane -> worker port checks\"\r\ntime=\"2020-12-11T11:57:01+01:00\" level=info msg=\"Image [rancher/rke-tools:v0.1.66] exists on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:57:04+01:00\" level=info msg=\"Starting container [rke-port-checker] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:57:04+01:00\" level=info msg=\"[network] Successfully started [rke-port-checker] container on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:57:04+01:00\" level=info msg=\"Removing container [rke-port-checker] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:57:05+01:00\" level=info msg=\"[network] Running workers -> control plane port checks\"\r\ntime=\"2020-12-11T11:57:05+01:00\" level=info msg=\"Image [rancher/rke-tools:v0.1.66] exists on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:57:08+01:00\" level=info msg=\"Starting container [rke-port-checker] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:57:08+01:00\" level=info msg=\"[network] Successfully started [rke-port-checker] container on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:57:08+01:00\" level=info msg=\"Removing container [rke-port-checker] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:57:08+01:00\" level=info msg=\"[network] Checking KubeAPI port Control Plane hosts\"\r\ntime=\"2020-12-11T11:57:08+01:00\" level=info msg=\"[network] Removing port listener containers\"\r\ntime=\"2020-12-11T11:57:08+01:00\" level=info msg=\"Removing container [rke-etcd-port-listener] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:57:09+01:00\" level=info msg=\"[remove/rke-etcd-port-listener] Successfully removed container on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:57:09+01:00\" level=info msg=\"Removing container [rke-cp-port-listener] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:57:09+01:00\" level=info msg=\"[remove/rke-cp-port-listener] Successfully removed container on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:57:09+01:00\" level=info msg=\"Removing container [rke-worker-port-listener] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:57:10+01:00\" level=info msg=\"[remove/rke-worker-port-listener] Successfully removed container on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:57:10+01:00\" level=info msg=\"[network] Port listener containers removed successfully\"\r\ntime=\"2020-12-11T11:57:10+01:00\" level=info msg=\"[certificates] Deploying kubernetes certificates to Cluster nodes\"\r\ntime=\"2020-12-11T11:57:10+01:00\" level=info msg=\"Checking if container [cert-deployer] is running on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:57:10+01:00\" level=info msg=\"Image [rancher/rke-tools:v0.1.66] exists on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:57:13+01:00\" level=info msg=\"Starting container [cert-deployer] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:57:14+01:00\" level=info msg=\"Checking if container [cert-deployer] is running on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:57:19+01:00\" level=info msg=\"Checking if container [cert-deployer] is running on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:57:19+01:00\" level=info msg=\"Removing container [cert-deployer] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:57:19+01:00\" level=info msg=\"[reconcile] Rebuilding and updating local kube config\"\r\ntime=\"2020-12-11T11:57:19+01:00\" level=info msg=\"Successfully Deployed local admin kubeconfig at [C:\\\\Dev\\\\cst\\\\xxxx\\\\xxxx\\\\xxxx\\\\xxxx-rancher\\\\quickstart\\\\azure\\\\terraform-provider-rke-tmp-169182102/kube_config_cluster.yml]\"\r\ntime=\"2020-12-11T11:57:21+01:00\" level=info msg=\"[certificates] Successfully deployed kubernetes certificates to Cluster nodes\"\r\ntime=\"2020-12-11T11:57:21+01:00\" level=info msg=\"[file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [20.52.135.101]\"\r\ntime=\"2020-12-11T11:57:21+01:00\" level=info msg=\"Image [rancher/rke-tools:v0.1.66] exists on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:57:24+01:00\" level=info msg=\"Starting container [file-deployer] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:57:25+01:00\" level=info msg=\"Successfully started [file-deployer] container on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:57:25+01:00\" level=info msg=\"Waiting for [file-deployer] container to exit on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:57:25+01:00\" level=info msg=\"Waiting for [file-deployer] container to exit on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:57:25+01:00\" level=info msg=\"Removing container [file-deployer] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T11:57:25+01:00\" level=info msg=\"[remove/file-deployer] Successfully removed container on host [20.52.135.101]\"\r\ntime=\"2020-12-11T11:57:25+01:00\" level=info msg=\"[/etc/kubernetes/audit-policy.yaml] Successfully deployed audit policy file to Cluster control nodes\"\r\ntime=\"2020-12-11T11:57:25+01:00\" level=info msg=\"[reconcile] Reconciling cluster state\"\r\ntime=\"2020-12-11T11:57:25+01:00\" level=info msg=\"[reconcile] This is newly generated cluster\"\r\ntime=\"2020-12-11T11:57:25+01:00\" level=info msg=\"Pre-pulling kubernetes images\"\r\ntime=\"2020-12-11T11:57:25+01:00\" level=info msg=\"Pulling image [rancher/hyperkube:v1.19.3-rancher1] on host [20.52.135.101], try #1\"\r\ntime=\"2020-12-11T12:00:19+01:00\" level=info msg=\"Image [rancher/hyperkube:v1.19.3-rancher1] exists on host [20.52.135.101]\"\r\ntime=\"2020-12-11T12:00:19+01:00\" level=info msg=\"Kubernetes images pulled successfully\"\r\ntime=\"2020-12-11T12:00:19+01:00\" level=info msg=\"[etcd] Building up etcd plane..\"\r\ntime=\"2020-12-11T12:00:19+01:00\" level=info msg=\"Image [rancher/rke-tools:v0.1.66] exists on host [20.52.135.101]\"\r\ntime=\"2020-12-11T12:01:09+01:00\" level=warning msg=\"Failed to create Docker container [etcd-fix-perm] on host [20.52.135.101]: Cannot connect to the Docker daemon at npipe:////./pipe/docker_engine. Is the docker daemon running?\"\r\ntime=\"2020-12-11T12:01:10+01:00\" level=warning msg=\"Failed to create Docker container [etcd-fix-perm] on host [20.52.135.101]: Error response from daemon: Conflict. The container name \\\"/etcd-fix-perm\\\" is already in use by container \\\"6b2133859f518f3912419a19092c643ea4208d7b9c1ac953e7bcdec287c50d9f\\\". You have to remove (or rename) that container to be able to reuse that name.\"\r\ntime=\"2020-12-11T12:01:10+01:00\" level=warning msg=\"Failed to create Docker container [etcd-fix-perm] on host [20.52.135.101]: Error response from daemon: Conflict. The container name \\\"/etcd-fix-perm\\\" is already in use by container \\\"6b2133859f518f3912419a19092c643ea4208d7b9c1ac953e7bcdec287c50d9f\\\". You have to remove (or rename) that container to be able to reuse that name.\"\r\n\r\nFailed running cluster err:[etcd] Failed to bring up Etcd Plane: Failed to create [etcd-fix-perm] container on host [20.52.135.101]: Failed to create Docker container [etcd-fix-perm] on host [20.52.135.101]: Error response from daemon: Conflict. The container name \"/etcd-fix-perm\" is already in use by container \"6b2133859f518f3912419a19092c643ea4208d7b9c1ac953e7bcdec287c50d9f\". You have to remove (or rename) that container to be able to reuse that name.\r\n========================================\r\n\r\n\r\n  on ..\\rancher-common\\rke.tf line 4, in resource \"rke_cluster\" \"rancher_cluster\":\r\n   4: resource \"rke_cluster\" \"rancher_cluster\" {\r\n```\r\n\r\nHow can this be solved?",
      "comments": [
        "Thanks to the people in the Slack, we could figure out that the `Standard_LRS` storage on the VMs is an issue, because they are too slow. When I changed the `infra.tf` file to use `Premium_LRS` storage disks for the VMs, the system came up in one go.\r\n\r\nSo I'd like to suggest to change these values in the template by default, so that others don't run into that issue. And yes, its a bit more expensive for the person trying it out on Azure, but if you want to try out rancher you really want that to work on the first go, and not troubleshoot such a thing.\r\n",
        "Can confirm that swapping to Premium LRS fixed the issue. "
      ]
    },
    "codes": [
      "awareness",
      "provider"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/oracle-terraform-modules/terraform-oci-oke/issues/180",
    "content": {
      "title": "Allow configurable minimum for worker node pools",
      "body": "<!--- Please keep this note for the community --->\r\n\r\n### Community Note\r\n\r\n* Please vote on this issue by adding a \ud83d\udc4d [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to the original issue to help the community and maintainers prioritize this request\r\n* Please do not leave \"+1\" or \"me too\" comments, they generate extra noise for issue followers and do not help prioritize the request\r\n* If you are interested in working on this issue or have submitted a pull request, please leave a comment\r\n\r\n<!--- Thank you for keeping this note for the community --->\r\n\r\n### Description\r\n\r\n<!--- Please leave a helpful description of the feature request here. --->\r\nCurrently, a minimum of 3 worker nodes per node pool is created. This minimum value is hard coded. Some customers would like to have smaller node pools, especially for non-production environment to minimize costs.\r\n\r\n### New or Affected Resource(s)\r\n\r\n<!--- Please list any new or affected resources and data sources that are part of the request. For example: \"oci_core_vcn\" --->\r\n\r\n\r\n### Potential Terraform Configuration\r\n\r\n<!--- Information about code formatting: https://help.github.com/articles/basic-writing-and-formatting-syntax/#quoting-code --->\r\n\r\n```hcl\r\n# Copy-paste any Terraform configurations for how the requested feature may be used. \r\n```\r\n\r\n### References\r\n\r\n<!---\r\nInformation about referencing Github Issues: https://help.github.com/articles/basic-writing-and-formatting-syntax/#referencing-issues-and-pull-requests\r\n\r\nAre there any other GitHub issues (open or closed) or pull requests that should be linked here? Vendor blog posts or documentation? For example:\r\n\r\n--->\r\n",
      "comments": [
        "Closed in #184 "
      ]
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/christophetd/Adaz/issues/35",
    "content": {
      "title": "VPN instead of inbound rules",
      "body": "how about VPN in the ES box for example to join the network instead of the NSG rules which might affect the flexibility if you keep connecting from different network.",
      "comments": [
        "I agree having an OpenVPN server / Azure VPN Gateway would be nice, but on the other hand it would add non-negligible complexity to the overall setup, so I'd rather not add that for now.\r\n\r\nAnother option would be to add a boolean setting to:\r\n- Not expose the Windows machines to the Internet\r\n- Use the ES box as a bastion open to anyone on the Internet, to be able to do SSH tunnelling through it",
        "Closing with the rationale it would add too much cost / complexity / time to spin up the lab."
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/codequest-eu/terraform-modules/issues/42",
    "content": {
      "title": "ecs/host_group: add cpu_credits variable",
      "body": "EC2 T3 and T3a instances are started in unlimited bursting mode by default, would be nice to be able to switch it off to ensure costs are predictable.\r\n\r\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances.html\r\nhttps://www.terraform.io/docs/providers/aws/r/instance.html#cpu_credits",
      "comments": [
        "Looks like [aws_launch_configuration](https://www.terraform.io/docs/providers/aws/r/launch_configuration.html) doesn't support `cpu_credits` and we'd have to migrate to [aws_launch_template](https://www.terraform.io/docs/providers/aws/r/launch_template.html) as suggested by https://github.com/terraform-providers/terraform-provider-aws/issues/2491#issuecomment-386296092"
      ]
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/codequest-eu/terraform-modules/issues/126",
    "content": {
      "title": "ecs/host_group: support spot instances",
      "body": "We should figure out a way to support using spot for ECS instances to limit costs.",
      "comments": []
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/codequest-eu/terraform-modules/issues/128",
    "content": {
      "title": "cloudfront: use CloudFront functions instead of Lambda",
      "body": "AWS introduced CloudFront functions which can replace viewer request/response Lambdas. They are simpler and cheaper, but with stricter time constraints. We should use them for basic auth for example.\r\n\r\nhttps://aws.amazon.com/blogs/aws/introducing-cloudfront-functions-run-your-code-at-the-edge-with-low-latency-at-any-scale/\r\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-functions.html",
      "comments": []
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/issues/20",
    "content": {
      "title": "Investigate Connect Gateway as an alternative for VPN based approach",
      "body": "Investigate https://cloud.google.com/anthos/multicluster-management/gateway/tutorials/cloud-build-integration as the default which maybe simpler and cost effective than https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/tree/main/modules/workerpool-gke-ha-vpn",
      "comments": [
        "Unfortunately Connect Gateway currently requires (a) Anthos licensing and (b) the hub membership must be created via gcloud. I'm investigating in b/227658842."
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/cds-snc/covid-alert-metrics-terraform/issues/115",
    "content": {
      "title": "Remove Elastic File Storage",
      "body": "It's no longer needed so we shouldn't pay for it. ",
      "comments": []
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/cds-snc/covid-alert-metrics-terraform/issues/238",
    "content": {
      "title": "CloudWatch chart: Request sizes for metrics payloads",
      "body": "To help monitor payload size growth, a CloudWatch chart should be created that tracks payload sizes for incoming requests.\r\n\r\nRelated: incident-en-4xx-high",
      "comments": [
        "I couldn't find a provided payload size metric from AWS, which means I think we'd have to do this with a custom metric filter.  Using the [CloudWatch calculator](https://calculator.aws/#/createCalculator/CloudWatch), just for API PutMetricData requests for our metric filter, it would be pricey!\r\n\r\n```sh\r\n# API gateway traffic for last 30 days\r\n331,350,445 requests x 0.00001 USD = 3,313.50445 USD\r\nCloudWatch API requests cost (monthly): 3,313.50 USD\r\n```\r\n\r\nI'll confirm with our AWS account manager.\r\n",
        "Request tracked in support case [9044902791](https://console.aws.amazon.com/support/home#/case/?displayId=9044902791).",
        "Confirmed that this would be ~3,300 USD per month, so closing as it's cost prohibitive."
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Linaro/qa-reports.linaro.org/issues/53",
    "content": {
      "title": "Make scheduler a cronjob",
      "body": "I think we can make scheduler pod into a k8s cronjob and save costs a bit by having a pod sitting idle most of time.\r\n\r\nhttps://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/",
      "comments": []
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Linaro/qa-reports.linaro.org/issues/54",
    "content": {
      "title": "Configure workers to autoscale down to 0",
      "body": "I was reading thru this issue (https://github.com/kubernetes/kubernetes/issues/69687) I noticed our workers sit idle when there aren't tasks in queue. We could save costs by setting minimum number of pods to 0 (zero) when no task is present in queue and fire up workers depending on the amount of tasks.\r\n\r\nThe PR (https://github.com/kubernetes/kubernetes/pull/74526) seems to be in alpha but not yet available in stable K8s nor in EKS, but it'll be nice to keep an eye on this feature when it becomes available to save costs running qa-reports",
      "comments": [
        "To attempt next week https://itnext.io/horizontal-pod-autoscale-with-custom-metrics-8cb13e9d475"
      ]
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/datarevenue-berlin/OpenMLOps/issues/70",
    "content": {
      "title": "Error creating S3 bucket: BucketAlreadyOwnedByYou",
      "body": "When I try to follow the instruction at https://github.com/datarevenue-berlin/OpenMLOps/blob/master/tutorials/set-up-open-source-production-mlops-architecture-aws.md\r\n\r\nI got to the step running: terraform apply -var-file=my_vars.tfvars\r\n\r\nBut I got: Error creating S3 bucket: BucketAlreadyOwnedByYou: Your previous request to create the named bucket succeeded and you already own it.\r\n\u2502 \r\n\u2502   with aws_s3_bucket.mlflow_artifact_root,\r\n\u2502   on main.tf line 11, in resource \"aws_s3_bucket\" \"mlflow_artifact_root\":\r\n\u2502   11: resource \"aws_s3_bucket\" \"mlflow_artifact_root\"\r\n\r\nIf I delete the S3 bucket and rerun the command, I got: Error loading state: S3 bucket does not exist.\r\n\r\nSo maybe it was using that bucket, but then later try to create the bucket again?",
      "comments": [
        "The first one is expected if you already have created that bucket. The second one sounds like it is trying to load the Terraform state from the S3 bucket that you have now deleted.\r\n\r\nIt's useful to save Terraform state in S3 if you are working in a team environment, but it's simpler to just save terraform state locally. You can toggle it in https://github.com/datarevenue-berlin/OpenMLOps/blob/master/terraform_backend.tf and see some general information there too.",
        "In the output, it says:\r\nWarning: Backend configuration ignored\r\n\u2502 \r\n\u2502   on ../OpenMLOps/terraform_backend.tf line 17, in terraform:\r\n\u2502   17:   backend \"s3\" {\r\n\u2502 \r\n\u2502 Any selected backend applies to the entire configuration, so Terraform expects provider configurations only in the root module.\r\n\r\nSo I think it does not use the config in terraform backend.",
        " @user please move your backend file to your `OpenMLOps-AWS` repo and do, from there, a `terraform init --backend-config=<yourbackendfile> --reconfigure` then try again applying.",
        "I got error:\r\n\u2502 Error: Duplicate backend configuration\r\n\u2502 \r\n\u2502   on terraform_backend.tf line 22, in terraform:\r\n\u2502   22:   backend \"local\" {}\r\n\u2502 \r\n\u2502 A module may have only one backend configuration. The backend was previously configured at main.tf:2,3-15.\r\n\r\nWhen I comment out the terraform block in main.tf, then I got another error:\r\n\u2502 Error: Unsupported block type\r\n\u2502 \r\n\u2502   on terraform_backend.tf line 16:\r\n\u2502   16: terraform {\r\n\u2502 \r\n\u2502 Blocks of type \"terraform\" are not expected here.",
        "Please leave the terraform block in main.tf. The backend file actually is only required if you are going to use S3, and its not what is inside the `terraform_backend.tf` file ( I noticed how confusing it is right now, so I will request we remove that file). If you wish to use terraform with state saving on local, you can ignore that file, otherwise you need to fill it (or create some txt with) like this:\r\n```\r\nbucket = \"aws-bucket-name\"\r\nkey  = \"filename-in-s3\"\r\nregion = \"awsregion\"\r\n```",
        "You can check more info here: https://www.terraform.io/docs/language/settings/backends/configuration.html\r\n\r\nThe extra file might not even be necessary, you could set those keys directly into `main.tf` (or just set it to local there) and it should work.",
        "Hi, I got to the end of the tutorial. But the link https://mlops.pixtavietnam.com/ return 404. While the link https://jupyter.mlops.pixtavietnam.com/ return 401. How do I get the auth link https://mlops.pixtavietnam.com/auth/profile/registration to work? Thanks",
        "So, your \"homepage\" won't work unless you have routing rules set for it that lead to a service returning a webpage.\r\nNow, the 401 you are getting in the second URL is normal, since you are not logged in yet.\r\nThe third I think is wrong, try `https://mlops.pixtavietnam.com/profile/auth/registration`. If it doesn't work, then you will need to get a log from your oathkeeper pod, to see if the request is hitting",
        "Thank you, I got it almost 100% working. But I got `prefect.exceptions.ClientError: Your Prefect Server instance has no tenants. Create a tenant with `prefect server create-tenant` for pod prefect-server-agent. I think in my previous run, this pod ran fine. How do I fix this one?",
        "Hmm, checkout this issue, there is some relevant info at the very bottom of the discussion:\r\nhttps://github.com/datarevenue-berlin/OpenMLOps/issues/67\r\n\r\nI hope it helps, but if it doesn't, please tell us :) ",
        "It seems to have fixed itself overnight :)\r\nThank you a lot for your help!\r\n\r\nJust one more question: If I don't need to use the cluster immediately, should I 'terraform destroy' to save money and create cluster again later. Or what can I do to save money and resources? Like temporarily stop pods/nodes or something?",
        "I would say the safest is to destroy, but you can try scaling everything to 0 and then monitor your costs in AWS console to see if they are going up. Would be nice to know if that's an option but I believe they will still charge you :x"
      ]
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/datarevenue-berlin/OpenMLOps/issues/98",
    "content": {
      "title": "Approximate monthly AWS costs ",
      "body": "I know there's a ton of conditional nuance to even asking how much the AWS resources will cost, but I'm asking in the spirit of a baseline for people who've wondered the same. Assuming basic weekday usage by a team of 2 data scientists (or perhaps based on DataRevenue's own consumption load) how much would the services provisioned cost monthly?",
      "comments": [
        "Hello! Yes, there is a ton of nuance, your costs could go from 40$ all the way to 90$+ monthly on a 'standard' OML deployment with moderate usage. If that sounds like too much, maybe try deploying it on minikube instead"
      ]
    },
    "codes": [
      "awareness",
      "provider"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/mdn/infra/issues/38",
    "content": {
      "title": "Setup an elastic.co account",
      "body": "This is a hosted elastic search service thats needed. Setup an an account [here](https://www.elastic.co/) and get a corp card placed in on that account. Budget is around $320/month $271/month with an annual contract. ",
      "comments": [
        " @user I think currently, for MDN only, we're paying around $145/month for the production instance and about $15/month for the stage instance, so I would say the monthly cost would not be more than $200/month.",
        "> @user I think currently, for MDN only, we're paying around $145/month for the production instance and about $15/month for the stage instance, so I would say the monthly cost would not be more than $200/month.\r\n\r\nThanks @user  I'll get his information to our vendor folks",
        "Account has been setup"
      ]
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/mdn/infra/issues/27",
    "content": {
      "title": "Setup Prod (Read-Only PoC) instance of MDN",
      "body": "Once stage is up and running setup the prod instance as a read-only, proof-of concept deployment.\r\n\r\n# Acceptance Criteria\r\n* [x] Read-only snapshot of production available at new MozIT cloud domain\r\n* [x] Automated tests / checklist run against new instance\r\n* [x] Follow-on tasks for go-live are captured in new issues\r\n\r\n# Tasks:\r\n* [x] Decide if we start with production-sized services (do it once, start paying) or stage-size services (pay less for a month or so) -- 9/10/2018 (@escattone) starting with stage-size services\r\n* [x] Combine [oregon/stage.sh](https://github.com/mdn/infra/blob/master/apps/mdn/mdn-aws/k8s/regions/oregon/stage.sh) and [portland/prod.mm.sh](https://github.com/mdn/infra/blob/master/apps/mdn/mdn-aws/k8s/regions/portland/prod.mm.sh) into `oregon/prod.mm.sh` (production read-only) and `oregon/prod.mm.min.sh` (production read-only but with stage-size replica counts)\r\n* [x] Provision prod CloudFront for the primary, attachments, and interactive-examples CDN's\r\n* [x] Provision prod database (production size)\r\n* [x] Provision prod redis (production size)\r\n* [x] Provision prod memcache (production size)\r\n* [x] Provision prod elasticsearch (production size)\r\n* [x] Download encrypted, compressed dump of the production RDS instance in MozMEAO\r\n* [x] Decrypt and load the compressed DB dump into the new prod RDS instance in IT\r\n* (Optional) Anonymize the new prod RDS instance in IT\r\n* [x] Copy EFS backup (attachments) from prod S3 bucket into new S3 bucket\r\n* [x] Make the K8s namespace, shared-storage, and services\r\n* [x] Add the access URL's of the backing services (RDS, Redis) to `mdn-secrets.prod`\r\n* [x] Deploy the `mdn-backup-secrets`, `mdn-newrelic-secrets`, `mdn-speedcurve-secrets.yaml`, and `mdn-secrets.prod`\r\n* [x] Deploy the S3 EFS sync (`make k8s-sync-from-s3-cron`) to pull (hourly) the EFS assets from the IT-owned S3 bucket into the IT-owned EFS volume\r\n* [x] Create DNS CNAME for `prod.mdn.mozit.cloud` that points to the new ELB for the `web` service created above\r\n* [x] Create DNS CNAME for `developer-prod.mdn.mozit.cloud` that points to the new prod primary CDN\r\n* [x] Create DNS CNAME for `demos.mdn.mozit.cloud` that points to the new prod attachments CDN\r\n* [x] Create DNS CNAME for `interactive-examples.mdn.mozit.cloud` that points to the new interactive-examples CDN\r\n* [x] Create, validate, and attach new certificates for the new `web` ELB, the prod primary CDN, the attachments CDN, and the interactive-examples CDN (see comment below)\r\n* [x] Make the K8s deployments\r\n* [x] Login to the MDN admin console and change the Site to `developer-prod.mdn.mozit.cloud`\r\n* [x] Create, populate, and promote new search index via the admin console\r\n* [x] Make the `humans` and `sitemaps` (run `./manage.py make_humans` and `./manage.py make_sitemaps` on the web pod)\r\n* [x] Perform [checklist](https://github.com/mdn/infra/blob/master/apps/mdn/mdn-aws/docs/stage-checklist.md)",
      "comments": [
        "https://github.com/mozilla/mdn-k8s-private/pull/65\r\nhttps://github.com/mdn/infra/pull/62",
        "Yesterday, I created and attached 4 new AWS ACM certificates, one for each of the following:\r\n- Primary Prod CDN -- with domain names:\r\n    - developer.mozilla.org\r\n    - developer.mozilla.com\r\n    - mdn.mozilla.org\r\n    - developer-new.mozilla.org\r\n    - developers.mozilla.org\r\n    - developer-prod.mdn.mozit.cloud\r\n- Attachments CDN -- with domain names:\r\n    - mdn.mozillademos.org\r\n    - demos.mdn.mozit.cloud\r\n- Interactive-Examples CDN -- with domain names:\r\n    - interactive-examples.mdn.mozilla.net\r\n    - interactive-examples.mdn.mozit.cloud\r\n- Web ELB -- with domain names:\r\n    - developer.mozilla.org\r\n    - developer.mozilla.com\r\n    - mdn.mozilla.org\r\n    - developer-new.mozilla.org\r\n    - developers.mozilla.org\r\n    - developer-prod.mdn.mozit.cloud\r\n    - prod.mdn.mozit.cloud\r\n    - mdn.mozillademos.org\r\n    - demos.mdn.mozit.cloud\r\n    - cdn.mdn.mozilla.net\r\n\r\nI removed `devmo.developer.mozilla.org` from the certs for the primary prod CDN and the web ELB, since it has not been in our allowed hosts for at least the past year and doesn't resolve anyway.\r\n\r\nSee https://github.com/mdn/infra/pull/66",
        "The [new prod PoC instance](https://developer-prod.mdn.mozit.cloud) is running in **maintenance mode** and available for testing. It is using a dump of the prod DB from two days ago.",
        "## Home page\r\n\r\nLoad https://developer-prod.mdn.mozit.cloud/en-US/\r\n\r\n* [x] Loads without errors\r\n* [x] Does have banner \"MDN is currently in read-only maintenance mode. Learn more.\"\r\n* [x] Does not have \"Sign in\" or \"View Profile\" at the top\r\n* [x] Has entries for the Hacks Blog\r\n\r\n## Article page\r\n\r\nLoad https://developer-prod.mdn.mozit.cloud/en-US/docs/Web/HTML\r\n\r\n* [x] Loads without errors\r\n* [x] Has Maintenance Mode banner\r\n* [x] Does not have \"Sign in\" or \"View Profile\" at the top",
        "# Functional Tests\r\n\r\nWith a Kuma environment and Docker, [run the functional tests](https://kuma.readthedocs.io/en/latest/tests-ui.html#run-tests-locally-using-selenium-docker-images):\r\n\r\n```\r\nscripts/run_functional_tests.sh --maintenance-mode --base-url https://developer-prod.mdn.mozit.cloud -m \"not login\" tests/functional\r\n```\r\n\r\nThis will run a selection of safe Selenium tests (no login, no page edits) with\r\nChrome and Firefox, running in Docker images. A failing test is automatically\r\nre-run before calling it a failure. Test results are created, with screenshots,\r\ntracebacks, and logs for failing tests, including xfail and rerun tests.\r\n\r\n* [x] Functional tests pass\r\n\r\nDetails:\r\n```\r\n(mdntest) rjohnson-25186:kuma rjohnson$ scripts/run_functional_tests.sh --maintenance-mode --base-url https://developer-prod.mdn.mozit.cloud -m \"not login\" tests/functional\r\n*** Building integration tests image...\r\nSending build context to Docker daemon  179.3MB\r\nStep 1/10 : FROM python:2.7-slim\r\n2.7-slim: Pulling from library/python\r\nDigest: sha256:0a43a6d7858af4a42427c792b682936d2cd34e183fb026627f53ddb556d4bf62\r\nStatus: Image is up to date for python:2.7-slim\r\n ---> c9cde4658340\r\nStep 2/10 : WORKDIR /app\r\n ---> Using cache\r\n ---> fda1425b260f\r\nStep 3/10 : RUN set -ex &&     apt-get update &&     apt-get install -y --no-install-recommends         mime-support         build-essential         libxml2-dev         libxslt1.1         libxslt1-dev         zlib1g-dev     && rm -rf /var/lib/apt/lists/*\r\n ---> Using cache\r\n ---> c7cd96772567\r\nStep 4/10 : ENV PYTEST_PROCESSES 5\r\n ---> Using cache\r\n ---> 6a8ffee2f698\r\nStep 5/10 : ENV PRIVACY \"public restricted\"\r\n ---> Using cache\r\n ---> 32ac9e9d4741\r\nStep 6/10 : ENV TESTS_PATH /app/tests\r\n ---> Using cache\r\n ---> 43a2778303ec\r\nStep 7/10 : ENV RESULTS_PATH /app/results\r\n ---> Using cache\r\n ---> 0c3166366abb\r\nStep 8/10 : COPY ./requirements /app/requirements\r\n ---> Using cache\r\n ---> 4d3ee7eb9284\r\nStep 9/10 : RUN pip install --no-cache-dir -r requirements/test.txt\r\n ---> Using cache\r\n ---> 29fc609364e0\r\nStep 10/10 : COPY tests /app/tests\r\n ---> 3976ba7b1070\r\nSuccessfully built 3976ba7b1070\r\nSuccessfully tagged kuma-integration-tests:latest\r\n*** Running dockerized chrome...\r\ndd589bea217129e241bd100ca176e699f38feabe3fc924c7ef2edfa36de22977\r\n*** Running dockerized firefox...\r\n30eceeb4e436298924c64cfe882a901387a3f849fd3b7fdefe0743ea071e358e\r\n*** Running integration tests against chrome...\r\n=============================================== test session starts ===============================================\r\nplatform linux2 -- Python 2.7.15, pytest-3.1.3, py-1.4.33, pluggy-0.4.0\r\ndriver: Remote\r\nsensitiveurl: .* *** WARNING: sensitive url matches https://developer-prod.mdn.mozit.cloud ***\r\nmetadata: {'Python': '2.7.15', 'Driver': 'Remote', 'Capabilities': {'browserName': 'chrome'}, 'Server': 'hub:4444', 'Base URL': 'https://developer-prod.mdn.mozit.cloud', 'Platform': 'Linux-4.9.93-linuxkit-aufs-x86_64-with-debian-9.5', 'kuma': {u'services': {u'search': {u'available': True, u'count': 60165, u'populated': True}, u'kumascript': {u'available': True, u'revision': u'e57e27c371416c7e104f7843318e9166dbe3c1f3'}, u'test_accounts': {u'available': False}, u'database': {u'available': True, u'document_count': 130018, u'populated': True}}, u'version': 1, u'request': {u'url': u'https://developer-prod.mdn.mozit.cloud/_kuma_status.json', u'is_secure': True, u'host': u'developer-prod.mdn.mozit.cloud', u'scheme': u'https'}, 'response': {'headers': {'X-Cache': 'Miss from cloudfront', 'strict-transport-security': 'max-age=63072000', 'x-content-type-options': 'nosniff', 'Content-Language': 'en-US', 'Content-Encoding': 'gzip', 'Transfer-Encoding': 'chunked', 'Expires': 'Wed, 12 Sep 2018 22:01:35 GMT', 'Vary': 'Accept-Encoding', 'Server': 'meinheld/0.6.1', 'Connection': 'keep-alive', 'x-xss-protection': '1; mode=block', 'X-Amz-Cf-Id': 'n5WOJjNVEf5pXffVAELwdVxd2l52uo3MPjd1dioWUkwXgCXFM7MYOg==', 'Cache-Control': 'no-cache, no-store, must-revalidate, max-age=0', 'Date': 'Wed, 12 Sep 2018 22:01:35 GMT', 'X-Frame-Options': 'DENY', 'Content-Type': 'application/json', 'Via': '1.1 27fdab293d6dff03ec20e408968c606d.cloudfront.net (CloudFront)'}}, u'settings': {u'ATTACHMENT_HOST': u'demos.mdn.mozit.cloud', u'PROTOCOL': u'https://', u'INTERACTIVE_EXAMPLES_BASE': u'https://interactive-examples.mdn.mozit.cloud', u'MAINTENANCE_MODE': True, u'STATIC_URL': u'https://developer-prod.mdn.mozit.cloud/static/', u'SITE_URL': u'https://developer-prod.mdn.mozit.cloud', u'ATTACHMENT_ORIGIN': u'prod.mdn.mozit.cloud', u'DEBUG': False, u'ALLOWED_HOSTS': [u'developer.mozilla.org', u'cdn.mdn.mozilla.net', u'mdn.mozillademos.org', u'demos.mdn.mozit.cloud', u'developer-prod.mdn.mozit.cloud', u'prod.mdn.mozit.cloud', u'developer.mozilla.com', u'mdn.mozilla.org', u'developer-new.mozilla.org', u'developers.mozilla.org'], u'REVISION_HASH': u'0591dd661bfa5e34f19a74428dff5cc8a172998c', u'LEGACY_HOSTS': [u'cdn.mdn.mozilla.net', u'developer.mozilla.com', u'mdn.mozilla.org', u'developer-new.mozilla.org', u'developers.mozilla.org']}}, 'Plugins': {'variables': '1.7.1', 'selenium': '1.11.4', 'rerunfailures': '2.1.0', 'html': '1.16.1', 'base-url': '1.4.1', 'metadata': '1.5.1'}, 'Packages': {'py': '1.4.33', 'pytest': '3.1.3', 'pluggy': '0.4.0'}}\r\nbaseurl: https://developer-prod.mdn.mozit.cloud\r\nrootdir: /app, inifile:\r\nplugins: selenium-1.11.4, html-1.16.1, rerunfailures-2.1.0, variables-1.7.1, metadata-1.5.1, base-url-1.4.1\r\ncollected 119 items\r\n\r\ntests/functional/test_article.py ..s..s...\r\ntests/functional/test_article_edit.py s\r\ntests/functional/test_article_revision.py ..\r\ntests/functional/test_content_experiment.py ss\r\ntests/functional/test_dashboard.py ...x......\r\ntests/functional/test_feedback.py ...\r\ntests/functional/test_home.py .....s..\r\ntests/functional/test_language_selector.py .\r\ntests/functional/test_maintenance_mode_redirects.py ...............................................................\r\ntests/functional/test_notfound.py ...\r\ntests/functional/test_profiles.py .\r\ntests/functional/test_report.py ..\r\ntests/functional/test_search.py .......\r\n\r\n------------------------------------ generated html file: /results/pytest.html ------------------------------------\r\n=============================================== 7 tests deselected ================================================\r\n======================== 105 passed, 6 skipped, 7 deselected, 1 xfailed in 198.31 seconds =========================\r\n*** Running integration tests against firefox...\r\n=============================================== test session starts ===============================================\r\nplatform linux2 -- Python 2.7.15, pytest-3.1.3, py-1.4.33, pluggy-0.4.0\r\ndriver: Remote\r\nsensitiveurl: .* *** WARNING: sensitive url matches https://developer-prod.mdn.mozit.cloud ***\r\nmetadata: {'Python': '2.7.15', 'Driver': 'Remote', 'Capabilities': {'browserName': 'firefox'}, 'Server': 'hub:4444', 'Base URL': 'https://developer-prod.mdn.mozit.cloud', 'Platform': 'Linux-4.9.93-linuxkit-aufs-x86_64-with-debian-9.5', 'kuma': {u'services': {u'search': {u'available': True, u'count': 60165, u'populated': True}, u'kumascript': {u'available': True, u'revision': u'e57e27c371416c7e104f7843318e9166dbe3c1f3'}, u'test_accounts': {u'available': False}, u'database': {u'available': True, u'document_count': 130018, u'populated': True}}, u'version': 1, u'request': {u'url': u'https://developer-prod.mdn.mozit.cloud/_kuma_status.json', u'is_secure': True, u'host': u'developer-prod.mdn.mozit.cloud', u'scheme': u'https'}, 'response': {'headers': {'X-Cache': 'Miss from cloudfront', 'strict-transport-security': 'max-age=63072000', 'x-content-type-options': 'nosniff', 'Content-Language': 'en-US', 'Content-Encoding': 'gzip', 'Transfer-Encoding': 'chunked', 'Expires': 'Wed, 12 Sep 2018 22:04:55 GMT', 'Vary': 'Accept-Encoding', 'Server': 'meinheld/0.6.1', 'Connection': 'keep-alive', 'x-xss-protection': '1; mode=block', 'X-Amz-Cf-Id': 'T2PwuuXOTg7VCUXiEJMp0AxsK_q1GdQXwijkdCmoU4IYtMlhKYo6pg==', 'Cache-Control': 'no-cache, no-store, must-revalidate, max-age=0', 'Date': 'Wed, 12 Sep 2018 22:04:55 GMT', 'X-Frame-Options': 'DENY', 'Content-Type': 'application/json', 'Via': '1.1 afb9be97319013ab1a18f338fce40f2a.cloudfront.net (CloudFront)'}}, u'settings': {u'ATTACHMENT_HOST': u'demos.mdn.mozit.cloud', u'PROTOCOL': u'https://', u'INTERACTIVE_EXAMPLES_BASE': u'https://interactive-examples.mdn.mozit.cloud', u'MAINTENANCE_MODE': True, u'STATIC_URL': u'https://developer-prod.mdn.mozit.cloud/static/', u'SITE_URL': u'https://developer-prod.mdn.mozit.cloud', u'ATTACHMENT_ORIGIN': u'prod.mdn.mozit.cloud', u'DEBUG': False, u'ALLOWED_HOSTS': [u'developer.mozilla.org', u'cdn.mdn.mozilla.net', u'mdn.mozillademos.org', u'demos.mdn.mozit.cloud', u'developer-prod.mdn.mozit.cloud', u'prod.mdn.mozit.cloud', u'developer.mozilla.com', u'mdn.mozilla.org', u'developer-new.mozilla.org', u'developers.mozilla.org'], u'REVISION_HASH': u'0591dd661bfa5e34f19a74428dff5cc8a172998c', u'LEGACY_HOSTS': [u'cdn.mdn.mozilla.net', u'developer.mozilla.com', u'mdn.mozilla.org', u'developer-new.mozilla.org', u'developers.mozilla.org']}}, 'Plugins': {'variables': '1.7.1', 'selenium': '1.11.4', 'rerunfailures': '2.1.0', 'html': '1.16.1', 'base-url': '1.4.1', 'metadata': '1.5.1'}, 'Packages': {'py': '1.4.33', 'pytest': '3.1.3', 'pluggy': '0.4.0'}}\r\nbaseurl: https://developer-prod.mdn.mozit.cloud\r\nrootdir: /app, inifile:\r\nplugins: selenium-1.11.4, html-1.16.1, rerunfailures-2.1.0, variables-1.7.1, metadata-1.5.1, base-url-1.4.1\r\ncollected 119 items\r\n\r\ntests/functional/test_article.py ..s..s...\r\ntests/functional/test_article_edit.py s\r\ntests/functional/test_article_revision.py ..\r\ntests/functional/test_content_experiment.py ss\r\ntests/functional/test_dashboard.py ..FEX......\r\ntests/functional/test_feedback.py ...\r\ntests/functional/test_home.py .....s..\r\ntests/functional/test_language_selector.py .\r\ntests/functional/test_maintenance_mode_redirects.py ...............................................................\r\ntests/functional/test_notfound.py ...\r\ntests/functional/test_profiles.py .\r\ntests/functional/test_report.py ..\r\ntests/functional/test_search.py F......\r\n\r\n------------------------------------ generated html file: /results/pytest.html ------------------------------------\r\n===================================================== ERRORS ======================================================\r\n________________________________ ERROR at teardown of test_dashboard_load_page_two ________________________________\r\n\r\nrequest = <SubRequest 'driver' for <Function 'test_dashboard_load_page_two'>>\r\ndriver_class = <class 'selenium.webdriver.remote.webdriver.WebDriver'>\r\ndriver_kwargs = {'browser_profile': None, 'command_executor': 'http://hub:4444/wd/hub', 'desired_capabilities': {'browserName': 'firefox'}}\r\n\r\n    @user \n    def driver(request, driver_class, driver_kwargs):\r\n        \"\"\"Returns a WebDriver instance based on options and capabilities\"\"\"\r\n        driver = driver_class(**driver_kwargs)\r\n\r\n        event_listener = request.config.getoption('event_listener')\r\n        if event_listener is not None:\r\n            # Import the specified event listener and wrap the driver instance\r\n            mod_name, class_name = event_listener.rsplit('.', 1)\r\n            mod = __import__(mod_name, fromlist=[class_name])\r\n            event_listener = getattr(mod, class_name)\r\n            if not isinstance(driver, EventFiringWebDriver):\r\n                driver = EventFiringWebDriver(driver, event_listener())\r\n\r\n        request.node._driver = driver\r\n        yield driver\r\n>       driver.quit()\r\n\r\n/usr/local/lib/python2.7/site-packages/pytest_selenium/pytest_selenium.py:143:\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n/usr/local/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py:689: in quit\r\n    self.execute(Command.QUIT)\r\n/usr/local/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py:312: in execute\r\n    self.error_handler.check_response(response)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nself = <selenium.webdriver.remote.errorhandler.ErrorHandler object at 0x7fde430d95d0>\r\nresponse = {'status': 500, 'value': '{\"value\":{\"error\":\"session not created\",\"message\":\"Tried to run command without establishing...s::imp::thread::{{impl}}::new::thread_start\\n                        at /checkout/src/libstd/sys/unix/thread.rs:84\"}}'}\r\n\r\n    def check_response(self, response):\r\n        \"\"\"\r\n            Checks that a JSON response from the WebDriver does not have an error.\r\n\r\n            :Args:\r\n             - response - The JSON response from the WebDriver server as a dictionary\r\n               object.\r\n\r\n            :Raises: If the response contains an error message.\r\n            \"\"\"\r\n        status = response.get('status', None)\r\n        if status is None or status == ErrorCode.SUCCESS:\r\n            return\r\n        value = None\r\n        message = response.get(\"message\", \"\")\r\n        screen = response.get(\"screen\", \"\")\r\n        stacktrace = None\r\n        if isinstance(status, int):\r\n            value_json = response.get('value', None)\r\n            if value_json and isinstance(value_json, basestring):\r\n                import json\r\n                try:\r\n                    value = json.loads(value_json)\r\n                    if len(value.keys()) == 1:\r\n                        value = value['value']\r\n                    status = value.get('error', None)\r\n                    if status is None:\r\n                        status = value[\"status\"]\r\n                        message = value[\"value\"]\r\n                        if not isinstance(message, basestring):\r\n                            value = message\r\n                            message = message.get('message')\r\n                    else:\r\n                        message = value.get('message', None)\r\n                except ValueError:\r\n                    pass\r\n\r\n        exception_class = ErrorInResponseException\r\n        if status in ErrorCode.NO_SUCH_ELEMENT:\r\n            exception_class = NoSuchElementException\r\n        elif status in ErrorCode.NO_SUCH_FRAME:\r\n            exception_class = NoSuchFrameException\r\n        elif status in ErrorCode.NO_SUCH_WINDOW:\r\n            exception_class = NoSuchWindowException\r\n        elif status in ErrorCode.STALE_ELEMENT_REFERENCE:\r\n            exception_class = StaleElementReferenceException\r\n        elif status in ErrorCode.ELEMENT_NOT_VISIBLE:\r\n            exception_class = ElementNotVisibleException\r\n        elif status in ErrorCode.INVALID_ELEMENT_STATE:\r\n            exception_class = InvalidElementStateException\r\n        elif status in ErrorCode.INVALID_SELECTOR \\\r\n                or status in ErrorCode.INVALID_XPATH_SELECTOR \\\r\n                or status in ErrorCode.INVALID_XPATH_SELECTOR_RETURN_TYPER:\r\n            exception_class = InvalidSelectorException\r\n        elif status in ErrorCode.ELEMENT_IS_NOT_SELECTABLE:\r\n            exception_class = ElementNotSelectableException\r\n        elif status in ErrorCode.ELEMENT_NOT_INTERACTABLE:\r\n            exception_class = ElementNotInteractableException\r\n        elif status in ErrorCode.INVALID_COOKIE_DOMAIN:\r\n            exception_class = InvalidCookieDomainException\r\n        elif status in ErrorCode.UNABLE_TO_SET_COOKIE:\r\n            exception_class = UnableToSetCookieException\r\n        elif status in ErrorCode.TIMEOUT:\r\n            exception_class = TimeoutException\r\n        elif status in ErrorCode.SCRIPT_TIMEOUT:\r\n            exception_class = TimeoutException\r\n        elif status in ErrorCode.UNKNOWN_ERROR:\r\n            exception_class = WebDriverException\r\n        elif status in ErrorCode.UNEXPECTED_ALERT_OPEN:\r\n            exception_class = UnexpectedAlertPresentException\r\n        elif status in ErrorCode.NO_ALERT_OPEN:\r\n            exception_class = NoAlertPresentException\r\n        elif status in ErrorCode.IME_NOT_AVAILABLE:\r\n            exception_class = ImeNotAvailableException\r\n        elif status in ErrorCode.IME_ENGINE_ACTIVATION_FAILED:\r\n            exception_class = ImeActivationFailedException\r\n        elif status in ErrorCode.MOVE_TARGET_OUT_OF_BOUNDS:\r\n            exception_class = MoveTargetOutOfBoundsException\r\n        elif status in ErrorCode.JAVASCRIPT_ERROR:\r\n            exception_class = JavascriptException\r\n        elif status in ErrorCode.SESSION_NOT_CREATED:\r\n            exception_class = SessionNotCreatedException\r\n        elif status in ErrorCode.INVALID_ARGUMENT:\r\n            exception_class = InvalidArgumentException\r\n        elif status in ErrorCode.NO_SUCH_COOKIE:\r\n            exception_class = NoSuchCookieException\r\n        elif status in ErrorCode.UNABLE_TO_CAPTURE_SCREEN:\r\n            exception_class = ScreenshotException\r\n        elif status in ErrorCode.ELEMENT_CLICK_INTERCEPTED:\r\n            exception_class = ElementClickInterceptedException\r\n        elif status in ErrorCode.INSECURE_CERTIFICATE:\r\n            exception_class = InsecureCertificateException\r\n        elif status in ErrorCode.INVALID_COORDINATES:\r\n            exception_class = InvalidCoordinatesException\r\n        elif status in ErrorCode.INVALID_SESSION_ID:\r\n            exception_class = InvalidSessionIdException\r\n        elif status in ErrorCode.UNKNOWN_METHOD:\r\n            exception_class = UnknownMethodException\r\n        else:\r\n            exception_class = WebDriverException\r\n        if value == '' or value is None:\r\n            value = response['value']\r\n        if isinstance(value, basestring):\r\n            if exception_class == ErrorInResponseException:\r\n                raise exception_class(response, value)\r\n            raise exception_class(value)\r\n        if message == \"\" and 'message' in value:\r\n            message = value['message']\r\n\r\n        screen = None\r\n        if 'screen' in value:\r\n            screen = value['screen']\r\n\r\n        stacktrace = None\r\n        if 'stackTrace' in value and value['stackTrace']:\r\n            stacktrace = []\r\n            try:\r\n                for frame in value['stackTrace']:\r\n                    line = self._value_or_default(frame, 'lineNumber', '')\r\n                    file = self._value_or_default(frame, 'fileName', '<anonymous>')\r\n                    if line:\r\n                        file = \"%s:%s\" % (file, line)\r\n                    meth = self._value_or_default(frame, 'methodName', '<anonymous>')\r\n                    if 'className' in frame:\r\n                        meth = \"%s.%s\" % (frame['className'], meth)\r\n                    msg = \"    at %s (%s)\"\r\n                    msg = msg % (meth, file)\r\n                    stacktrace.append(msg)\r\n            except TypeError:\r\n                pass\r\n        if exception_class == ErrorInResponseException:\r\n            raise exception_class(response, message)\r\n        elif exception_class == UnexpectedAlertPresentException and 'alert' in value:\r\n            raise exception_class(message, screen, stacktrace, value['alert'].get('text'))\r\n>       raise exception_class(message, screen, stacktrace)\r\nE       SessionNotCreatedException: Message: Tried to run command without establishing a connection\r\n\r\n/usr/local/lib/python2.7/site-packages/selenium/webdriver/remote/errorhandler.py:237: SessionNotCreatedException\r\n------------------------------------------------- pytest-selenium -------------------------------------------------\r\nWARNING: Failed to gather URL: Message: No active session with ID 8b140dd6-30bd-4a3e-b3ba-9c8c56b6ec01\r\n\r\nWARNING: Failed to gather screenshot: Message: No active session with ID 8b140dd6-30bd-4a3e-b3ba-9c8c56b6ec01\r\n\r\nWARNING: Failed to gather HTML: Message: No active session with ID 8b140dd6-30bd-4a3e-b3ba-9c8c56b6ec01\r\n\r\nWARNING: Failed to gather log types: Message:\r\n==================================================== FAILURES =====================================================\r\n__________________________________________ test_dashboard_load_page_two ___________________________________________\r\n\r\nbase_url = 'https://developer-prod.mdn.mozit.cloud'\r\nselenium = <selenium.webdriver.remote.webdriver.WebDriver (session=\"8b140dd6-30bd-4a3e-b3ba-9c8c56b6ec01\")>\r\n\r\n    @user \n    @user \n    def test_dashboard_load_page_two(base_url, selenium):\r\n        page = DashboardPage(selenium, base_url).open()\r\n        # save id of first revision on page one\r\n        first_row_id = page.first_row_id\r\n        # click on page two link\r\n>       page.click_page_two()\r\n\r\ntests/functional/test_dashboard.py:50:\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\ntests/pages/dashboard.py:107: in click_page_two\r\n    self.wait.until(lambda s: 'closed' in self.find_element(*self._first_notification_locator).get_attribute('class'))\r\n/usr/local/lib/python2.7/site-packages/selenium/webdriver/support/wait.py:71: in until\r\n    value = method(self._driver)\r\ntests/pages/dashboard.py:107: in <lambda>\r\n    self.wait.until(lambda s: 'closed' in self.find_element(*self._first_notification_locator).get_attribute('class'))\r\n/usr/local/lib/python2.7/site-packages/pypom/view.py:32: in find_element\r\n    return self.selenium.find_element(strategy, locator)\r\n/usr/local/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py:955: in find_element\r\n    'value': value})['value']\r\n/usr/local/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py:312: in execute\r\n    self.error_handler.check_response(response)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nself = <selenium.webdriver.remote.errorhandler.ErrorHandler object at 0x7fde430d95d0>\r\nresponse = {'status': 500, 'value': '{\"value\":{\"error\":\"unknown error\",\"message\":\"Failed to decode response from marionette\",\"sta...s::imp::thread::{{impl}}::new::thread_start\\n                        at /checkout/src/libstd/sys/unix/thread.rs:84\"}}'}\r\n\r\n    def check_response(self, response):\r\n        \"\"\"\r\n            Checks that a JSON response from the WebDriver does not have an error.\r\n\r\n            :Args:\r\n             - response - The JSON response from the WebDriver server as a dictionary\r\n               object.\r\n\r\n            :Raises: If the response contains an error message.\r\n            \"\"\"\r\n        status = response.get('status', None)\r\n        if status is None or status == ErrorCode.SUCCESS:\r\n            return\r\n        value = None\r\n        message = response.get(\"message\", \"\")\r\n        screen = response.get(\"screen\", \"\")\r\n        stacktrace = None\r\n        if isinstance(status, int):\r\n            value_json = response.get('value', None)\r\n            if value_json and isinstance(value_json, basestring):\r\n                import json\r\n                try:\r\n                    value = json.loads(value_json)\r\n                    if len(value.keys()) == 1:\r\n                        value = value['value']\r\n                    status = value.get('error', None)\r\n                    if status is None:\r\n                        status = value[\"status\"]\r\n                        message = value[\"value\"]\r\n                        if not isinstance(message, basestring):\r\n                            value = message\r\n                            message = message.get('message')\r\n                    else:\r\n                        message = value.get('message', None)\r\n                except ValueError:\r\n                    pass\r\n\r\n        exception_class = ErrorInResponseException\r\n        if status in ErrorCode.NO_SUCH_ELEMENT:\r\n            exception_class = NoSuchElementException\r\n        elif status in ErrorCode.NO_SUCH_FRAME:\r\n            exception_class = NoSuchFrameException\r\n        elif status in ErrorCode.NO_SUCH_WINDOW:\r\n            exception_class = NoSuchWindowException\r\n        elif status in ErrorCode.STALE_ELEMENT_REFERENCE:\r\n            exception_class = StaleElementReferenceException\r\n        elif status in ErrorCode.ELEMENT_NOT_VISIBLE:\r\n            exception_class = ElementNotVisibleException\r\n        elif status in ErrorCode.INVALID_ELEMENT_STATE:\r\n            exception_class = InvalidElementStateException\r\n        elif status in ErrorCode.INVALID_SELECTOR \\\r\n                or status in ErrorCode.INVALID_XPATH_SELECTOR \\\r\n                or status in ErrorCode.INVALID_XPATH_SELECTOR_RETURN_TYPER:\r\n            exception_class = InvalidSelectorException\r\n        elif status in ErrorCode.ELEMENT_IS_NOT_SELECTABLE:\r\n            exception_class = ElementNotSelectableException\r\n        elif status in ErrorCode.ELEMENT_NOT_INTERACTABLE:\r\n            exception_class = ElementNotInteractableException\r\n        elif status in ErrorCode.INVALID_COOKIE_DOMAIN:\r\n            exception_class = InvalidCookieDomainException\r\n        elif status in ErrorCode.UNABLE_TO_SET_COOKIE:\r\n            exception_class = UnableToSetCookieException\r\n        elif status in ErrorCode.TIMEOUT:\r\n            exception_class = TimeoutException\r\n        elif status in ErrorCode.SCRIPT_TIMEOUT:\r\n            exception_class = TimeoutException\r\n        elif status in ErrorCode.UNKNOWN_ERROR:\r\n            exception_class = WebDriverException\r\n        elif status in ErrorCode.UNEXPECTED_ALERT_OPEN:\r\n            exception_class = UnexpectedAlertPresentException\r\n        elif status in ErrorCode.NO_ALERT_OPEN:\r\n            exception_class = NoAlertPresentException\r\n        elif status in ErrorCode.IME_NOT_AVAILABLE:\r\n            exception_class = ImeNotAvailableException\r\n        elif status in ErrorCode.IME_ENGINE_ACTIVATION_FAILED:\r\n            exception_class = ImeActivationFailedException\r\n        elif status in ErrorCode.MOVE_TARGET_OUT_OF_BOUNDS:\r\n            exception_class = MoveTargetOutOfBoundsException\r\n        elif status in ErrorCode.JAVASCRIPT_ERROR:\r\n            exception_class = JavascriptException\r\n        elif status in ErrorCode.SESSION_NOT_CREATED:\r\n            exception_class = SessionNotCreatedException\r\n        elif status in ErrorCode.INVALID_ARGUMENT:\r\n            exception_class = InvalidArgumentException\r\n        elif status in ErrorCode.NO_SUCH_COOKIE:\r\n            exception_class = NoSuchCookieException\r\n        elif status in ErrorCode.UNABLE_TO_CAPTURE_SCREEN:\r\n            exception_class = ScreenshotException\r\n        elif status in ErrorCode.ELEMENT_CLICK_INTERCEPTED:\r\n            exception_class = ElementClickInterceptedException\r\n        elif status in ErrorCode.INSECURE_CERTIFICATE:\r\n            exception_class = InsecureCertificateException\r\n        elif status in ErrorCode.INVALID_COORDINATES:\r\n            exception_class = InvalidCoordinatesException\r\n        elif status in ErrorCode.INVALID_SESSION_ID:\r\n            exception_class = InvalidSessionIdException\r\n        elif status in ErrorCode.UNKNOWN_METHOD:\r\n            exception_class = UnknownMethodException\r\n        else:\r\n            exception_class = WebDriverException\r\n        if value == '' or value is None:\r\n            value = response['value']\r\n        if isinstance(value, basestring):\r\n            if exception_class == ErrorInResponseException:\r\n                raise exception_class(response, value)\r\n            raise exception_class(value)\r\n        if message == \"\" and 'message' in value:\r\n            message = value['message']\r\n\r\n        screen = None\r\n        if 'screen' in value:\r\n            screen = value['screen']\r\n\r\n        stacktrace = None\r\n        if 'stackTrace' in value and value['stackTrace']:\r\n            stacktrace = []\r\n            try:\r\n                for frame in value['stackTrace']:\r\n                    line = self._value_or_default(frame, 'lineNumber', '')\r\n                    file = self._value_or_default(frame, 'fileName', '<anonymous>')\r\n                    if line:\r\n                        file = \"%s:%s\" % (file, line)\r\n                    meth = self._value_or_default(frame, 'methodName', '<anonymous>')\r\n                    if 'className' in frame:\r\n                        meth = \"%s.%s\" % (frame['className'], meth)\r\n                    msg = \"    at %s (%s)\"\r\n                    msg = msg % (meth, file)\r\n                    stacktrace.append(msg)\r\n            except TypeError:\r\n                pass\r\n        if exception_class == ErrorInResponseException:\r\n            raise exception_class(response, message)\r\n        elif exception_class == UnexpectedAlertPresentException and 'alert' in value:\r\n            raise exception_class(message, screen, stacktrace, value['alert'].get('text'))\r\n>       raise exception_class(message, screen, stacktrace)\r\nE       WebDriverException: Message: Failed to decode response from marionette\r\n\r\n/usr/local/lib/python2.7/site-packages/selenium/webdriver/remote/errorhandler.py:237: WebDriverException\r\n------------------------------------------------- pytest-selenium -------------------------------------------------\r\nWARNING: Failed to gather URL: Message: Tried to run command without establishing a connection\r\n\r\nWARNING: Failed to gather screenshot: Message: Tried to run command without establishing a connection\r\n\r\nWARNING: Failed to gather HTML: Message: Tried to run command without establishing a connection\r\n______________________________________________ test_search_homepage _______________________________________________\r\n\r\nbase_url = 'https://developer-prod.mdn.mozit.cloud'\r\nselenium = <selenium.webdriver.remote.webdriver.WebDriver (session=\"30f85b9c-2bb2-4b1c-9b57-ee7509889650\")>\r\n\r\n    @user \n    @user \n    @user \n    def test_search_homepage(base_url, selenium):\r\n        # open homepage\r\n        page = HomePage(selenium, base_url).open()\r\n        # search for CSS in big box\r\n        search = page.search_for_term(SEARCH_TERM)\r\n        # search term is in search box\r\n>       assert search.search_input_value == SEARCH_TERM\r\n\r\ntests/functional/test_search.py:23:\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\ntests/pages/search.py:31: in search_input_value\r\n    return self.find_element(*self._search_input_locator).get_attribute('value')\r\n/usr/local/lib/python2.7/site-packages/pypom/view.py:32: in find_element\r\n    return self.selenium.find_element(strategy, locator)\r\n/usr/local/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py:955: in find_element\r\n    'value': value})['value']\r\n/usr/local/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py:312: in execute\r\n    self.error_handler.check_response(response)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nself = <selenium.webdriver.remote.errorhandler.ErrorHandler object at 0x7fde429f0910>\r\nresponse = {'status': 404, 'value': '{\"value\":{\"error\":\"no such element\",\"message\":\"Unable to locate element: [id=\\\"search-q\\\"]\",...Error@chrome://marionette/content/error.js:463:5\\nelement.find/</<@chrome://marionette/content/element.js:291:16\\n\"}}'}\r\n\r\n    def check_response(self, response):\r\n        \"\"\"\r\n            Checks that a JSON response from the WebDriver does not have an error.\r\n\r\n            :Args:\r\n             - response - The JSON response from the WebDriver server as a dictionary\r\n               object.\r\n\r\n            :Raises: If the response contains an error message.\r\n            \"\"\"\r\n        status = response.get('status', None)\r\n        if status is None or status == ErrorCode.SUCCESS:\r\n            return\r\n        value = None\r\n        message = response.get(\"message\", \"\")\r\n        screen = response.get(\"screen\", \"\")\r\n        stacktrace = None\r\n        if isinstance(status, int):\r\n            value_json = response.get('value', None)\r\n            if value_json and isinstance(value_json, basestring):\r\n                import json\r\n                try:\r\n                    value = json.loads(value_json)\r\n                    if len(value.keys()) == 1:\r\n                        value = value['value']\r\n                    status = value.get('error', None)\r\n                    if status is None:\r\n                        status = value[\"status\"]\r\n                        message = value[\"value\"]\r\n                        if not isinstance(message, basestring):\r\n                            value = message\r\n                            message = message.get('message')\r\n                    else:\r\n                        message = value.get('message', None)\r\n                except ValueError:\r\n                    pass\r\n\r\n        exception_class = ErrorInResponseException\r\n        if status in ErrorCode.NO_SUCH_ELEMENT:\r\n            exception_class = NoSuchElementException\r\n        elif status in ErrorCode.NO_SUCH_FRAME:\r\n            exception_class = NoSuchFrameException\r\n        elif status in ErrorCode.NO_SUCH_WINDOW:\r\n            exception_class = NoSuchWindowException\r\n        elif status in ErrorCode.STALE_ELEMENT_REFERENCE:\r\n            exception_class = StaleElementReferenceException\r\n        elif status in ErrorCode.ELEMENT_NOT_VISIBLE:\r\n            exception_class = ElementNotVisibleException\r\n        elif status in ErrorCode.INVALID_ELEMENT_STATE:\r\n            exception_class = InvalidElementStateException\r\n        elif status in ErrorCode.INVALID_SELECTOR \\\r\n                or status in ErrorCode.INVALID_XPATH_SELECTOR \\\r\n                or status in ErrorCode.INVALID_XPATH_SELECTOR_RETURN_TYPER:\r\n            exception_class = InvalidSelectorException\r\n        elif status in ErrorCode.ELEMENT_IS_NOT_SELECTABLE:\r\n            exception_class = ElementNotSelectableException\r\n        elif status in ErrorCode.ELEMENT_NOT_INTERACTABLE:\r\n            exception_class = ElementNotInteractableException\r\n        elif status in ErrorCode.INVALID_COOKIE_DOMAIN:\r\n            exception_class = InvalidCookieDomainException\r\n        elif status in ErrorCode.UNABLE_TO_SET_COOKIE:\r\n            exception_class = UnableToSetCookieException\r\n        elif status in ErrorCode.TIMEOUT:\r\n            exception_class = TimeoutException\r\n        elif status in ErrorCode.SCRIPT_TIMEOUT:\r\n            exception_class = TimeoutException\r\n        elif status in ErrorCode.UNKNOWN_ERROR:\r\n            exception_class = WebDriverException\r\n        elif status in ErrorCode.UNEXPECTED_ALERT_OPEN:\r\n            exception_class = UnexpectedAlertPresentException\r\n        elif status in ErrorCode.NO_ALERT_OPEN:\r\n            exception_class = NoAlertPresentException\r\n        elif status in ErrorCode.IME_NOT_AVAILABLE:\r\n            exception_class = ImeNotAvailableException\r\n        elif status in ErrorCode.IME_ENGINE_ACTIVATION_FAILED:\r\n            exception_class = ImeActivationFailedException\r\n        elif status in ErrorCode.MOVE_TARGET_OUT_OF_BOUNDS:\r\n            exception_class = MoveTargetOutOfBoundsException\r\n        elif status in ErrorCode.JAVASCRIPT_ERROR:\r\n            exception_class = JavascriptException\r\n        elif status in ErrorCode.SESSION_NOT_CREATED:\r\n            exception_class = SessionNotCreatedException\r\n        elif status in ErrorCode.INVALID_ARGUMENT:\r\n            exception_class = InvalidArgumentException\r\n        elif status in ErrorCode.NO_SUCH_COOKIE:\r\n            exception_class = NoSuchCookieException\r\n        elif status in ErrorCode.UNABLE_TO_CAPTURE_SCREEN:\r\n            exception_class = ScreenshotException\r\n        elif status in ErrorCode.ELEMENT_CLICK_INTERCEPTED:\r\n            exception_class = ElementClickInterceptedException\r\n        elif status in ErrorCode.INSECURE_CERTIFICATE:\r\n            exception_class = InsecureCertificateException\r\n        elif status in ErrorCode.INVALID_COORDINATES:\r\n            exception_class = InvalidCoordinatesException\r\n        elif status in ErrorCode.INVALID_SESSION_ID:\r\n            exception_class = InvalidSessionIdException\r\n        elif status in ErrorCode.UNKNOWN_METHOD:\r\n            exception_class = UnknownMethodException\r\n        else:\r\n            exception_class = WebDriverException\r\n        if value == '' or value is None:\r\n            value = response['value']\r\n        if isinstance(value, basestring):\r\n            if exception_class == ErrorInResponseException:\r\n                raise exception_class(response, value)\r\n            raise exception_class(value)\r\n        if message == \"\" and 'message' in value:\r\n            message = value['message']\r\n\r\n        screen = None\r\n        if 'screen' in value:\r\n            screen = value['screen']\r\n\r\n        stacktrace = None\r\n        if 'stackTrace' in value and value['stackTrace']:\r\n            stacktrace = []\r\n            try:\r\n                for frame in value['stackTrace']:\r\n                    line = self._value_or_default(frame, 'lineNumber', '')\r\n                    file = self._value_or_default(frame, 'fileName', '<anonymous>')\r\n                    if line:\r\n                        file = \"%s:%s\" % (file, line)\r\n                    meth = self._value_or_default(frame, 'methodName', '<anonymous>')\r\n                    if 'className' in frame:\r\n                        meth = \"%s.%s\" % (frame['className'], meth)\r\n                    msg = \"    at %s (%s)\"\r\n                    msg = msg % (meth, file)\r\n                    stacktrace.append(msg)\r\n            except TypeError:\r\n                pass\r\n        if exception_class == ErrorInResponseException:\r\n            raise exception_class(response, message)\r\n        elif exception_class == UnexpectedAlertPresentException and 'alert' in value:\r\n            raise exception_class(message, screen, stacktrace, value['alert'].get('text'))\r\n>       raise exception_class(message, screen, stacktrace)\r\nE       NoSuchElementException: Message: Unable to locate element: [id=\"search-q\"]\r\n\r\n/usr/local/lib/python2.7/site-packages/selenium/webdriver/remote/errorhandler.py:237: NoSuchElementException\r\n------------------------------------------------- pytest-selenium -------------------------------------------------\r\nURL: https://developer-prod.mdn.mozit.cloud/en-US/search?q=css&topic=apps&topic=html&topic=css&topic=js&topic=api&topic=canvas&topic=svg&topic=webgl&topic=mobile&topic=webdev&topic=http&topic=webext&topic=standards\r\n=============================================== 7 tests deselected ================================================\r\n=============== 2 failed, 103 passed, 6 skipped, 7 deselected, 1 xpassed, 1 error in 412.20 seconds ===============\r\n*** Shutting down dockerized browsers...\r\nselenium-chrome-kuma\r\nselenium-chrome-kuma\r\nselenium-firefox-kuma\r\nselenium-firefox-kuma\r\n*** Test results in test_results/functional_20180912_1501\r\n```\r\n\r\nThe Chrome tests passed, and the Firefox tests had two tests with issues:\r\n- `tests/functional/test_dashboard.py::test_dashboard_load_page_two` (this test generated an error as well as a failure in the run above)\r\n- `tests/functional/test_search.py::test_search_homepage` \r\n\r\nBoth of theses tests ran as expected when run separately.\r\n\r\nHere are the details of the successful re-run (sucessful expected failure) of `test_dashboard.py::test_dashboard_load_page_two`:\r\n\r\n```\r\n(mdntest) rjohnson-25186:kuma rjohnson$ BROWSERS=firefox scripts/run_functional_tests.sh --maintenance-mode --base-url https://developer-prod.mdn.mozit.cloud -m \"not login\" tests/functional/test_dashboard.py::test_dashboard_load_page_two\r\n*** Building integration tests image...\r\nSending build context to Docker daemon  179.3MB\r\nStep 1/10 : FROM python:2.7-slim\r\n2.7-slim: Pulling from library/python\r\nDigest: sha256:0a43a6d7858af4a42427c792b682936d2cd34e183fb026627f53ddb556d4bf62\r\nStatus: Image is up to date for python:2.7-slim\r\n ---> c9cde4658340\r\nStep 2/10 : WORKDIR /app\r\n ---> Using cache\r\n ---> fda1425b260f\r\nStep 3/10 : RUN set -ex &&     apt-get update &&     apt-get install -y --no-install-recommends         mime-support         build-essential         libxml2-dev         libxslt1.1         libxslt1-dev         zlib1g-dev     && rm -rf /var/lib/apt/lists/*\r\n ---> Using cache\r\n ---> c7cd96772567\r\nStep 4/10 : ENV PYTEST_PROCESSES 5\r\n ---> Using cache\r\n ---> 6a8ffee2f698\r\nStep 5/10 : ENV PRIVACY \"public restricted\"\r\n ---> Using cache\r\n ---> 32ac9e9d4741\r\nStep 6/10 : ENV TESTS_PATH /app/tests\r\n ---> Using cache\r\n ---> 43a2778303ec\r\nStep 7/10 : ENV RESULTS_PATH /app/results\r\n ---> Using cache\r\n ---> 0c3166366abb\r\nStep 8/10 : COPY ./requirements /app/requirements\r\n ---> Using cache\r\n ---> 4d3ee7eb9284\r\nStep 9/10 : RUN pip install --no-cache-dir -r requirements/test.txt\r\n ---> Using cache\r\n ---> 29fc609364e0\r\nStep 10/10 : COPY tests /app/tests\r\n ---> Using cache\r\n ---> 3976ba7b1070\r\nSuccessfully built 3976ba7b1070\r\nSuccessfully tagged kuma-integration-tests:latest\r\n*** Running dockerized firefox...\r\n50605d2230e5b07397949598b6a1b0b74ea4dfc8668c9b5b73cd16af603bf9cd\r\n*** Running integration tests against firefox...\r\n=============================================== test session starts ===============================================\r\nplatform linux2 -- Python 2.7.15, pytest-3.1.3, py-1.4.33, pluggy-0.4.0\r\ndriver: Remote\r\nsensitiveurl: .* *** WARNING: sensitive url matches https://developer-prod.mdn.mozit.cloud ***\r\nmetadata: {'Python': '2.7.15', 'Driver': 'Remote', 'Capabilities': {'browserName': 'firefox'}, 'Server': 'hub:4444', 'Base URL': 'https://developer-prod.mdn.mozit.cloud', 'Platform': 'Linux-4.9.93-linuxkit-aufs-x86_64-with-debian-9.5', 'kuma': {u'services': {u'search': {u'available': True, u'count': 60165, u'populated': True}, u'kumascript': {u'available': True, u'revision': u'e57e27c371416c7e104f7843318e9166dbe3c1f3'}, u'test_accounts': {u'available': False}, u'database': {u'available': True, u'document_count': 130018, u'populated': True}}, u'version': 1, u'request': {u'url': u'https://developer-prod.mdn.mozit.cloud/_kuma_status.json', u'is_secure': True, u'host': u'developer-prod.mdn.mozit.cloud', u'scheme': u'https'}, 'response': {'headers': {'X-Cache': 'Miss from cloudfront', 'strict-transport-security': 'max-age=63072000', 'x-content-type-options': 'nosniff', 'Content-Language': 'en-US', 'Content-Encoding': 'gzip', 'Transfer-Encoding': 'chunked', 'Expires': 'Wed, 12 Sep 2018 22:53:46 GMT', 'Vary': 'Accept-Encoding', 'Server': 'meinheld/0.6.1', 'Connection': 'keep-alive', 'x-xss-protection': '1; mode=block', 'X-Amz-Cf-Id': 'TMTD5z3SQvjBpCM4HaztolgU5cHDWnu3nGAFsiV6lXdEhNhRRKL_5A==', 'Cache-Control': 'no-cache, no-store, must-revalidate, max-age=0', 'Date': 'Wed, 12 Sep 2018 22:53:46 GMT', 'X-Frame-Options': 'DENY', 'Content-Type': 'application/json', 'Via': '1.1 19f9923c4e449b92312c8813bf9135f5.cloudfront.net (CloudFront)'}}, u'settings': {u'ATTACHMENT_HOST': u'demos.mdn.mozit.cloud', u'PROTOCOL': u'https://', u'INTERACTIVE_EXAMPLES_BASE': u'https://interactive-examples.mdn.mozit.cloud', u'MAINTENANCE_MODE': True, u'STATIC_URL': u'https://developer-prod.mdn.mozit.cloud/static/', u'SITE_URL': u'https://developer-prod.mdn.mozit.cloud', u'ATTACHMENT_ORIGIN': u'prod.mdn.mozit.cloud', u'DEBUG': False, u'ALLOWED_HOSTS': [u'developer.mozilla.org', u'cdn.mdn.mozilla.net', u'mdn.mozillademos.org', u'demos.mdn.mozit.cloud', u'developer-prod.mdn.mozit.cloud', u'prod.mdn.mozit.cloud', u'developer.mozilla.com', u'mdn.mozilla.org', u'developer-new.mozilla.org', u'developers.mozilla.org'], u'REVISION_HASH': u'0591dd661bfa5e34f19a74428dff5cc8a172998c', u'LEGACY_HOSTS': [u'cdn.mdn.mozilla.net', u'developer.mozilla.com', u'mdn.mozilla.org', u'developer-new.mozilla.org', u'developers.mozilla.org']}}, 'Plugins': {'variables': '1.7.1', 'selenium': '1.11.4', 'rerunfailures': '2.1.0', 'html': '1.16.1', 'base-url': '1.4.1', 'metadata': '1.5.1'}, 'Packages': {'py': '1.4.33', 'pytest': '3.1.3', 'pluggy': '0.4.0'}}\r\nbaseurl: https://developer-prod.mdn.mozit.cloud\r\nrootdir: /app, inifile:\r\nplugins: selenium-1.11.4, html-1.16.1, rerunfailures-2.1.0, variables-1.7.1, metadata-1.5.1, base-url-1.4.1\r\ncollected 1 item\r\n\r\ntests/functional/test_dashboard.py x\r\n\r\n------------------------------------ generated html file: /results/pytest.html ------------------------------------\r\n=========================================== 1 xfailed in 14.94 seconds ============================================\r\n*** Shutting down dockerized browsers...\r\nselenium-firefox-kuma\r\nselenium-firefox-kuma\r\n*** Test results in test_results/functional_20180912_1553\r\n```\r\n\r\nHere are the details of the successful re-run of `tests/functional/test_search.py::test_search_homepage`:\r\n\r\n```\r\n(mdntest) rjohnson-25186:kuma rjohnson$ BROWSERS=firefox scripts/run_functional_tests.sh --maintenance-mode --base-url https://developer-prod.mdn.mozit.cloud -m \"not login\" tests/functional/test_search.py::test_search_homepage\r\n*** Building integration tests image...\r\nSending build context to Docker daemon  179.3MB\r\nStep 1/10 : FROM python:2.7-slim\r\n2.7-slim: Pulling from library/python\r\nDigest: sha256:0a43a6d7858af4a42427c792b682936d2cd34e183fb026627f53ddb556d4bf62\r\nStatus: Image is up to date for python:2.7-slim\r\n ---> c9cde4658340\r\nStep 2/10 : WORKDIR /app\r\n ---> Using cache\r\n ---> fda1425b260f\r\nStep 3/10 : RUN set -ex &&     apt-get update &&     apt-get install -y --no-install-recommends         mime-support         build-essential         libxml2-dev         libxslt1.1         libxslt1-dev         zlib1g-dev     && rm -rf /var/lib/apt/lists/*\r\n ---> Using cache\r\n ---> c7cd96772567\r\nStep 4/10 : ENV PYTEST_PROCESSES 5\r\n ---> Using cache\r\n ---> 6a8ffee2f698\r\nStep 5/10 : ENV PRIVACY \"public restricted\"\r\n ---> Using cache\r\n ---> 32ac9e9d4741\r\nStep 6/10 : ENV TESTS_PATH /app/tests\r\n ---> Using cache\r\n ---> 43a2778303ec\r\nStep 7/10 : ENV RESULTS_PATH /app/results\r\n ---> Using cache\r\n ---> 0c3166366abb\r\nStep 8/10 : COPY ./requirements /app/requirements\r\n ---> Using cache\r\n ---> 4d3ee7eb9284\r\nStep 9/10 : RUN pip install --no-cache-dir -r requirements/test.txt\r\n ---> Using cache\r\n ---> 29fc609364e0\r\nStep 10/10 : COPY tests /app/tests\r\n ---> Using cache\r\n ---> 3976ba7b1070\r\nSuccessfully built 3976ba7b1070\r\nSuccessfully tagged kuma-integration-tests:latest\r\n*** Running dockerized firefox...\r\nb41f2f8da480ceb4bb6fcbcbe2cb908e26db390551036d22fd79a514fcdf2e54\r\n*** Running integration tests against firefox...\r\n=============================================== test session starts ===============================================\r\nplatform linux2 -- Python 2.7.15, pytest-3.1.3, py-1.4.33, pluggy-0.4.0\r\ndriver: Remote\r\nsensitiveurl: .* *** WARNING: sensitive url matches https://developer-prod.mdn.mozit.cloud ***\r\nmetadata: {'Python': '2.7.15', 'Driver': 'Remote', 'Capabilities': {'browserName': 'firefox'}, 'Server': 'hub:4444', 'Base URL': 'https://developer-prod.mdn.mozit.cloud', 'Platform': 'Linux-4.9.93-linuxkit-aufs-x86_64-with-debian-9.5', 'kuma': {u'services': {u'search': {u'available': True, u'count': 60165, u'populated': True}, u'kumascript': {u'available': True, u'revision': u'e57e27c371416c7e104f7843318e9166dbe3c1f3'}, u'test_accounts': {u'available': False}, u'database': {u'available': True, u'document_count': 130018, u'populated': True}}, u'version': 1, u'request': {u'url': u'https://developer-prod.mdn.mozit.cloud/_kuma_status.json', u'is_secure': True, u'host': u'developer-prod.mdn.mozit.cloud', u'scheme': u'https'}, 'response': {'headers': {'X-Cache': 'Miss from cloudfront', 'strict-transport-security': 'max-age=63072000', 'x-content-type-options': 'nosniff', 'Content-Language': 'en-US', 'Content-Encoding': 'gzip', 'Transfer-Encoding': 'chunked', 'Expires': 'Wed, 12 Sep 2018 22:57:46 GMT', 'Vary': 'Accept-Encoding', 'Server': 'meinheld/0.6.1', 'Connection': 'keep-alive', 'x-xss-protection': '1; mode=block', 'X-Amz-Cf-Id': 'ozWfIbu0UDutRBQrDIlnWdIiz4sYNfnc2g6bbW5711rFlRxQA0oFZg==', 'Cache-Control': 'no-cache, no-store, must-revalidate, max-age=0', 'Date': 'Wed, 12 Sep 2018 22:57:46 GMT', 'X-Frame-Options': 'DENY', 'Content-Type': 'application/json', 'Via': '1.1 817705cae774a11f586612cbd427091c.cloudfront.net (CloudFront)'}}, u'settings': {u'ATTACHMENT_HOST': u'demos.mdn.mozit.cloud', u'PROTOCOL': u'https://', u'INTERACTIVE_EXAMPLES_BASE': u'https://interactive-examples.mdn.mozit.cloud', u'MAINTENANCE_MODE': True, u'STATIC_URL': u'https://developer-prod.mdn.mozit.cloud/static/', u'SITE_URL': u'https://developer-prod.mdn.mozit.cloud', u'ATTACHMENT_ORIGIN': u'prod.mdn.mozit.cloud', u'DEBUG': False, u'ALLOWED_HOSTS': [u'developer.mozilla.org', u'cdn.mdn.mozilla.net', u'mdn.mozillademos.org', u'demos.mdn.mozit.cloud', u'developer-prod.mdn.mozit.cloud', u'prod.mdn.mozit.cloud', u'developer.mozilla.com', u'mdn.mozilla.org', u'developer-new.mozilla.org', u'developers.mozilla.org'], u'REVISION_HASH': u'0591dd661bfa5e34f19a74428dff5cc8a172998c', u'LEGACY_HOSTS': [u'cdn.mdn.mozilla.net', u'developer.mozilla.com', u'mdn.mozilla.org', u'developer-new.mozilla.org', u'developers.mozilla.org']}}, 'Plugins': {'variables': '1.7.1', 'selenium': '1.11.4', 'rerunfailures': '2.1.0', 'html': '1.16.1', 'base-url': '1.4.1', 'metadata': '1.5.1'}, 'Packages': {'py': '1.4.33', 'pytest': '3.1.3', 'pluggy': '0.4.0'}}\r\nbaseurl: https://developer-prod.mdn.mozit.cloud\r\nrootdir: /app, inifile:\r\nplugins: selenium-1.11.4, html-1.16.1, rerunfailures-2.1.0, variables-1.7.1, metadata-1.5.1, base-url-1.4.1\r\ncollected 1 item\r\n\r\ntests/functional/test_search.py .\r\n\r\n------------------------------------ generated html file: /results/pytest.html ------------------------------------\r\n============================================ 1 passed in 5.36 seconds =============================================\r\n*** Shutting down dockerized browsers...\r\nselenium-firefox-kuma\r\nselenium-firefox-kuma\r\n*** Test results in test_results/functional_20180912_1557\r\n```",
        "# Headless Tests\r\n\r\nHeadless tests require a Python environment with the requirements in\r\n``requirements/test.txt``. See [Setting up test virtual environment](https://kuma.readthedocs.io/en/latest/tests-ui.html#setting-up-test-virtual-environment).\r\nRun the headless tests, which make safe HTTP requests (no POSTs, no logins):\r\n\r\n```\r\npytest --maintenance-mode --base-url https://developer-prod.mdn.mozit.cloud tests/headless \r\n```\r\n\r\n* [x] Headless tests pass\r\n\r\nThe headless tests returned 4 failures, but each of the 4 failures is expected (returns a 302 status when a status of 200 was expected) since https://developer-prod.mdn.mozit.cloud is configured in maintenance mode. In other words, the failures are due to the tests not being properly designed to run correctly against a system in maintenance mode.\r\n\r\nDetails:\r\n```\r\n(mdntest) rjohnson-25186:kuma rjohnson$ pytest --maintenance-mode --base-url https://developer-prod.mdn.mozit.cloud tests/headless\r\n=============================================== test session starts ===============================================\r\nplatform darwin -- Python 2.7.12, pytest-3.5.1, py-1.5.3, pluggy-0.6.0\r\nsensitiveurl: .* *** WARNING: sensitive url matches https://developer-prod.mdn.mozit.cloud ***\r\nmetadata: {'Python': '2.7.12', 'Driver': None, 'Capabilities': {}, 'Base URL': 'https://developer-prod.mdn.mozit.cloud', 'Platform': 'Darwin-17.7.0-x86_64-i386-64bit', 'kuma': {u'services': {u'search': {u'available': True, u'count': 60165, u'populated': True}, u'kumascript': {u'available': True, u'revision': u'e57e27c371416c7e104f7843318e9166dbe3c1f3'}, u'test_accounts': {u'available': False}, u'database': {u'available': True, u'document_count': 130018, u'populated': True}}, u'version': 1, u'request': {u'url': u'https://developer-prod.mdn.mozit.cloud/_kuma_status.json', u'is_secure': True, u'host': u'developer-prod.mdn.mozit.cloud', u'scheme': u'https'}, 'response': {'headers': {'X-Cache': 'Miss from cloudfront', 'strict-transport-security': 'max-age=63072000', 'x-content-type-options': 'nosniff', 'Content-Language': 'en-US', 'Content-Encoding': 'gzip', 'Transfer-Encoding': 'chunked', 'Expires': 'Wed, 12 Sep 2018 23:07:57 GMT', 'Vary': 'Accept-Encoding', 'Server': 'meinheld/0.6.1', 'Connection': 'keep-alive', 'x-xss-protection': '1; mode=block', 'X-Amz-Cf-Id': 'Eqp1NjIX9v548Mb97JR7LZNFqsm3qOUwb39izsudBgscKzTdCg1toQ==', 'Cache-Control': 'no-cache, no-store, must-revalidate, max-age=0', 'Date': 'Wed, 12 Sep 2018 23:07:57 GMT', 'X-Frame-Options': 'DENY', 'Content-Type': 'application/json', 'Via': '1.1 617383234aa18e133ce6e5179e83aa88.cloudfront.net (CloudFront)'}}, u'settings': {u'ATTACHMENT_HOST': u'demos.mdn.mozit.cloud', u'PROTOCOL': u'https://', u'INTERACTIVE_EXAMPLES_BASE': u'https://interactive-examples.mdn.mozit.cloud', u'MAINTENANCE_MODE': True, u'STATIC_URL': u'https://developer-prod.mdn.mozit.cloud/static/', u'SITE_URL': u'https://developer-prod.mdn.mozit.cloud', u'ATTACHMENT_ORIGIN': u'prod.mdn.mozit.cloud', u'DEBUG': False, u'ALLOWED_HOSTS': [u'developer.mozilla.org', u'cdn.mdn.mozilla.net', u'mdn.mozillademos.org', u'demos.mdn.mozit.cloud', u'developer-prod.mdn.mozit.cloud', u'prod.mdn.mozit.cloud', u'developer.mozilla.com', u'mdn.mozilla.org', u'developer-new.mozilla.org', u'developers.mozilla.org'], u'REVISION_HASH': u'0591dd661bfa5e34f19a74428dff5cc8a172998c', u'LEGACY_HOSTS': [u'cdn.mdn.mozilla.net', u'developer.mozilla.com', u'mdn.mozilla.org', u'developer-new.mozilla.org', u'developers.mozilla.org']}}, 'Plugins': {'variables': '1.7.1', 'selenium': '1.11.4', 'xdist': '1.16.0', 'rerunfailures': '2.1.0', 'html': '1.16.1', 'base-url': '1.4.1', 'metadata': '1.5.1'}, 'Packages': {'py': '1.5.3', 'pytest': '3.5.1', 'pluggy': '0.6.0'}}\r\nbaseurl: https://developer-prod.mdn.mozit.cloud\r\nrootdir: /Users/rjohnson/repos/kuma, inifile: pytest.ini\r\nplugins: xdist-1.16.0, variables-1.7.1, selenium-1.11.4, rerunfailures-2.1.0, metadata-1.5.1, html-1.16.1, base-url-1.4.1\r\ncollected 1696 items\r\n\r\ntests/headless/test_cdn.py ...F.FF..................F.....................................................................s.................................................................................................................................................................................................................................................................................................................. [  0%]\r\ntests/headless/test_endpoints.py ...                                                                        [  0%]\r\ntests/headless/test_redirects.py ...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... [  0%]\r\ntests/headless/test_robots.py .....\r\n\r\n==================================================== FAILURES =====================================================\r\n______________________________________ test_not_cached[/en-US/users/signin] _______________________________________\r\nTraceback (most recent call last):\r\n  File \"/Users/rjohnson/repos/kuma/tests/headless/test_cdn.py\", line 117, in test_not_cached\r\n    assert_not_cached(base_url + slug, 200, is_behind_cdn)\r\n  File \"/Users/rjohnson/repos/kuma/tests/headless/test_cdn.py\", line 64, in assert_not_cached\r\n    **request_kwargs)\r\n  File \"/Users/rjohnson/repos/kuma/tests/headless/test_cdn.py\", line 52, in assert_not_cached_by_cdn\r\n    assert response.status_code == expected_status_code\r\nAssertionError: assert 302 == 200\r\n +  where 302 = <Response [302]>.status_code\r\n______________________________________ test_not_cached[/en-US/unsubscribe/1] ______________________________________\r\nTraceback (most recent call last):\r\n  File \"/Users/rjohnson/repos/kuma/tests/headless/test_cdn.py\", line 117, in test_not_cached\r\n    assert_not_cached(base_url + slug, 200, is_behind_cdn)\r\n  File \"/Users/rjohnson/repos/kuma/tests/headless/test_cdn.py\", line 64, in assert_not_cached\r\n    **request_kwargs)\r\n  File \"/Users/rjohnson/repos/kuma/tests/headless/test_cdn.py\", line 52, in assert_not_cached_by_cdn\r\n    assert response.status_code == expected_status_code\r\nAssertionError: assert 302 == 200\r\n +  where 302 = <Response [302]>.status_code\r\n_________________________________________ test_not_cached[/admin/login/] __________________________________________\r\nTraceback (most recent call last):\r\n  File \"/Users/rjohnson/repos/kuma/tests/headless/test_cdn.py\", line 117, in test_not_cached\r\n    assert_not_cached(base_url + slug, 200, is_behind_cdn)\r\n  File \"/Users/rjohnson/repos/kuma/tests/headless/test_cdn.py\", line 64, in assert_not_cached\r\n    **request_kwargs)\r\n  File \"/Users/rjohnson/repos/kuma/tests/headless/test_cdn.py\", line 52, in assert_not_cached_by_cdn\r\n    assert response.status_code == expected_status_code\r\nAssertionError: assert 302 == 200\r\n +  where 302 = <Response [302]>.status_code\r\n___________________________ test_not_cached_admin_login_required[/admin/users/user/1/] ____________________________\r\nTraceback (most recent call last):\r\n  File \"/Users/rjohnson/repos/kuma/tests/headless/test_cdn.py\", line 165, in test_not_cached_admin_login_required\r\n    assert response.headers['location'].endswith(\r\nAssertionError: assert False\r\n +  where False = <built-in method endswith of str object at 0x109767130>(('/admin/login/?next=' + '/admin/users/user/1/'))\r\n +    where <built-in method endswith of str object at 0x109767130> = '/en-US/maintenance-mode'.endswith\r\n +    and   '/admin/users/user/1/' = quote('/admin/users/user/1/')\r\n============================================= short test summary info =============================================\r\nSKIP [1] tests/headless/test_cdn.py:295: unconditional skip\r\n=============================== 4 failed, 1691 passed, 1 skipped in 398.56 seconds ================================\r\n```",
        "# Full Manual Tests\r\n\r\n## Content tests\r\n\r\n* [x] https://demos.mdn.mozit.cloud/en-US/docs/Learn/CSS/Styling_text/Fundamentals$samples/Color - 200, sample as a stand-alone page\r\n* [x] https://demos.mdn.mozit.cloud/files/12984/web-font-example.png - 200, PNG of some \"Hipster ipsum\" text\r\n* [x] https://developer-prod.mdn.mozit.cloud/@api/deki/files/3613/=hut.jpg - 200, image of a hat\r\n* [x] https://developer-prod.mdn.mozit.cloud/contribute.json - 200, project info\r\n* [x] https://developer-prod.mdn.mozit.cloud/diagrams/workflow/workflow.svg - 200, SVG with images\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/Firefox/Releases - 200, Firefox logo, list of releases (zoned URL)\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/dashboards/macros - 200, list of macros and page counts\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/dashboards/revisions - 200, list of recent changes\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/Learn/CSS/Styling_text/Fundamentals#Color - 200, with sample as iframe\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/Learn/CSS/Styling_text/Fundamentals$toc - 200, HTML table of contents\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/Web/HTML$children - 200, JSON list of child pages\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/Web/HTML$compare?locale=en-US&to=1299417&from=1293895 - 200, compares revisions\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/Web/HTML$history - 200, list of revisions\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/Web/HTML$json - 200, JSON of page metadata\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/Web/HTML$revision/1293895 - 200, historical revision\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/all - 200, paginated list of docs\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/ckeditor_config.js - 200, JavaScript\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/feeds/atom/files/ - 200, Atom feed of changed files\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/feeds/rss/all/ - 200, RSS feed of new pages\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/feeds/rss/needs-review/ - 200, RSS feed of pages needing review\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/feeds/rss/needs-review/technical - 200, RSS feed of pages needing technical review\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/feeds/rss/revisions - 200, RSS feed of changes\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/feeds/rss/tag/CSS - 200, RSS feed of pages with CSS tag\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/needs-review/editorial - 200, paginated list of documents\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/tag/ARIA - 200, list of documents\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/tags - 200, paginated list of tags\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/top-level - 200, paginated list of documents\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/with-errors - 200, (empty?) paginated list of documents\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/without-parent - 200, paginated list of documents\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/profiles/sheppy - 200, Sheppy's profile\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/promote/ - 200, Promote MDN with 4 buttons\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/search - 200, Search results\r\n* [x] https://developer-prod.mdn.mozit.cloud/fellowship/ - 301 to archive of 2015 MDN Fellowship Program\r\n* [x] https://developer-prod.mdn.mozit.cloud/files/12984/web-font-example.png - Redirects to https://demos.mdn.mozit.cloud\r\n* [x] https://developer-prod.mdn.mozit.cloud/fr/docs/feeds/rss/l10n-updates/ - 200, RSS feed of out-of-date pages\r\n* [x] https://developer-prod.mdn.mozit.cloud/fr/docs/localization-tag/inprogress - 200, paginated list of documents\r\n* [x] https://developer-prod.mdn.mozit.cloud/humans.txt - 200, list of GitHub usernames\r\n* [x] https://developer-prod.mdn.mozit.cloud/media/kumascript-revision.txt - 200, git commit hash for kumascript\r\n* [x] https://developer-prod.mdn.mozit.cloud/media/revision.txt - 200, git commit hash for kuma\r\n* [x] https://developer-prod.mdn.mozit.cloud/miel  - 500 Internal Server Error\r\n* [x] https://developer-prod.mdn.mozit.cloud/presentations/microsummaries/index.html - 200, 2006 OSCON presentation\r\n* [x] https://developer-prod.mdn.mozit.cloud/robots.txt - 200, robots disallow list\r\n* [x] https://developer-prod.mdn.mozit.cloud/samples/webgl/sample3 - 200, Shows WebGL demo\r\n* [x] https://developer-prod.mdn.mozit.cloud/sitemap.xml - 200, list of sitemaps\r\n* [x] https://developer-prod.mdn.mozit.cloud/sitemaps/en-US/sitemap.xml - 200, list of en-US pages\r\n\r\n## Anonymous tests\r\n\r\nTest these URLs as an anonymous user:\r\n\r\n* [x] https://developer-prod.mdn.mozit.cloud/admin/users/user/1/ - 302 redirect to maintenance-mode page.\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/dashboards/spam - 302 redirect to maintenance-mode page\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/Web/HTML$edit - 302 redirect to maintenance-mode page\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/Web/HTML$translate - 302 redirect to maintenance-mode page\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/Firefox/Releases$edit - 302 redirect to maintenance-mode page\r\n* [x] https://developer-prod.mdn.mozit.cloud/en-US/Firefox/Releases$translate - 302 redirect to maintenance-mode page\r\n\r\n**The following tests have been skipped for now as they do not apply when the system is in maintenance mode.**\r\n\r\n> ## Regular Account Tests\r\n> \r\n> Some things to try with a regular account, to exercise write functionality:\r\n> \r\n> * [ ] Create a new MDN user account (may require deleting your ``SocialAccount``)\r\n> * [ ] Send a account recovery link for an existing MDN user account\r\n> * [ ] Update the account profile\r\n> * [ ] Add and verify a new email for a profile\r\n> * [ ] Create a new page, such as https://developer-prod.mdn.mozit.cloud/en-US/docs/User:Test\r\n> * [ ] Create a translation of the new page\r\n> * [ ] Subscribe to a page, and change it with a different account\r\n> * [ ] Update the original page with a KumaScript macro, such as ``{{cssxref(\"background\")}}``.\r\n> * [ ] Update the translation of a changed English page\r\n> * [ ] Upload an image to the page\r\n> * [ ] Add the image to the page content\r\n> * [ ] Edit a zoned URL like https://developer-prod.mdn.mozit.cloud/en-US/Firefox/Releases\r\n> * [ ] Log out\r\n> \r\n> ## Admin Tests\r\n> \r\n> Some things to try with an admin account, to exercise restricted functionality:\r\n> \r\n> * [ ] Move a page\r\n> * [ ] Delete a page\r\n> * [ ] Move a zoned page\r\n> * [ ] View https://developer-prod.mdn.mozit.cloud/admin/users/user/1/ - 200, first user\r\n> * [ ] View https://developer-prod.mdn.mozit.cloud/en-US/dashboards/spam - 200, \"please wait\" or dashboard",
        "\u2705 I've completed the checklist for maintenance mode.\r\n\ud83c\udf89 All tests passed.",
        "Before I close this, I'd like to do a few more things:\r\n- I'd like to test the interactive examples in detail. Although not strictly necessary, there may be some tweaks needed to make it fully work outside the `interactive-examples.mdn.mozilla.net` domain.\r\n- I need to add the New Relic App IDs to the K8s secrets file",
        "## Interactive-Examples Tests:\r\nThese tests were manually run to ensure that interactive examples run properly from their new prod infrastructure:\r\n```\r\ninteractive-examples.mdn.mozit.cloud --> new interactive-examples CDN --> mdninteractive-b77d14bceaaa9ea4.s3-website-us-west-2.amazonaws.com (the website run from the mdninteractive-b77d14bceaaa9ea4 S3 bucket)\r\n```\r\nThese tests depend on https://github.com/mdn/infra/pull/71.\r\n\r\nPrior to testing the endpoints listed below, I did the following:\r\n- re-deployed https://developer-prod.mdn.mozit.cloud in normal (read/write) mode (using `https://github.com/mdn/infra/blob/master/apps/mdn/mdn-aws/k8s/regions/oregon/prod.mm.min.sh` but with `KUMA_MAINTENANCE_MODE` set to `False`)\r\n- configured https://developer-prod.mdn.mozit.cloud for GitHub logins via the admin console (using an OAuth application I created within my own GitHub account) \r\n- logged-in to https://developer-prod.mdn.mozit.cloud\r\n\r\nThe test performed for each endpoint listed below comprised the following steps:\r\n- `shift`-reload the page to re-render its `EmbedInteractiveExample` macro so that it uses `interactive-examples.mdn.mozit.cloud` rather than `interactive-examples.mdn.mozilla.net`\r\n- wait until the page has been re-rendered\r\n- using the dev tools, check that the interactive example loaded without error from `interactive-examples.mdn.mozit.cloud`\r\n- manually interact with the example to check that it works properly\r\n\r\nThe endpoints tested:\r\n- [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/Web/JavaScript/Reference/Functions/arguments\r\n- [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/Web/HTML/Element/iframe\r\n- [x] https://developer-prod.mdn.mozit.cloud/en-US/docs/Web/CSS/columns",
        "Created https://github.com/mozilla/mdn-k8s-private/pull/68 to add the New Relic application ID's and the SpeedCurve site ID to the `mdn-secrets` for the `mdn-prod` namespace within the `oregon` cluster.",
        "**Prior to performing these regular account tests, the site was placed into `normal` (read/write) mode.**\r\n\r\n## Regular Account Tests\r\n\r\nSome things to try with a regular account, to exercise write functionality:\r\n\r\n* [x] Create a new MDN user account (may require deleting your ``SocialAccount``)\r\n* [x] Send a account recovery link for an existing MDN user account\r\n* [x] Update the account profile\r\n* [x] Add and verify a new email for a profile\r\n* [x] Create a new page, such as https://developer-prod.mdn.mozit.cloud/en-US/docs/User:Test\r\n* [x] Create a translation of the new page\r\n* [x] Subscribe to a page, and change it with a different account\r\n* [x] Update the original page with a KumaScript macro, such as ``{{cssxref(\"background\")}}``.\r\n* [x] Update the translation of a changed English page\r\n* [x] Upload an image to the page\r\n* [x] Add the image to the page content\r\n* [x] Edit a zoned URL like https://developer-prod.mdn.mozit.cloud/en-US/Firefox/Releases\r\n* [x] Log out\r\n\r\n## Admin Tests\r\n\r\nSome things to try with an admin account, to exercise restricted functionality:\r\n\r\n* [x] Move a page\r\n* [x] Move a zoned page\r\n* [x] Delete a page\r\n* [x] Purge a deleted page\r\n* [x] Restore a deleted page\r\n* [x] View https://developer-prod.mdn.mozit.cloud/admin/users/user/1/ - 200, first user\r\n* [x] View https://developer-prod.mdn.mozit.cloud/en-US/dashboards/spam - 200, \"please wait\" or dashboard\r\n",
        "Initial list of follow-on issues required prior to go-live:\r\n- Finalize step-by-step cutover plan (#43)\r\n- Setup DR (Read-Only) PoC instance of MDN (#78)",
        "This is complete. \ud83c\udf89 "
      ]
    },
    "codes": [
      "instance",
      "awareness",
      "increase"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/mdn/infra/issues/130",
    "content": {
      "title": "Post Cutover - AWS & Jenkins Tasks",
      "body": "# User Story\r\nAs an MDN staff developer, after successfully executing the MDN cut-over plan, I'd like to make sure that updates to the MDN repos no longer trigger actions upon MozMEAO-related infrastructure, as well as that any we take care of any AWS tasks (e.g., the sample database downloads are handled from the MozIT AWS account rather than MozMEAO account).\r\n\r\n# Acceptance Criteria\r\n* The `mdn-downloads` S3 bucket and its content have been moved from the MozMEAO account to the MozIT account\r\n* The four (4) MDN pipelines within the MozMEAO Jenkins service have been paused/disabled so that we no longer trigger effects within the MozMEAO world (e.g., image pushes to quay.io as well as pushes to MozMEAO Kubernetes clusters)\r\n* Temporary IAM users within the MozIT account that were created solely for the pre-cutover period have been deleted\r\n* Unused S3 buckets have been removed from the MozIT AWS account\r\n\r\n# Tasks\r\n* [x] Copy the content of the MozMEAO `mdn-downloads` S3 bucket to a temporary S3 bucket in the MozIT account\r\n* [x] Delete the MozMEAO `mdn-downloads` bucket\r\n* [x] Create a new `mdn-downloads` bucket within the MozIT account (with `public` access)\r\n* [x] Transfer data from temporary S3 bucket to the `mdn-downloads` bucket\r\n* [x] Delete the temporary S3 bucket\r\n* [x] Pause/disable the `mdn_multibranch_pipeline` (Kuma) within the MozMEAO Jenkins service \r\n* [x] Pause/disable the `kumascript_multibranch_pipeline` (Kumascript) within the MozMEAO Jenkins service\r\n* [x] Pause/disable the `mdn_interactive_examples` pipeline within the MozMEAO Jenkins service\r\n* [x] Pause/disable the `MozMEAO MDN backup` pipeline within the MozMEAO Jenkins service\r\n* [x] Delete the `migration-sync-user` from the MozIT AWS account\r\n* [x] Delete the following unused S3 buckets from the MozIT AWS account and from the Terraform code in the Infra repo:\r\n    * [x] `mdn-db-storage-anonymized-c2037ed87dd96008`\r\n    * [x] `mdn-db-storage-c2037ed87dd96008`\r\n    * [x] `mdn-db-storage-c2037ed87dd96008-logs`\r\n    * [x] `mdn-downloads-c2037ed87dd96008`\r\n    * [x] `mdn-downloads-c2037ed87dd96008-logs`\r\n\r\n",
      "comments": [
        " @user I like the plan of deleting and recreating ``mdn-downloads``. There is a chance that AWS still won't allow us to reuse the name ``mdn-downloads`` for the S3 bucket. If that happens, we should probably switch to a DNS name we control to point to the S3 bucket, rather than my initial shortcut of using AWS's name. That will require documentation changes and a bug number (or maybe we reuse the MozIT bug). But we can talk about that if we need to.",
        "On Tuesday Oct. 30, I deleted the `mdn-downloads` S3 bucket in the MozMEAO account, and after about 2 hours, I was able to re-create it within the MozIT account. As an experiment, I also tried setting-up an S3 bucket with a custom domain name (`downloads.mdn.mozit.cloud`). I was able to do that, but the problem is that `https` can't be used with that bucket unless we add a CDN as well, since AWS does not allow custom certificates to be attached to S3 buckets. That's definitely not worth the cost given how infrequently the downloads bucket is currently used.\r\n\r\nThis is done."
      ]
    },
    "codes": [
      "saving",
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/mdn/infra/issues/296",
    "content": {
      "title": "Switch to m5 class of EC2 instances",
      "body": "We're currently using an m4 class of instances which is the previous generation of the general purpose instances. As it stands all of our k8s nodes use the m4 class, so we should take some time to actually upgrade them to the newest class.\r\n\r\nOnce of the benefits of upgrading is that it will be cheaper to run and also provides us with more flexibility when buying RIs from AWS",
      "comments": [
        "Instances currently need updating\r\n\r\nus-west-2\r\n- [x] k8s ec2 master - m4.large\r\n- [x] k8s ec2 nodes - m4.xlarge\r\n\r\neu-west-1\r\n- [x] k8s ec2 master - m4.large\r\n- [x] k8s ec2 nodes - m4.xlarge\r\n\r\n"
      ]
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/mdn/infra/issues/304",
    "content": {
      "title": "Redis instance class update",
      "body": "Once #303 is complete we need to look into updating redis to a newer instance class. Currently its running on `cache.m3.xlarge` and `cache.t3.small`, look into using a more modern instance class.",
      "comments": [
        "### Summary\r\nAfter discussing this with @user  we can move ahead on this task, updating our stage and prod Redis instances to use the latest server class, without first doing #303. In other words, we can do #303 at a later time. This enables IT to move ahead and purchase/use reserved instances for Redis rather than the more expensive on-demand instances we're using now.\r\n\r\n### Plan\r\nDo the following for both stage and prod:\r\n1. Create new Redis instance (that uses reserve instances of the latest server class)\r\n1. Put MDN into maintenance mode (for prod make sure this has been announced via a soapbox message at least 30 minutes prior)\r\n1. Ensure Redis Celery queues have drained (`celery inspect active`)\r\n1. Change the Redis URL's in the K8s secrets to point to the new Redis instance, and deploy (the new secrets will NOT be used until all of the deployments that use them are rolled)\r\n1. Take MDN out of maintenance mode (which will \"roll\" all of the Kuma-image-based deployments, but not the Kumascript-image-based deployment) and also manually roll the `kumascript` deployment since it uses Redis as well. As the new pods are spun-up, they'll pick-up and use the new Redis URL's.\r\n1. Delete the old Redis instance.\r\n\r\n### Acceptance Criteria\r\n- [ ] Stage Redis instance updated to use reserved instances of the latest server class \r\n- [ ] Prod Redis instance updated to use reserved instances of the latest server class "
      ]
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/cloudnativedaysjp/dreamkast-infra/issues/1260",
    "content": {
      "title": "migrate of synthetics monitoring service",
      "body": "### Description\r\n\r\nWe're currently using CloudWatch Synthetics for end-to-end monitoring.\r\n\r\nHowever, because CloudWatch Synthetics is expensive, we need to consider alternative solutions.\r\n\r\n### Todo in this issue\r\n\r\n- investigate some synthetics monitoring services.\r\n- migrate to new synthetics monitoring service with the same functionality/configuration as we have now",
      "comments": [
        "\u82f1\u8a9e\u306a\u304a\u3057\u305f",
        "Dk\u306eML\u3067Uptime Robot\u306e\u30a2\u30ab\u30a6\u30f3\u30c8\u4f5c\u3063\u305f\r\n\r\nhttps://uptimerobot.com/",
        "## Uptime Robot\r\nhttps://uptimerobot.com/\r\n\r\nPricing\r\nhttps://uptimerobot.com/pricing/\r\n\r\n- Free\r\n- Pro $15/month\r\n\r\n## Pingdom\r\nhttps://www.pingdom.com/\r\n\r\nPricing\r\nhttps://www.pingdom.com/pricing/\r\n\r\n- $10/month\r\n\r\n## Better Uptime\r\nhttps://betteruptime.com/\r\n\r\nPricing\r\nhttps://betteruptime.com/pricing\r\n\r\n- Basic(Free)\r\n- Fleelancer  $24/month\r\n\r\n## Freshping\r\n\r\nPricing\r\nhttps://www.freshworks.com/website-monitoring/pricing/\r\n\r\n- Free\r\n- Blossom $11/month  ",
        "Uptime Robot is LGTM.\r\n\r\nThere are exporter & dashboard for Uptime Robot. \r\n\r\n* https://github.com/lekpamartin/uptimerobot_exporter\r\n* https://grafana.com/grafana/dashboards/9955\r\n\r\nI create other PR to install exporter & dashboard, and close this issue.\r\n"
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/binbashar/le-tf-infra-aws/issues/50",
    "content": {
      "title": "Reduce apps-devstg/7_cloud-nuke exec window -> Every 24hs",
      "body": "### What?\r\nReduce apps-devstg/7_cloud-nuke exec window -> Every 24hs\r\n- https://github.com/binbashar/bb-devops-tf-infra-aws/blob/master/apps-devstg/7_cloud-nuke/main.tf\r\n- https://github.com/binbashar/bb-devops-tf-infra-aws/blob/master/apps-devstg/7_cloud-nuke/variables.tf\r\n\r\n```\r\nvariable \"cloudwatch_schedule_expression\" {\r\n  description = \"Define the aws cloudwatch event rule schedule expression, eg: everyday at 22hs cron(0 22 ? * MON-FRI *)\"\r\n  type        = string\r\n  default     = \"cron(0 00 ? * FRI *)\"\r\n}\r\n```\r\n\r\n### Why?\r\nAvoid incurring in not expected billing and costs.",
      "comments": []
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/binbashar/le-tf-infra-aws/issues/70",
    "content": {
      "title": "Cost Optimization | Implement schedule-stop-start lambda automation ",
      "body": "### What?\r\n\r\n- Add cloud scheduler stop start lambda automation based on EC2 tags to daily stop any shared account EC2\r\n- module: https://github.com/binbashar/terraform-aws-lambda-scheduler-stop-start\r\n\r\n### Why?\r\n\r\n- :moneybag: Avoid incurring on not necessary costs\r\n- :heavy_check_mark: **Pritunl VPN server** \r\n    - started: 06:00 AM\r\n    - stopped: 00:00 AM \r\n- :heavy_check_mark: **Jenkins Master server**\r\n    - started: manually from Web UI / awscli \r\n    - stopped: 00:00 AM ",
      "comments": []
    },
    "codes": [
      "networking",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/binbashar/le-tf-infra-aws/issues/86",
    "content": {
      "title": "Support terraform-aws-transit-gateway use case",
      "body": "### What?\r\n* Support terraform-aws-transit-gateway use case v\u00eda module: https://github.com/binbashar/terraform-aws-transit-gateway\r\n* Consider having a dedicated Network VPC deploy in the AWS Organization\r\n   * https://aws.amazon.com/blogs/networking-and-content-delivery/creating-a-single-internet-exit-point-from-multiple-vpcs-using-aws-transit-gateway/ \r\n\r\n![image](https://user-images.githubusercontent.com/18539704/83149816-f0187d80-a0d0-11ea-868b-a003764669d2.png)\r\n\r\n### Why?\r\n- :one: Simplified and consolidate network management w/ TGW since:\r\n   * It allows to create a network mesh with minimal configuration effort.\r\n   * Reuse the same VPN connection for multiple VPCs.\r\n- :two: The number of VPC Peering grows exponentially with the number of VPCs that you need to connect.\r\n- :three: Transit Gateway acts as a gateway connecting up to 5.000 networks.\r\n- :four: Cost optimization: \r\n   - If `N\u00b0 Site-to-Site VPN Cx >> N\u00b0 VPC peering` -> :heavy_check_mark: TGW \r\n   - If `N\u00b0 VPC peering  >> N\u00b0 Site-to-Site VPN Cx` -> :heavy_check_mark: VPC Peering  \r\n\r\n\r\n#### Considerations\r\n* Possibly having a dedicated Network Account in your AWS Org could be a good idea.\r\n* After attaching a VPC to a Transit Gateway, you need to update the routing tables of your subnets as well. \r\n* Simplified 1 x TGW to consolidate all Site-to-Site VPN Cxs + Additionally, defining custom route tables to configure the routing within the Transit Gateway.\r\n* Pricing:    \r\n   - Attaching a VPC to a Transit Gateway costs $36.00 per month.\r\n   - A VPN connection costs $36.00 per month.\r\n   - Traffic costs are the same for VPC Peering and Transit Gateway.\r\n\r\n#### Scenario: \r\n- 3 x VPC Peering (https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-basics.html)\r\n- 7 x Site-to-Site VPN Tunnels (https://aws.amazon.com/vpn/pricing/) \r\n\r\n#### :heavy_dollar_sign: TGW Price Calculation:\r\n```\r\n3 * VPC Peerings + 1 TW Attachment w/ VPN Direct Connect\r\n(3 * $36) + $36 + traffic\r\n     $108 + $36 + traffic\r\n           $144 + traffic \r\n```\r\n\r\n#### :heavy_dollar_sign: VPC Peering Price Calculation\r\n```\r\n(3 * VPC Peering) + 7 * Site-to-Site VPN Connections\r\n                0 + ($0,05 * 730hs) * 7 + traffic\r\n                                $36 * 7 + traffic\r\n                                   $252 + traffic\r\n```\r\n\r\n---\r\n\r\n#### Ref Articles:\r\n- https://cloudonaut.io/advanved-aws-networking-pitfalls-that-you-should-avoid/\r\n- https://aws.amazon.com/blogs/architecture/the-journey-to-cloud-networking/",
      "comments": [
        " @user As discussed will maintain the current peering approach for the already deployed accounts:\r\n- apps-devstg \r\n- apps-prd\r\n\r\nBut we'll need to create a new Network Account and deploy here the TGW through the module https://github.com/binbashar/terraform-aws-transit-gateway and allow a new account eg: \r\n- apps-devstg-twg \r\n- apps-prd-twg \r\n \r\nto route traffic to the shared account v\u00eda the Network Account TGW \r\n\r\nCC: @user  ",
        "To review along with @user and @user \n\n- NACL for SSM\n- SSM? But even SSH to provision via Ansible? Half that cancels out one of the pros of using SSM\n- Network NACLs => only for the TGW approach\n- shared NACLs, devstg, prd for TGW approach\n- shared, devstg, prd NACLs for the VPC Peering approach\n- Should VPC Peering and TGW be unified in one layer to conditionally define the NACLs?\n- Current use of network account + TGW => only for outbound of private subnets? should it be considered also for inbound? would it be cost effective?",
        "https://github.com/binbashar/le-tf-infra-aws/releases/tag/v1.3.33"
      ]
    },
    "codes": [
      "networking",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/binbashar/le-tf-infra-aws/issues/169",
    "content": {
      "title": "Feature | Cost Efficiency | Review and implement simple budget actions examples",
      "body": "## What?\r\n\r\nReview and implement simple budget actions example \r\n- https://thechief.io/c/news/aws-announces-new-budget-actions/\r\n- https://aws.amazon.com/blogs/aws-cost-management/get-started-with-aws-budgets-actions/\r\n\r\n## Why?\r\n- Improve our cost efficiency Well Architected Pillar \r\n- In a few words they can trigger actions when the established budget is exceeded\r\n- e.g. stop instances or attach policies or directly attach a workflow to ask for approval before proceeding\r\n\r\n",
      "comments": []
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/binbashar/le-tf-infra-aws/issues/154",
    "content": {
      "title": "Feature | Managing secrets in your Terraform code w/ Hashicorp Vault secret store ",
      "body": "## What?\r\nManaging secrets in your Terraform code w/ Hashicorp Vault secret store. In other words, storing your secrets in a dedicated secret store: that is, a database that is designed specifically for securely storing sensitive data and tightly controlling access to it.\r\n\r\n## Why?\r\n- Keep plain text secrets out of your code and version control system.\r\n- Your secrets are stored in a dedicated secret store that enforces encryption and strict access control (Auth + MFA).\r\n- Everything is defined in the code itself (versionable). There are no extra manual steps or wrapper scripts required (It only lives here). \r\n- Using a web UI to store secrets is a nice user experience with a minimal learning curve.\r\n- Secret stores typically support rotating secrets and versioning, which is useful in case a secret got compromised. You can even enable rotation on a scheduled basis (e.g., every 30 days) as a preventative measure.\r\n- Secret stores typically support detailed audit logs that show you exactly who accessed what data, hence auditable and compliant.\r\n- Secret stores typically expose an API that can easily be used from all your applications, and not just Terraform code.  \r\n- Create code snippets that show you exactly how to read your secrets from apps written in Java, Python, JavaScript, Ruby, Go, etc.\r\n- Favoring securing automation.\r\n- DR could be considered and granted by design.\r\n\r\n## How?\r\n#### Managing Vault w/ Terraform \r\n   - https://www.hashicorp.com/resources/vault-configuration-as-code-via-terraform-stories-from-the-trenches\r\n   - https://github.com/terraform-providers/terraform-provider-vault\r\n\r\n#### Inject secrets into Terraform using the Vault provider\r\n   - https://learn.hashicorp.com/tutorials/terraform/secrets-vault\r\n   - https://www.hashicorp.com/resources/best-practices-using-hashicorp-terraform-with-hashicorp-vault\r\n\r\n#### Read More\r\n- https://blog.gruntwork.io/a-comprehensive-guide-to-managing-secrets-in-your-terraform-code-1d586955ace1\r\n- https://blog.scalesec.com/a-comparison-of-secrets-managers-for-aws-ba64e8029314?gi=38d6db002f2a\r\n\r\n### Important Considerations\r\n#### Drawbacks to this technique\r\n- Since the secrets are not versioned, packaged, and tested with your code, configuration errors are more likely, such as adding a new secret in one environment (e.g., staging) but forgetting to add it in another environment (e.g., production).\r\n- Most managed secret stores cost money. For example, AWS Secrets Manager charges $0.40 per month for each secret you store, plus $0.05 for every 10,000 API calls you make to store or retrieve data.\r\n- If you\u2019re using a self-managed secret store such as HashiCorp Vault, then you\u2019re both spending money to run the store (e.g., paying AWS for 3\u20135 EC2 instances to run Vault in a highly available mode) and spending time and money to have your team deploy, configure, manage, update, and monitor the store.\r\n- Not as test friendly: when writing tests for your Terraform code (e.g., with Terratest), you will need to do extra work to write data to your secret stores.",
      "comments": [
        "@binbashar/leverage-ref-architecture-aws-admin @binbashar/leverage-ref-architecture-aws-dev \r\n\r\nTeam closing this issue with the following PRs\r\n- https://github.com/binbashar/le-tf-vault/pull/11\r\n- https://github.com/binbashar/le-tf-infra-aws/pull/289"
      ]
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/binbashar/le-tf-infra-aws/issues/214",
    "content": {
      "title": "Finance | AWS Activate credit request",
      "body": "## What?\r\n- \u2705 Apply to [AWS Activate](https://aws.amazon.com/activate/) in order to get credits for Leverage AWS Development and Demo Accounts. \r\n- \ud83d\udcd2 **Read More:** https://www.parkmycloud.com/blog/aws-credits/\r\n\r\n## Why?\r\n- Get financial support for https://leverage.binbash.com.ar development\r\n- From AWS Activate web\r\n> Build and scale with up to $100,000 in AWS Activate credits\r\n> AWS Activate provides startups with a host of benefits, including AWS credits*, AWS support plan credits, and training, to help grow your business. Activate benefits are designed to give you the right mix of tools and expert support so you can succeed with AWS while optimizing performance, managing risk, and keeping costs under control.",
      "comments": [
        "@binbashar/leverage-ref-architecture-aws-dev @binbashar/leverage-ref-architecture-aws-admin Team we've run out of AWS credit, so let be extra careful with our spendings (as usual). Thanks in advance! \r\n\r\n![image](https://user-images.githubusercontent.com/18539704/119715391-5518c780-be3a-11eb-99c9-0c85e9b8dc0a.png)\r\n",
        "We've applied to the Founders package: This package is for bootstrapped startups that aren't associated with an accelerator, incubator, venture capital firm, or other startup-enabling organization.\r\n\r\n![image](https://user-images.githubusercontent.com/18539704/136420199-673fa91d-8269-4795-9436-fb99c4ab06e5.png)\r\n\r\nRead More: https://aws.amazon.com/activate/contact-us/\r\n\r\nCC: @user @marceloberesvil @user "
      ]
    },
    "codes": [
      "awareness",
      "provider"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/binbashar/le-tf-infra-aws/issues/329",
    "content": {
      "title": "Feature | Scheduled Ref Architecture Code and Modules update process",
      "body": "## What?\r\n\r\n- Scheduled Ref Architecture Code and Modules update process, that for example uses https://github.com/minamijoyo/tfupdate and once a month automatically creates a PR to update: \r\n  - Update version constraints of Terraform core, providers, and modules\r\n  - Update all your Terraform configurations recursively under a given directory\r\n  - Get the latest release version from the GitHub, GitLab, or Terraform Registry\r\n- \ud83d\udcd2 **NOTE:** Currently, we are updating them manually as we come across each code.\r\n\r\n## Why?\r\n\r\n- \u2705 Interesting to have some kind of warning that says that there are new versions of the modules, something like what snyk or aqua has for checking programming language dependencies but in this case applied to TF module dependencies. And have the possibility to do an \"update\".\r\n- \u2705 Similar to what those services have, they build you a PR and then you can check and merge them. Maybe the added value of such a service would be to let us know that there are updates and at the same time show us some highlights of the features/fixes. Like to see some highlights of the features/fixes that we are missing out on by not upgrading.\r\n- \u2705 Get new AWS feature that allows to improve some aspect, like security, cost optimization, centralized management, etc.\r\n- \u2705 It would be interesting to know those changes.",
      "comments": [
        "We've tested along @user the `tfupdate` tool which automatically updates the terraform and provider versions code to its latest versions. We couldn't get the modules version to be updated with the tool. \r\n\r\n#### tfupdate output\r\n```\r\n\u256d\u2500 \uf179 \ue0b1 \uf07c ~/Binbash/repos/Leverage/ref-architecture/le-tf-infra-aws \ue0b1 on \uf113 \uf126 master \ue0b0\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\ue0b2 127 \u2718 \ue0b3 at 13:16:12 \uf017\r\n\u2570\u2500 docker run -it --rm \\\r\n-v /Users/exequielbarrirero/Binbash/repos/Leverage/ref-architecture/le-tf-infra-aws:/work \\\r\n--entrypoint=bash\r\nminamijoyo/tfupdate\r\n\r\nbash-5.1# cd apps-devstg/us-east-1/security-certs/\r\n\r\nbash-5.1# tfupdate terraform .\r\n```\r\n\r\n#### terraform code updated\r\n```terraform\r\n#=============================#\r\n# Backend Config (partial)    #\r\n#=============================#\r\nterraform {\r\n  - required_version = \">= 0.14.11\"\r\n  + required_version = \"1.1.9\"\r\n\r\n  required_providers {\r\n    - aws = \"~> 3.0\"\r\n    + aws = \"4.10.0\"\r\n  }\r\n\r\n  backend \"s3\" {\r\n```",
        "For the moment we'll focus the updates efforts at https://github.com/binbashar/le-tf-infra-aws/issues/370\r\n\r\nCC: @binbashar/leverage-ref-architecture-aws-admin @binbashar/leverage-ref-architecture-aws-dev "
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/cloudposse/terraform-aws-tfstate-backend/issues/15",
    "content": {
      "title": "Question on statefile for the backend",
      "body": "Is there a requirement that the tfstate file for the backend resources needs to be part of the deployment that uses the backend? I don't see anything in the docs regarding this but whenever I deploy with the s3/dynamo backend, terraform always tried to destroy the s3 state bucket and dynamodb lock table. I wouldn't think they are necessarily coupled but maybe it's a requirement by terraform? Makes for very messy deletion because you're deleting backend resource at the same time as everything else.",
      "comments": [
        " @user this sounds like maybe you're initializing the state backend, but then not importing the state. Please note this workflow is *different* from every other terraform module invocation. We have documented the process here: https://github.com/cloudposse/terraform-aws-tfstate-backend#usage and we've scripted it here: https://github.com/cloudposse/terraform-root-modules/blob/master/aws/tfstate-backend/scripts/init.sh\r\n \r\nBasically, the process is you provision the backends using terraform but without remote state. Then you re-run `terraform init` with backends configured and it will prompt to import the backend state to S3.\r\n",
        " @user If I'm understanding this correctly, the point of the script is to make the backend resources part of state. That is effectively what the second `terraform init` is doing. All resources that you later add, will also be a part of this state. You have one state file which has the backend resources plus anything else you add to your deployments. Thank you for your time btw",
        "That's sounds correct. One backend can be shared by multiple projects. Just use different key prefixes (e.g. `us-west-2/vpc/terraform.tfstate`). We recommend using one bucket per AWS account. Also, for true terraform-HA don't even share the bucket across regions.\r\n\r\n",
        "Okay, I think I'm sorta understanding how you would namespace the other backends. I'll just do a little experiment to confirm. Since you're storing the state file that manages the dynamodb and bucket for the backend itself in said bucket, you'd obviously run into problems when trying to delete the backend. And since deleting the backend shouldn't need to occur often I'm assuming that this is just a management cost that you'd have to deal with in the event that you actually need to do something with the backend.",
        " @user here's the process of destroying the bucket: https://github.com/cloudposse/terraform-root-modules/blob/master/aws/tfstate-backend/scripts/destroy.sh\r\n\r\n",
        " @user thanks for reporting your issues with the coldstart process and destruction. If you have further issues, please let us know! I'm going to close this issue for now."
      ]
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/cloudposse/terraform-aws-tfstate-backend/issues/64",
    "content": {
      "title": "Flag to only create s3 bucket and forego dynamodb creation in order to save money",
      "body": "Have a question? Please checkout our [Slack Community](https://slack.cloudposse.com) or visit our [Slack Archive](https://archive.sweetops.com/). \r\n\r\n[![Slack Community](https://slack.cloudposse.com/badge.svg)](https://slack.cloudposse.com)\r\n\r\n## Describe the Feature\r\n\r\nAn s3 bucket is much cheaper than dynamodb. For small projects with a single developer, it would be nice to only create the s3 bucket and forego the more expensive dynamodb database.\r\n\r\n## Expected Behavior\r\n\r\nFlag to only create s3 bucket and forego dynamodb creation in order to save money\r\n\r\nPerhaps `var.enable_dynamodb` and default it to `true`.\r\n",
      "comments": []
    },
    "codes": [
      "saving",
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/utoronto-2i2c/jupyterhub-deploy/issues/92",
    "content": {
      "title": "Cost Optimization of JupyterHub Service",
      "body": "Hi @user \r\n\r\n[Continuing our discussion from Team last week.... ]\r\nBroadly, the idea is to simulate (via Discrete Event Simulation) the cluster operation to find optimal system parameters (Node type, guarantee ratio, etc.) or optimal policies (how to configure node pools, etc. to exploit the pattern in user behavior) to minimize the cost while providing certain performance guarantee. \r\n\r\nRight now, I am learning about Prometheus and Grafana. I couldn't see the explore option on the Grafana. Could please help me with how to get the data. I am looking for data of users' arrival and departure from the system. ",
      "comments": [
        "Thanks for opening this, @user \r\n\r\n> I am looking for data of users' arrival and departure from the system.\r\n\r\nGrafana and Prometheus usually collect time series aggregate data - so you can ask for things like 'at this point of time, how many users were on the system?' rather than 'what happened at this minute'? Reading https://thenewstack.io/what-is-the-difference-between-metrics-and-events/ might help clarify the difference between metrics (which is what grafana has) vs events (which is what will provide the data you are looking for). If this is the specific dataset you want, I can look at the logs and produce that for you.\r\n\r\nFor optimization, I'd suggest reading up on the current work being done before starting. Some prior reading that might be useful:\r\n\r\n- https://github.com/kubernetes/autoscaler, which automatically sizes our cluster based on need. Configuring this is what ultimately optimizes cost, so understanding its limitations is vitally important to see what can be useful.\r\n- https://zero-to-jupyterhub.readthedocs.io/en/stable/administrator/optimization.html talks about how JupyterHub uses the autoscaler.\r\n- https://github.com/jupyterhub/zero-to-jupyterhub-k8s/search?q=placeholder&type=issues lists current conversation about 'pod placeholders', which is what JupyterHub uses to signal the autoscaler. This is what's in our control to modify, and what the JupyterHub community is actively working on.\r\n- https://discourse.jupyter.org/t/request-for-implementation-jupyterhub-aware-kubernetes-cluster-autoscaler/7669/15 was the result of the last time I looked at deep optimization.\r\n\r\nI hope that getting an awareness of the current state of implementable solutions helps form your research questions.\r\n\r\n"
      ]
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/fpco/terraform-aws-foundation/issues/103",
    "content": {
      "title": "Lightweight tests",
      "body": "Currently this repository has modules under `examples` that when executed will verify against AWS if modules are correct. This is the most robust and correct way to handle testing. But it suffers from a few problems:\r\n\r\n- Running it entails costs\r\n- If `examples` don\u2019t cover all `modules`, then coverage is less than 100%\r\n\r\nI propose adding a minimal test environment that can check the code _without_ running a `plan` or `apply`, by executing _at least and for every commit_:\r\n\r\n1. `terraform init` for _all_ modules\r\n1. `tflint`\r\n1. checking undeclared variable types\r\n1. checking for inline values\r\n\r\n## `terraform init`\r\nThe easiest way to do this is to create a `tests/main.tf` file that calls every single module once, with all parameters filled with the default for each type. That means an empty string for strings; empty list for lists; and empty map for maps. `terraform init` surprisingly doesn\u2019t care about this. It works. The same module is used for all next steps.\r\n\r\n## `tflint`\r\nRunning against the `init`ed `tests/main.tf` finds problematic code as soon as it\u2019s added. In my opinion we should treat `NOTICE` as OK to pass, but anything `WARNING` and above as a failed build.\r\n\r\n## Undeclared variable types\r\nThis checks if all variables have a type at declaration site. The following is an example of a valid snippet:\r\n\r\n```\r\nvariable \"foo\" {\r\n  type        = \"string\"\r\n  description = \"bar\"\r\n  default     = \"baz\"\r\n}\r\n```\r\n\r\nOmitting `type = \"string\"` is a test error. This guarantees that type errors are identical at every call site, as undeclared types will only be resolved at the callee, leading to potentially two modules resolving to different types. This also means that even though `default = []` will infer `type = \"list\"`, without adding it you will only find breakage if changing to another type once you use the changed variable at the call site.\r\n\r\nI have started patching [`terraform-index`](github.com/kerscher/terraform-index) such that variables return their types instead of only their names. When a type hasn\u2019t been explicitly set, it will list it as `undeclared`. Any `undeclared` should trigger a build error. This uses the `hcl` library used by `terraform` itself.\r\n\r\nOnce there are no more `undeclared`, I can use the patched `terraform-index` to auto-generate the `tests/main.tf` by using default values for every type and the list of variables for each module. The commit would be trivial.\r\n\r\n## Inline\r\nInline values prevent templates from being checked as individual files externally, but this is only a nuisance. The real danger is that forgetting to close an inline delimiter changes semantics of the entire `.tf` file. Check the example below:\r\n\r\n```\r\nmodule \"a\" {\r\n  source = \"some-path\"\r\n  param = <<EOF\r\nbar\r\nEOT\r\n}\r\n\r\nmodule \"b\" {\r\n  source = \"some-path\"\r\n  param = <<EOF\r\nbaz\r\nEOF\r\n}\r\n```\r\n\r\nThe following code above will `terraform apply` correctly, but `module.a.param` will not be `bar`, and `module.b` will never be created. In modules with nested resources or a large amount of modules this can easily pass unnoticed \u2014 specially common since `module`s take no `count`.\r\n\r\nA counter-argument is that syntax highlight could potentially showcase this problem. But this cannot be checked without human intervention, as a checker cannot know if `module.b` should or not be created.\r\n\r\n# Implementing\r\n\r\nAdding under `scripts/ci/*.sh` tests that call each of the things above. Once an image has been created with the dependencies in the tests, we can add it to Travis and add a badge with build status. This last step is by far the easiest part.",
      "comments": [
        "This is really great, I love what you have done here. I don't have much feedback to add, but I'll drop what I do have in the PR.",
        " @user I would love to hear your thoughts on making these tests robust. Initially I added shell scripts, but converting to Haskell program(s) should be relatively easy.\r\n\r\nBut I could not think of a way to express invariants that [`validity`](github.com/NorfairKing/validity) would accept :grinning:.",
        "> @user I would love to hear your thoughts on making these tests robust. \r\n\r\nI'm very glad you asked!\r\n\r\n> Initially I added shell scripts, but converting to Haskell program(s) should be relatively easy.\r\n\r\nFunnily enough, that's actualy the only review comment I gave on the PR, and I read the PR before I read your question here.\r\nI'm more opposed to bash than I am in favour of Haskell in this case, and the only cheap way I can think of making these robust involves two things:\r\n\r\n- An idiot-proof guide to running the tests manually.\r\n- Running the tests on CI.\r\n\r\nI would love it if we could have more comprehensive tests on CI, but that would be rather expensive, so I would make those manually trigered, but I guess that's a seperate issue."
      ]
    },
    "codes": [
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/fpco/terraform-aws-foundation/issues/111",
    "content": {
      "title": "Rewrite test suite in \u201creal\u201d programming language",
      "body": "The tests under `scripts/ci` are written in GNU Bash, and need some external binaries in place to be executed. Their logic warrants a language that can provides more guarantees that our checks will be correct. This issue is for specifying and keeping track of this refactoring.\r\n\r\n# Requirements for the test suite program(s)\r\n\r\n- _Should_ be written in a language with safer guarantees than GNU Bash\r\n- _Should_ be testable\r\n- External dependencies _can_ be exchanged for language libraries where appropriate\r\n- When external dependencies are used, not having them _should_ exit and trigger a build error immediately\r\n- Output _can_ be parseable by other tools \u2014 e.g. JSON\r\n- Test suite _should_ provide entries for errors, warnings and notices\r\n  - Any errors _must_ trigger a build failure\r\n  - Warning _can_ trigger a build failure if user runs with an enforcing option, but by default _shouldn\u2019t_\r\n  - No notice _should_ trigger a build failure\r\n- There _must_ be a subset of the test suite that can be executed on every commit without incurring AWS costs\r\n- There _should_ be a subset of the test suite than verifies code against AWS APIs\r\n  - Any error from the AWS APIs _must_ trigger a build error\r\n- Automatically generating full module coverage _can_ be provided, using HCL AST parser to do code generation\r\n\r @user see if this represents a good start on converting to a more robust test suite :point_up_2: ",
      "comments": [
        " @user Love it.\r\n\r\n> Should \r\n\r\nSomething something mention the RFC.\r\n\r\n> When external dependencies are used, not having them should exit and trigger a build error immediately\r\n\r\nThis could become hard to test.",
        "By external dependencies I mean binaries to execute. This should be easy to test, no? Check `PATH` from the env var and if the binary is there and is executable by the user. If it isn\u2019t, exit with an error and print out the binary missing. If you want to avoid many commits to fix those, check all binaries you will call.\r\n\r\nIn fact, if using something like [`RIO.Process.withProcess`](https://www.stackage.org/haddock/nightly-2018-04-23/rio-0.1.1.0/RIO-Process.html#g:5), it will happen by default, no? Though we could provide a smart constructor to `ProcessConfig`, so only \u201cregistered\u201d binaries can be called. Then at entry point you validate them before running anything else. When it reaches `withProcess`, you\u2019re sure the binary is there.",
        "> This should be easy to test, no? \r\n\r\nTesting this would include removing each of the binaries in turn to see if we now trigger a build error. \r\n\r\nWhat you describe is the implementation, not the test.\r\n\r\nIn any case, this is not something that's likely to be implemented wrong, as you point out in\r\n\r\n> In fact, if using something like RIO.Process.withProcess, it will happen by default, no? "
      ]
    },
    "codes": [
      "awareness",
      "provider"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/jenkins-x/terraform-aws-eks-jx/issues/184",
    "content": {
      "title": "Jenkins-X3 on EKS private subnets / Bastion / OpenVPN",
      "body": "### Summary \r\nHi guys, James Strachan adviced me to create a ticket here.\r\n\r\nI am trying to create a very secure prod ready solution. I need the EKS cluster to be on private subnets and to allow it to be accesible via a simple `bastion HOST ec2 instance`. And the bastion Host to be accesible via OpenVPN. \r\n\r\nI will try to experiment with:\r\n```\r\n\r\n  cluster_endpoint_private_access = true\r\n  cluster_in_private_subnet       = true\r\n```\r\n(somehow I need HA-Vault external one- no eks), will see how it goes.\r\n\r\nIn general it's a good Idea to have such think by default, lot's of banks has such requirements in our days. No extra cost at all, OpenVPN is free for 2-3 users I think.\r\n\r\nI don't know if that's possible with Jenkins-X3 and I don't know if Jenkinsx3 it's compatible for such think at all(how it will behave in a closed env), may I ask for any advice, guidance?\r\n\r\n\r\n",
      "comments": [
        " @user I will work on this issue soon (starting next week): https://github.com/jenkins-x/terraform-aws-eks-jx/issues/158\r\nBasically create a secure eks cluster by following the recommendations from aws.\r\n",
        "/assign",
        " @user  Appreciate! Happy to contribute as well, try things if requires testing or.. \r\nenjoy the weekend!\r\nCheers!",
        "Issues go stale after 90d of inactivity.\nMark the issue as fresh with `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with `/close`.\nProvide feedback via https://jenkins-x.io/community.\n/lifecycle stale",
        "Stale issues rot after 30d of inactivity.\nMark the issue as fresh with `/remove-lifecycle rotten`.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with `/close`.\nProvide feedback via https://jenkins-x.io/community.\n/lifecycle rotten"
      ]
    },
    "codes": [
      "networking",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/CodeForPoznan/Infrastructure/issues/57",
    "content": {
      "title": "Change S3 Object zip creation of empty lambda body",
      "body": "How about instead of creating a auto-generated zip placeholder for each lambda we instead create a named zip file? Django/flask projects will have at least two lambdas - for migration and for API. Each of these contains the same code, packed the same way, with same deps.\r\nI see a point in having duplicated zips on S3 if the lambdas underneath will use the same one anyway except the simplicity in 1:1 mapping of lambda to zip.",
      "comments": [
        "That would turn out to be way more complicated on terraform side and would break the current name convention. The duplicates aren't that bad anyway (30MB max) and S3 is cheaper than added configuration so yeah, a clever but also probably too clever solution for not-really-that-important problem. Closing this one."
      ]
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/CodeForPoznan/Infrastructure/issues/86",
    "content": {
      "title": "Use bastion as a NAT Gateway Instance",
      "body": "The reasoning here is as follows:\r\n\r\n- We have a single VPC (default one) and a single RDS database which lives inside of that VPC.\r\n- Database is private, no point of contact exists outside of VPC, thus our Lambdas (meaning - apps like pah, cfpv3) have to live inside of that VPC as well, which they do.\r\n- Every lambda that lives inside of a VPC loses its internet access unless the VPC has Internet Gateway assigned, which in our case it doesn't.\r\n\r\nOur default VPC does not have such gateway, because the cost of recommended managed gateway is astonishingly high ($35) compared with our current expenses ($15), which also means that our Lambdas do not have internet connection.\r\n\r\nThis didn't matter to us until recently, when we tried to introduce SES into cfpv3. We tried to interact with it in two ways:\r\n- via SMTP connection\r\n- with Boto3 (REST API)\r\n\r\nboth of these options require public internet access, because the API endpoints are internet facing.\r\n\r\n\r\nIn order to use SES and not lose the connection to DB we have to choose one of the following:\r\n\r\n1. create AWS Managed Internet Gateway (simplest but also the most expensive option)\r\n2. create Gateway Instance (bit more troublesome, might be insufficient in distant future, very cheap)\r\n3. create VPC Endpoint for SES (simple, very limited compared to either of the above, not-great, not-terrible price)\r\n4. move the DB out of VPC, assign DNS name, block unwanted traffic at the Security Group (risky, against good practices, vulnerable, free)\r\n\r\n\r\nI spent the last few days learning about all of above options and to me it seems like the GW Instance (2) is the best one.\r\nWe would retain the full internet access, which would definitely come in useful later on when we'd work with Github and Auth0. \r\n\r\nReasons against other options:\r\n- (1) at least two times more expensive than our current monthly bill for the whole infrastructure. It might not be a lot on a larger scale, but for now that is a significant cost for a very small gain (only one lambda needs it and only for one, narrow purpose).\r\nIt's safe and scalable but that comes at too high price in my opinion. \r\n\r\n- (3) This option is very limiting - VPC Endpoint would let us use SES in Lambda, but only that and nothing else. If we'd want to use other AWS Services we'd have to set up another Endpoint, and the problem with no internet access would still stand (which can be solved only by 1 or 2 AFAIK). We need internet access for future integrations.\r\n\r\n- (4) Moving the DB out is risky. It's easy to mess this up and leave the DB open to attacks, even if we set up some long secure password, also - we can't really limit the traffic between lambdas and DB itself tightly due to different management levels they are using, so the DB would be always at least a bit unsafe, especially considering that the Infra code is public.\r\n\r\nThe 2 option has an added bonus of saving money, because we could convert currently used bastion host to a GW and still retain it's purpose as a jump host. That's super cool because we're paying for it anyway, and it's $4/mo, for just sitting there and not doing much anyway. Upper limit of data transfer is somewhere around 5Gb/s, which is plenty for our use case. We could also buy a reserved instance as we did for the RDS to cut the price even more. See this link for details: https://www.kabisa.nl/tech/cost-saving-with-nat-instances/\r\n\r\nThe only problem with it is that it's:\r\n- single place of failure for the outgoing connections (meaning SES + other future integrations)\r\n- managed by us, which can have its down sides if someone decides to reboot or switch it off\r\n- sparse visibility if something actually goes wrong (we could just treat is as a black box and deal with it that way)\r\n\r\nI still think that it's the best option, for the time being. Obviously I'd love to have the managed GW and just forget about the problem but the cost forces us to consider other alternatives, and the GW Instance is the closest we can get for reasonable price.",
      "comments": [
        "I'm happy to hear other opinions - @user  @user I created that ticket to ask for a green/red light on that conversion of bastion to GW.",
        "We should probably check if the AWS grant we are planning to get includes coverage for option 1 (for current or future reference)"
      ]
    },
    "codes": [
      "networking",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/opszero/terraform-aws-kubespot/issues/68",
    "content": {
      "title": "AWS / Security / Flow Logs",
      "body": "- [ ] VPC\r\n   - [ ] Flow Logs\r\n\r\nEnable VPC flow logs. Public to CloudWatch or S3. (be aware there is additional CloudWatch costs for VPC flow logs) Docs",
      "comments": []
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/commitdev/zero-aws-eks-stack/issues/114",
    "content": {
      "title": "Enable Encryption and SecureTransport on S3 buckets by default",
      "body": "1. S3 buckets for assets and Terraform state should have encryption enabled by default through the aws provider resource:\nhttps://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket#enable-default-server-side-encryption\n\n2. And though we don't really access buckets directly we should restrict traffic to HTTPS as well.\nFor this we can add `aws:SecureTransport` to the bucket policy: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket_policy\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition_operators.html#Conditions_Boolean\n",
      "comments": [
        "List of resource o f\"aws_s3_bucket\":\n- modules/s3_hosting/main.tf:r esource \"aws_s3_bucket\" \"client_assets\"\n- modules/cloudtrail/main.tf: resource \"aws_s3_bucket\" \"cloudtrail\"\n- bootstrap/remote-state/main.tf: resource \"aws_s3_bucket\" \"terraform_remote_state\" ",
        "By default, the log files delivered by CloudTrail to your bucket are encrypted by Amazon server-side encryption with Amazon S3-managed encryption keys (SSE-S3). To provide a security layer that is directly manageable, you can instead use server-side encryption with AWS KMS\u2013managed keys (SSE-KMS) for your CloudTrail log files.\n\nAfter certain experimental and discussions, we decided to use default SSE-S3 rather than SSE-KMS (which introduces extra maintenance cost for startups)."
      ]
    },
    "codes": [
      "awareness",
      "increase",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/commitdev/zero-aws-eks-stack/issues/166",
    "content": {
      "title": "Investigate using NAT instances instead of NAT gateways to save cost",
      "body": "It could be cheaper to use NAT instances if we use small instance classes, though we would potentially introduce issues for people who need a lot of outbound traffic from their cluster. Could be something we prompt for during setup - let them choose..",
      "comments": [
        "Specs:\n- NAT Instance suggested for Staging environment only, while keep NAT gateway for Production\n- One NAT instance per AZ (optional)",
        "Spec:\n- Enable S3 VPC endpoint to save cost for S3 access ",
        "Put NAT instance as part of VPC module, with a controller `enable_nat_gateway`. When it is set as false, create NAT instance instead of NAT gateway.",
        "Allow user to define own NAT instance type (default: t3.nano)"
      ]
    },
    "codes": [
      "networking",
      "saving",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/commitdev/zero-aws-eks-stack/issues/184",
    "content": {
      "title": "Switch nginx ingress to use NLB",
      "body": "The current setup uses ELB for the nginx ingress controller, but there would be some benefits to switching over to NLB:\n- With the current setup we don't get remote IPs in nginx logs\n- We are already using L4 only on the ELB so why not, when NLB is better and more efficient at handling it.\n\n\nThe change should be straightforward, it will just require some testing.\n\nThese annotations need to be added to the ingress service:\n```\n    service.beta.kubernetes.io/aws-load-balancer-type: nlb\n    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp\n    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: '60'\n    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: 'true'\n```\n\nThe nginx default keepalive timeout is 75s but let's also specify that explicitly in the code, maybe with a comment telling people that the lb timeout must be shorter than the nginx keepalive.\n\nAfter this change, make sure that the nginx logs show the proper remote IP address, not an internal `10.` address.\nMake sure TLS still works properly. \n",
      "comments": [
        "Nice to go this way.\r\n\r\nThinking: currently we have wireguard VPN service using a NLB, wondering that if we can merge it into the new NLB **for less cost**, although it could be tricky.",
        "Unfortunately I'm not sure we'll be able to since the LB is created dynamically by the ingress controller. You could see if there's a configuration option to specify an existing LB though. ",
        "Just a quick investigation: no configuration specific for such \"reuse\" yet, but one issue requested [Create option to reuse an existing ALB instead of creating a new ALB per Ingress](https://github.com/kubernetes-sigs/aws-load-balancer-controller/issues/298). Should be a long waiting, a guy created this interesting [Ingress-Merge](https://github.com/jakubkulhan/ingress-merge) for solution."
      ]
    },
    "codes": [
      "saving",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/commitdev/zero-aws-eks-stack/issues/186",
    "content": {
      "title": "Investigate / Implement spot instance support for EKS cluster with node termination handler",
      "body": "I recently saw an AWS tool called the [node termination handler](https://github.com/aws/aws-node-termination-handler#instance-metadata-service-processor) which can automatically handle node draining due to events like spot instance termination notices.\nThis may make it feasible to use spot instances for an EKS cluster for cost savings.\n\nWe would still need to do some investigation though to make sure it works properly without disrupting an environment, especially in a case where a cluster may be running near capacity. \n\nFor example, if you have a single node which is near capacity and it gets a termination notice, I would expect that it would:\n- attempt to drain the node\n- cluster autoscaler would see that pods were being rescheduled, but there's not enough capacity\n- cluster autoscaler increases desired capacity by 1\n- node comes up, workloads are scheduled\n- drain completes, node is terminated\n\nI feel like some of these things are going to be a little tricky though, for example if the new node doesn't come up in time for rescheduling to happen before termination of the previous node.\n\nIf it all works well we would need to add spot instances as an option per environment plus some documentation and recommendations around how/when to use this feature and potential cost savings.\n",
      "comments": []
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/commitdev/zero-aws-eks-stack/issues/173",
    "content": {
      "title": "Add option to enable/disable cloudtrail for cost savings",
      "body": "It can add $5-10 per month and may not provide much value to a smaller company so let's let people start without it if they want.\nWe can prompt for this during init and default to no",
      "comments": [
        "## Planned work\r\n\r\n- [ ] add a configuration option to the module definition to capture the option.\r\n\r\n- [ ] conditionally include the include/exclude the module in [main.tf](https://github.com/commitdev/zero-aws-eks-stack/blob/0ddb1e74f5e54789c1cdb9ee5d3774001a9522f1/templates/terraform/environments/shared/main.tf)",
        "Note that there's also some code in `user_access.tf` that references the cloudtrail bucket id that will need to be made conditional.",
        "Also just to clarify, the option doesn't need to be added to the CLI directly, just to the module definition file.",
        "> Not that there's also some code in `user_access.tf` that references the cloudtrail bucket id that will need to be made conditional.\r\n\r\nsaw the reference when I initially searched for the cloudtrail keyword in the repo; \r\nbut wasn't sure if I needed to change it. thanks for clarifying \ud83d\udc4c",
        "Do we use `count`controlled by a variable  to configure this module?",
        "Not at the moment, that's what will need to be added, as well as a zero module parameter so that the user can decide to enable or disable this feature. The value of the parameter can be templated into [shared/main.tf](https://github.com/commitdev/zero-aws-eks-stack/blob/0ddb1e74f5e54789c1cdb9ee5d3774001a9522f1/templates/terraform/environments/shared/main.tf) as a `local` and then used in a `count` statement."
      ]
    },
    "codes": [
      "saving",
      "feature",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/cds-snc/forms-terraform/issues/130",
    "content": {
      "title": "WAF: add resources to Shield Advanced",
      "body": "# Summary\r\nEnabling Shield Advanced will provide DDoS protection to our resources.\r\n\r\nAn example of how to do this can be see in Articles:\r\n* [Enroll resources](https://github.com/cds-snc/gc-articles/blob/6a1491ecc27d3308f7b6d65853a149741fb6e33a/infrastructure/terragrunt/aws/alarms/shield_advanced.tf)\r\n* [Add DDoS alarms](https://github.com/cds-snc/gc-articles/blob/6a1491ecc27d3308f7b6d65853a149741fb6e33a/infrastructure/terragrunt/aws/alarms/cloudwatch_ddos.tf)\r\n\r\n# Notes\r\nThere is no additional cost for this - the subscription is paid once at the org account level.",
      "comments": []
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/dfds/infrastructure-modules/issues/64",
    "content": {
      "title": "Persistence storage with CSI driver and AWS EFS",
      "body": "",
      "comments": [
        "The CSI driver in combination with AWS EFS solves the issue that AWS EBS has with being hard coupled to AZ at the prize of higher cost and no dynamic provisioning.",
        "Documentation:\r\nhttps://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html\r\nhttps://github.com/kubernetes-sigs/aws-efs-csi-driver",
        "Requirement 1.14 of AWS EKS.",
        "I had a chat with Willi and it makes sense now.  Right now we run the CSI Drive for EBS Volumes which means the developers can in theory provision persistent volumes using EBS.  The problem with this is that these volumes are AZ bound so we don't have any sort of fault tolerance on the data; if we lose an AZ then the data is gone.\r\n\r\nThe CSI Driver for EFS should, in theory, allow developers to provision EFS-volumes which are not AZ constrained. \r\n\r\nOff the back of this I need to do some more research.  Some of the CSI stuff is now offered as an EKS-Addon so it might be one approach, or the other alternative is that I start by trying a manual install of the EFS CSI Driver in my sandbox and see how it looks to actually provision and work with EFS-backed volumes.",
        "The work around this is now done, but the implementation differs from that used by the AWS CSI Driver for EBS so i've written some notes below to give a bit of explanation around the differences.\r\n\r\n**AWS CSI Driver for EBS**\r\n\r\nThe AWS CSI Driver for EBS is built in such a way that anyone can provision an EBS volume by simply defining a Persistent Volume Claim using a YAML manifest like this:\r\n\r\n```\r\napiVersion: v1\r\nkind: PersistentVolumeClaim\r\nmetadata:\r\n  name: test-disk\r\nspec:\r\n  storageClassName: csi-gp2\r\n  accessModes:\r\n    - ReadWriteOnce\r\n  resources:\r\n    requests:\r\n      storage: 10Gi\r\n```\r\n\r\nWhen such a manifest is applied in the cluster the driver will automatically ensure that a suitable EBS volume is provisioned.\r\n\r\n\r\n**AWS CSI Driver for EFS**\r\n\r\nThe AWS CSI Driver for EFS does not provide a means for an individual to provision an Elastic File System via YAML manifests.  Instead they have to be defined in Terraform and once created a Kubernetes Storage Class will also be defined which links to the file system.  Those wishing to use the EFS volume can then do so by referencing the appropriate storage class as shown in the sample below.\r\n\r\n```\r\napiVersion: v1\r\nkind: PersistentVolumeClaim\r\nmetadata:\r\n  name: efs-claim-1\r\nspec:\r\n  accessModes:\r\n    - ReadWriteMany\r\n  storageClassName: csi-efs-test-volume-1\r\n  resources:\r\n    requests:\r\n      storage: 5Gi\r\n``` \r\n\r\n\r\n**Installing the new AWS CSI Driver for EFS**\r\n\r\n1. Increment the reference to infrastructure-modules in both the Service and Cluster phase to **version** or later.  Executing an apply of the cluster phase and the service phase at this point would cause the CSI Driver for EFS to be deployed.  By default the latest version of the Helm Chart will be used unless a specific version is provided in the variable efs_csi_driver_chart_version.\r\n2. It is recommended that before applying the two phases you add a section of code similar to this in the HCL file for the Services phase.  Please note that you may need to increment the version if later releases have subsequently been made available:\r\n\r\n```\r\n  # --------------------------------------------------\r\n  # AWS EFS CSI Driver\r\n  # --------------------------------------------------\r\n\r\n  efs_csi_driver_chart_version = \"2.2.2\"\r\n```\r\n\r\n3. Once the code is updated then apply both the cluster and services phase.  This is necessary because there is a small change made to one of the IAM Roles which is provisioned during the cluster phase.  Failing to make this change will prevent pods from being attached to their requested file systems.\r\n\r\n4. The k8s-services portion of infrastructure-modules also includes a module reference which will create an EFS File System named 'volume-1'.  This will automatically result in a storage class called csi-efs-volume-1 being created.  Users of the cluster can then make use of this storage using a YAML manifest as shown below.\r\n\r\n```\r\n---\r\napiVersion: v1\r\nkind: PersistentVolumeClaim\r\nmetadata:\r\n  name: efs-claim-1\r\nspec:\r\n  accessModes:\r\n    - ReadWriteMany\r\n  storageClassName: csi-efs-volume-1\r\n  resources:\r\n    requests:\r\n      storage: 5Gi\r\n---\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: efs-app-1\r\nspec:\r\n  containers:\r\n    - name: app\r\n      image: centos\r\n      command: [\"/bin/sh\"]\r\n      args: [\"-c\", \"while true; do echo $(date -u) >> /data/out; sleep 5; done\"]\r\n      volumeMounts:\r\n        - name: persistent-storage\r\n          mountPath: /data\r\n  volumes:\r\n    - name: persistent-storage\r\n      persistentVolumeClaim:\r\n        claimName: efs-claim-1\r\n```\r\n"
      ]
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Quansight/qhub/issues/132",
    "content": {
      "title": "On-premise configuration documentation",
      "body": "Hello!\r\nWondering where I can find documentation or examples to deploy and configure qhub on-premise infrastructure.\r\nWould like to test it in a personal server before cloud.\r\nThanks!",
      "comments": [
        "Great question. There isn't currently documentation or ability to do this with the current state of QHub, but it is something that that has been discussed. The thought is to use minikube and have Terraform create a local cluster.\r\n\r\nIn the short term, you might consider spinning this up on Google Cloud since you get $300 in credits when you first sign up. The default configuration will run from ~$20-$100/month in compute costs depending on how many user nodes / dask workers you spin up. Running `terraform destroy` in the `infrastructure` and `state` folders once you're done testing would delete all the infrastructure.",
        " @user Expanding on @user comment.\r\n\r\nOn-Prem kubernetes is a pain and doesn't quite fit our target audience of folks who do not have strong in house devops. We have a sister project called qhub-hpc (that is not yet open sourced) that allows you to go from bare metal to an HPC setup with QHub integration. We are also experimenting with things like Nomad and k3s for an op-prem Kubernetes story. \r\n\r\nWe are also working on a pr to make testing on-prem with minikube easier.",
        "Hey @user \n\nhope you're doing good! Thank you so much for reaching out to us.\n\nI am sorry that we could not be more helpful. I will go ahead and close this for now, though. Feel free to reopen in case something else pops into your mind.\n\nTake care."
      ]
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Quansight/qhub/issues/227",
    "content": {
      "title": "QHub shutdown feature",
      "body": "> Similar issue/solution mentioned on #99 \r\n\r\n## Description\r\nAs a user, I would like to be able to deactivate my notebooks without the need to shut them down and lose all my disks, but they are also expensive to just keep it running.\r\n\r\n## Suggestion\r\nA QHub shutdown CLI feature, which would keep the deployment state (i.e., persist the disks) but deactivate all other resources, removing the waste of resources.\r\n\r\n## Context\r\nThis would be a useful feature for the community since spinning clusters is expensive and having a command to simply 'suspend' the allocated resources without major downsides would be quite useful (internally and externally).\r\n\r\n### To be discussed:\r\n- [ ] Where in the roadmap this fits?\r\n- [ ] Implementation ideas (check [comment](https://github.com/Quansight/qhub/issues/227#issuecomment-755384525) with suggestion).\r\n\r\n",
      "comments": [
        "So I think there may be some issues with shutting down while keeping the storage. The storage is associated with the kubernetes cluster itself via pvcs. Ideally this comes down to having a good backup and restore operation.\r\n\r\nId's consider this issue to be resolved by https://github.com/Quansight/qhub/issues/99 when it is completed.",
        "Here are my thoughts on how we could make this work by backing up the shared volume to block storage. These would be the steps:\r\n- backup NFS to block storage\r\n- destroy old QHub\r\n- deploy new QHub when ready\r\n- restore data from block storage to NFS\r\n\r\nThe NFS volume is quite slow, so it could take upwards of several hours for the data to restore. I wouldn't recommend this solution unless the cluster wasn't needed for more than a month. Diagram below.\r\n\r\n![Blank diagram](https://user-images.githubusercontent.com/49161327/103788870-f5458f80-5004-11eb-9742-cdfade1562e2.png)\r\n",
        "> Id's consider this issue to be resolved by #99 when it is completed.\r\n\r\nAwesome! I will mark this as a #99's duplicate then and close the issue."
      ]
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Quansight/qhub/issues/321",
    "content": {
      "title": "Node type assignment",
      "body": "Pods for some core services have migrated over to high-memory nodes, which have a much higher cost that the general nodes.  I tried killing the pod hoping it would restart on a different node, but usually it just restarts on the same node.\r\nI was able to temporarily correct the issue with the following command:\r\n`kubectl drain <pod> --ignore-daemonsets`\r\nThese services need Node-Selectors to ensure proper assignment to general nodes:\r\n- conda-store\r\n- nfs server\r\n- dask-gateway",
      "comments": [
        "Fixes for conda-store and nfs-server are available in #323 and https://github.com/Quansight/qhub-terraform-modules/pull/39\r\nFix for dask-gateway is available in https://github.com/Quansight/qhub-terraform-modules/pull/37",
        "This happens daily due to prefect jobs spinning up resources and causes increased costs of the cluster at rest since an extra high-memory node stays running until I run the command above.\r\n\r\nIn addition to the suggested fixes above, I think the proper fix is to set a taint on the nodes to prevent other workloads from being scheduled on them, as seen in 2i2c's terraform setup [here](https://github.com/2i2c-org/pilot-hubs/blob/master/terraform/main.tf#L141-L154).\r\n\r @user  any chance we can get this added to the next release?",
        "Added to the upcoming release milestone. @user if you feel confident in adding another PR to set the taints go for it.",
        " @user closing this issue. Let us know if merging that PR and the new release does not fix the issues."
      ]
    },
    "codes": [
      "cluster",
      "increase"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Quansight/qhub/issues/463",
    "content": {
      "title": "Build a table comparing the cost options between the different Cloud Providers s...",
      "body": "https://github.com/Quansight/qhub-cloud/blob/82cb87e24941f632a7e1999d3a5f2c295222d7a4/docs/source/05_reference/03_base_cost.md#L4\n\n---\n\n###### This issue was generated by [todo](https://todo.jasonet.co) based on a `TODO` comment in 82cb87e24941f632a7e1999d3a5f2c295222d7a4 when #441 was merged. cc @user ",
      "comments": [
        "This issue has been automatically marked as stale because there was no recent activity in 60 days. Remove the stale label or add a comment, otherwise, this issue will automatically be closed in 7 days if no further activity occurs.",
        "This issue was closed because it has been stalled for 7 days with no activity."
      ]
    },
    "codes": [
      "awareness",
      "provider"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Quansight/qhub/issues/729",
    "content": {
      "title": "[bug] Jupyterlab instances die 15 mins after the dask-cluster terminates",
      "body": "## Describe the bug\r\nA clear and concise description of what the problem is.\r\n\r\nIn the past week I have spun up many dask-clusters. And from what I can tell, each worker runs on their own node. However after the cluster has completed running (and no new clusters are subsequently launched), the users jupyterlab instance will get killed. This has happened many times while I was in the jupyterlab itself actively working on a notebook. \r\n\r\nThe amount of time between the cluster terminating and my jupyterlab instance dying is remarkably consistent at just about 15 minutes. \r\n\r\nFrom k9s pods point-of-view, I see the `dask-worker` pods terminate and drop away but the `aws-node` and `kube-proxy` pods stick around for the 15 mins. At the 15 minute mark after the cluster terminated, these `aws-node` and `kube-proxy` pods associated with my jupyterlab instance will also die.\r\n\r\nAlthough at this point I have little to go on, it feels like to me that there might be a process which cleans up \"unused\" pods/nodes and it believes my jupyterlab instance is such a pod/node. \r\n\r\nFor reference, we have two node groups, `general` and `user_worker`. This `user_worker` node group is used for both the jupyterlab instances and dask-workers (as a cost saving measure because AWS prevents us from full-scaling down all nodes in a node group, see [issue 595](https://github.com/Quansight/qhub/issues/595)).\r\n\r\nAlso our idle culler is set as follows in the `jupyterhub.yaml`:\r\n\r\n```\r\ncull:\r\n  enabled: true\r\n  timeout: 1800 # cull after 30min of inactivity\r\n  every: 600\r\n```\r\n\r\n## Your environment\r\nDescribe the environment in which you are experiencing the bug.\r\n> Include your conda version (use `conda --version`), k8s and any other relevant details.\r\n",
      "comments": [
        ">  I have spun up many dask-clusters. \r\n\r\nDoes it happen with any dask cluster as in cluster of any size?",
        "I will try to narrow the number down to night but I've noticed when I request 20 or fewer workers, things usually seem fine. However whenever I request many workers, 50+, I can almost guarantee that my jupyterlab instance will get killed. ",
        "Log from scheduler:\r\n\r\n```\r\n\u2502 distributed.core - ERROR - Exception while handling op heartbeat_worker                                                                                                                                                                    \u2502\r\n\u2502 Traceback (most recent call last):                                                                                                                                                                                                         \u2502\r\n\u2502   File \"/home/conda/store/91de068ec9f7aa232681e9e91b95d44050f0b94fec0deb15da53ce08bc6d63ed-DT/lib/python3.8/site-packages/distributed/core.py\", line 494, in handle_comm                                                                \u2502\r\n\u2502     result = handler(comm, **msg)                                                                                                                                                                                                          \u2502\r\n\u2502   File \"/home/conda/store/91de068ec9f7aa232681e9e91b95d44050f0b94fec0deb15da53ce08bc6d63ed-DT/lib/python3.8/site-packages/distributed/scheduler.py\", line 3887, in heartbeat_worker                                                     \u2502\r\n\u2502     ws._executing = {                                                                                                                                                                                                                      \u2502\r\n\u2502   File \"/home/conda/store/91de068ec9f7aa232681e9e91b95d44050f0b94fec0deb15da53ce08bc6d63ed-DT/lib/python3.8/site-packages/distributed/scheduler.py\", line 3888, in <dictcomp>                                                           \u2502\r\n\u2502     parent._tasks[key]: duration for key, duration in executing.items()                                                                                                                                                                    \u2502\r\n\u2502 KeyError: 'concat_parts-0df3a85c-6b0f-4d82-a7d7-4e8730766abb'\r\n```",
        "From my work with @user and @user  this issue was resolved by suspending the `AZRebalance` feature of the EC2 auto-scaling group, [here AWS documentation here](https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html)."
      ]
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Quansight/qhub/issues/595",
    "content": {
      "title": "[enhancement] Set `min_node: 0` for QHub deployed on AWS",
      "body": "## Description\r\nThe new datum QHub deployed on AWS has the limitation that each node groups must have [`min_node: 1` for autoscaling to work](https://docs.qhub.dev/en/latest/source/02_get_started/04_configuration.html?highlight=autoscaling#amazon-web-services-aws). This means that if we want multiple node groups, we will be charged for all the different EC2 instances that must be running as part of that node group. This will incur costs even when they are not being used (ie. dask-worker related node groups). \r\n\r\nIn order to cut down on operating costs, we only created one node-group (\"general\"). \r\n\r\n## Suggestion\r\nIt would be nice to have the ability to set `min_node: 0` for QHub deployed on AWS, like we had on GCP.  \r\n\r\n## Context\r\nThis is not a show-stopper. Raising this issue here because I imagine this is a feature that many other QHub/AWS users might appreciate as well \ud83d\ude00\r\n\r\n### If the request is related to a specific version of Kubernetes or Cloud provider, please provide these details:\r\nDescribe your environment:\r\n    - Kubernetes version (use `0kubectl version`): v1.19.8-eks-96780e\r\n    - Cloud provider (e.g. AWS, GCP, DigitalOcean, etc): AWS\r\n\r\n> Feel free to add any **additional context** or screenshots about the feature request here.\r\n",
      "comments": [
        "We would love to support this! This will work out of the box once AWS supports this https://github.com/aws/containers-roadmap/issues/724",
        "This issue has been automatically marked as stale because there was no recent activity in 60 days. Remove the stale label or add a comment, otherwise, this issue will automatically be closed in 7 days if no further activity occurs.",
        "This issue was closed because it has been stalled for 7 days with no activity."
      ]
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Quansight/qhub/issues/815",
    "content": {
      "title": "Use cloud provider which charges by the minute for CI",
      "body": "DO charges by the hour, even if the CI run only takes 15 minutes to run.  We should use a cloud provider which does offer by the minute pricing.  Digital Ocean suggests GCP may be cheapest for the node we're using though I haven't checked it's calculations.\r\n\r\n![image](https://user-images.githubusercontent.com/23342526/132866712-3781f545-76d3-4d2a-aeb4-2571edd00d16.png)\r\n",
      "comments": [
        "Amit recommends using AWS Spot instances.  I'll give that a try, but if they are too flaky then that is going to be a pain.",
        "Willing to do this, but I need some access to Quanisght's AWS account. @user \r\nAlternatively, create an IAM user with Permission Amazon EC2FullAccess and generate Access key ID and Access Secret Key and give them to me.\r\n![image](https://user-images.githubusercontent.com/23342526/133168481-c54441eb-d1b3-48e3-8c8a-a7b4a0e447c8.png)\r\n",
        " @user ",
        "Update: Dan gave me access and I created a IAM user with the appropriate permissions.  ",
        "Trying aws on the aws_ci branch, but the Kubernetes Tests are failing.  Unsure what the issue is.  Some logs are below around the time of failure.\r\n\r\n```\r\n[terraform]: module.kubernetes-nfs-server.kubernetes_deployment.main: Still creating... [9m50s elapsed]\r\n[terraform]: module.kubernetes-conda-store-server.kubernetes_deployment.main: Still creating... [9m50s elapsed]\r\n[terraform]: module.qhub.module.kubernetes-jupyterhub-ssh.kubernetes_deployment.jupyterhub-ssh: Still creating... [9m50s elapsed]\r\n[terraform]: module.qhub.module.kubernetes-jupyterhub-ssh.kubernetes_deployment.jupyterhub-sftp: Still creating... [9m50s elapsed]\r\n[terraform]: module.qhub.module.kubernetes-dask-gateway.kubernetes_deployment.gateway: Still creating... [9m50s elapsed]\r\n[terraform]: module.qhub.module.kubernetes-dask-gateway.kubernetes_deployment.controller: Still creating... [9m50s elapsed]\r\n[terraform]: module.kubernetes-nfs-server.kubernetes_deployment.main: Still creating... [10m0s elapsed]\r\n[terraform]: module.kubernetes-conda-store-server.kubernetes_deployment.main: Still creating... [10m0s elapsed]\r\n[terraform]: module.qhub.module.kubernetes-jupyterhub-ssh.kubernetes_deployment.jupyterhub-sftp: Still creating... [10m0s elapsed]\r\n[terraform]: module.qhub.module.kubernetes-dask-gateway.kubernetes_deployment.gateway: Still creating... [10m0s elapsed]\r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: Waiting for rollout to finish: 1 replicas wanted; 0 replicas Ready\r\n[terraform]: \u2502 \r\n[terraform]: \u2502   with module.kubernetes-nfs-server.kubernetes_deployment.main,\r\n[terraform]: \u2502   on modules/kubernetes/nfs-server/main.tf line 47, in resource \"kubernetes_deployment\" \"main\":\r\n[terraform]: \u2502   47: resource \"kubernetes_deployment\" \"main\" {\r\n[terraform]: \u2502 \r\n[terraform]: \u2575\r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: Waiting for rollout to finish: 1 replicas wanted; 0 replicas Ready\r\n[terraform]: \u2502 \r\n[terraform]: \u2502   with module.kubernetes-conda-store-server.kubernetes_deployment.main,\r\n[terraform]: \u2502   on modules/kubernetes/services/conda-store/main.tf line 46, in resource \"kubernetes_deployment\" \"main\":\r\n[terraform]: \u2502   46: resource \"kubernetes_deployment\" \"main\" {\r\n[terraform]: \u2502 \r\n[terraform]: \u2575\r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: Waiting for rollout to finish: 1 replicas wanted; 0 replicas Ready\r\n[terraform]: \u2502 \r\n[terraform]: \u2502   with module.qhub.module.kubernetes-dask-gateway.kubernetes_deployment.controller,\r\n[terraform]: \u2502   on modules/kubernetes/services/dask-gateway/controler.tf line 83, in resource \"kubernetes_deployment\" \"controller\":\r\n[terraform]: \u2502   83: resource \"kubernetes_deployment\" \"controller\" {\r\n[terraform]: \u2502 \r\n[terraform]: \u2575\r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: Waiting for rollout to finish: 1 replicas wanted; 0 replicas Ready\r\n[terraform]: \u2502 \r\n[terraform]: \u2502   with module.qhub.module.kubernetes-dask-gateway.kubernetes_deployment.gateway,\r\n[terraform]: \u2502   on modules/kubernetes/services/dask-gateway/gateway.tf line 102, in resource \"kubernetes_deployment\" \"gateway\":\r\n[terraform]: \u2502  102: resource \"kubernetes_deployment\" \"gateway\" {\r\n[terraform]: \u2502 \r\n[terraform]: \u2575\r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: Waiting for rollout to finish: 1 replicas wanted; 0 replicas Ready\r\n[terraform]: \u2502 \r\n[terraform]: \u2502   with module.qhub.module.kubernetes-jupyterhub-ssh.kubernetes_deployment.jupyterhub-sftp,\r\n[terraform]: \u2502   on modules/kubernetes/services/jupyterhub-ssh/sftp.tf line 37, in resource \"kubernetes_deployment\" \"jupyterhub-sftp\":\r\n[terraform]: \u2502   37: resource \"kubernetes_deployment\" \"jupyterhub-sftp\" {\r\n[terraform]: \u2502 \r\n[terraform]: \u2575\r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: Waiting for rollout to finish: 1 replicas wanted; 0 replicas Ready\r\n[terraform]: \u2502 \r\n[terraform]: \u2502   with module.qhub.module.kubernetes-jupyterhub-ssh.kubernetes_deployment.jupyterhub-ssh,\r\n[terraform]: \u2502   on modules/kubernetes/services/jupyterhub-ssh/ssh.tf line 54, in resource \"kubernetes_deployment\" \"jupyterhub-ssh\":\r\n[terraform]: \u2502   54: resource \"kubernetes_deployment\" \"jupyterhub-ssh\" {\r\n[terraform]: \u2502 \r\n[terraform]: \u2575\r\n\r\nProblem encountered: Terraform error\r\n\r\nError: Process completed with exit code 1.\r\n```",
        "Okay, I tried pulling the images before hand, and it that case I realized I am running out of space on the ec2 instance.  The ec2-instances seem to have 8-10GB filesystem available which is not enough to launch qhub since the zipped docker images themselves are < 7 GB.  I'm running out of space either when downloading the images or when unzipping the images.\r\n\r\nI see that some instances seem to have instance store volumes for additional storage, but I'm not sure if it's possible to add them to the instances spun up by CIRun.  @user could you comment on that? \r\n\r\nExample of running out of space when unzipping docker images\r\n![image](https://user-images.githubusercontent.com/23342526/133715260-6cf5414d-46a9-4b7a-b519-7c8f7f526c20.png)\r\n\r\n\r\n",
        "> I see that some instances seem to have instance store volumes for additional storage, but I'm not sure if it's possible to add them to the instances spun up by CIRun. @user could you comment on that?\r\n\r @user \r\nOne of the major usecase of Cirun is the customisation of machines. Since there are a ton of ways you can customise it, it's not trivial to allow that from the `.cirun.yml` file. So the recommended way to achieve this is via creating a custom AMI from the AWS interface (adding storage, installing software etc) and use that AMI ID in the `.cirun.yml` file.\r\n\r\nI just created a custom AMI with docker installed in the Quansight AWS account: `ami-08939857cf6893cde`. \r\nThe resulting .cirun.yml can be see in the [`test-aws`](https://github.com/Quansight/qhub/compare/test-aws?expand=1) branch:\r\n\r\n```diff\r\ndiff --git a/.cirun.yml b/.cirun.yml\r\nindex a195b61..a4e41e2 100644\r\n--- a/.cirun.yml\r\n+++ b/.cirun.yml\r\n@@ -3,12 +3,12 @@\r\n runners:\r\n   - name: run-k8s-tests\r\n     # Cloud Provider: DigitalOcean\r\n-    cloud: digitalocean\r\n+    cloud: aws\r\n     # Cheapest VM on DigitalOcean\r\n-    instance_type: g-4vcpu-16gb\r\n-    # Ubuntu-20.4  image\"\r\n-    machine_image: docker-20-04\r\n-    region: nyc1\r\n+    instance_type: t3a.2xlarge\r\n+    # Ubuntu-20.4 image\"\r\n+    machine_image: ami-08939857cf6893cde\r\n+    region: us-west-2\r\n     # Path of the relevant workflow file\r\n     workflow: .github/workflows/kubernetes_test.yaml\r\n     # Number of runners to provision on every trigger on Actions job\r\n```\r\n\r\nHere is the working GitHub Action: https://github.com/Quansight/qhub/runs/3632605318?check_suite_focus=true \r\n\r\n~It is failing at the moment, but that has nothing to do with Cirun.~\r\n\r\nNotes:\r\n- The custom AMI I created only adds docker on top of Ubuntu 20.04, we can also install K8s, Minikube, Node, etc to make every run of CI even faster.\r\n- The image is set as to create 35 GB machine to avoid disk space issues. Although we would pay for image storage monthly, which should be less than $4 monthly, but we'll save on the provision time, as it won't need to install things like say docker, which means cost would average out."
      ]
    },
    "codes": [
      "saving",
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Quansight/qhub/issues/689",
    "content": {
      "title": "[bug] general pods running on non-general nodes",
      "body": "There are a couple of general pods (meaning not user or worker work loads) on a non-general node group, in particular `cluster-autoscaler-aws-cluster-autoscaler` and `prefect-agent`.  I think these probably should be running on the general node.\r\n\r\nThese pods probably need to have a node affinity set (or possibly node selector, I am not quite clear).\r\n\r\nThis is related to a previous issue, #321, in which I also mentioned the idea of setting the taints on the nodes to prevent other workloads from shifting over to them, as 2i2c had done [here](https://github.com/2i2c-org/pilot-hubs/blob/57cd3238fcab4965d1dd8465a8230e6deb9f400f/terraform/main.tf#L145-L157).  \r\n\r\nI think that taints (and corresponding tolerations for the appropriate pods) would be the best and most general solution, since it should prevent any new workload additions (such as ClearML, etc) from scheduling on user and worker nodes, which should be exclusive to jupyter and dask work loads.\r\n\r\nIn the meantime, if implementing taints and tolerations would require more effort (not sure how hard it would be), I think it would be great if someone were able to add the appropriate node affinity for these pods to the general node group.  Some similar or related PRs for reference are #323, and from `qhub-terraform-modules` [PR#37](https://github.com/Quansight/qhub-terraform-modules/pull/37) and [PR#39](https://github.com/Quansight/qhub-terraform-modules/pull/39).\r\n\r\nThis is an important issue because it can cause increased cloud costs due to scheduling on possibly oversized nodes.",
      "comments": [
        " @user will answer this",
        "Hi @user @tylerpotts Any update on this.\r\nThank you.",
        "This issue has been automatically marked as stale because there was no recent activity in 60 days. Remove the stale label or add a comment, otherwise, this issue will automatically be closed in 7 days if no further activity occurs.",
        "This issue was closed because it has been stalled for 7 days with no activity."
      ]
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Quansight/qhub/issues/924",
    "content": {
      "title": "[enhancement] Development and production user interfaces",
      "body": "## Description\r\n\r\nCurrently the user interface uses some tools that improve the user experience, but there are more we can add. it would be nice to try different lab extensions before putting the into production. I am proposing at least two user interface images: the current prod version and a work in progress environment that we can use to add new features and test.\r\n\r\n## Value/benefit\r\n\r\ncurrently, we can't change the user experience without impacting everyone. another development image would give us the freedom to provide an enhanced user interface without effecting the current services. i'd like to experiment with some of the collaborative tools we serve with [hourhaus](https://github.com/deathbeds/hourhaus/blob/main/requirements.txt)\r\n\r\nthis request builds off of ideas in #903 with adding different user interfaces to the qhub experience.\r\n",
      "comments": [
        "The fastest and easiest way I see to do so, is forking and deploying your own forks, as I did for some of my PRs. By having a personal deployment, we're free to experiment and research without any limitation. The drawback is that it implies a cost for the cloud provider.\r\n\r\nAlternatively, we could imagine a `sandbox.qhub.dev` or `alpha.qhub.dev` or whatever, with its specific deploy repo, that we would use as a sandbox env for any experiment. The drawback here is the risk of collision if too much of us experiment at the same time.\r\n\r\nI hope I understood the question right :) ",
        "yea. i'm not going to find the time to setup my own qhub.\r\n\r\ni am thinking that there qhub.dev/alpha or qhub.dev/sandbox. these endpoints would deploy a different on a different endpoint that allows for experimentation on production data. in this conformation, early adopters can try out new features and fall back the blessed user interface when problems arise. ",
        "This can be part of a move to having JupyterLab images as more plug-and-play so you can easily switch in different images. Perhaps some of these ideas will help with this #1174 #1145 \r\nIdeally, people can work on JupyterLab config without understanding Docker at all."
      ]
    },
    "codes": [
      "awareness",
      "provider"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Quansight/qhub/issues/783",
    "content": {
      "title": "Cost monitoring of QHub cluster",
      "body": "## Summary\r\n\r\nThis is a large issue that we see come up a lot. Would like to have some way to assess costs of specific users and groups etc. Currently the most promising one is [kubecost](https://www.kubecost.com/) which has an open core model which should be easy enough to integrate.\r\n\r\n## Proposed implementation\r\n\r\nUsing the [kubecost helm chart](https://github.com/kubecost/cost-analyzer-helm-chart) create a helm chart and tie into the monitoring work with [prometheus and grafana](https://github.com/Quansight/qhub-internal/issues/7). This should not be implemented until this issue is implemented. Looks like kubecost does not implement authentication and will need to rely on sso and traefik-forward-auth for authentication.\r\n\r\n## Acceptance Criteria\r\n\r\n - [ ] kubecost is secured behind sso and only allows authenticated users\r\n - [ ] kubecost is explosed under the prefix path `/cost/*`\r\n - [ ] User is able to view the realtime costs of the given cluster\r\n - [ ] Kubecost reuses the prometheus server deployed for monitoring\r\n\r\n\r\n## Tasks to complete\r\n\r\n## Related to\r\n",
      "comments": [
        "per @user \r\n\r\nPlease see my [kubecost branch](https://github.com/Quansight/qhub/tree/kubecost) where I did most of this already.\r\n\r\nIt runs the 'full version' of Kubecost and doesn't share prometheus etc.",
        "This issue has been automatically marked as stale because there was no recent activity in 60 days. Remove the stale label or add a comment, otherwise, this issue will automatically be closed in 7 days if no further activity occurs.",
        "Useful feature. Let's keep this issue open.",
        "We are currently working on a custom tool to get the costs data and monitoring. I will update this once we get a POT available for testing."
      ]
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Quansight/qhub/issues/1110",
    "content": {
      "title": "[BUG] - AWS kubernetes resources not fully deleting properly (security group created by eks)",
      "body": "### OS system and architecture in which you are running QHub\r\n\r\nLinux\r\n\r\n### Expected behavior\r\n\r\nAll qhub resources should cleanly delete.\r\n\r\n```\r\n[terraform]: \u2502 Error: Plugin error\r\n[terraform]: \u2502 \r\n[terraform]: \u2502   with module.kubernetes-jupyterhub-ssh.kubernetes_manifest.jupyterhub-sftp-ingress,\r\n[terraform]: \u2502   on modules/kubernetes/services/jupyterhub-ssh/main.tf line 27, in resource \"kubernetes_manifest\" \"jupyterhub-sftp-ingress\":\r\n[terraform]: \u2502   27: resource \"kubernetes_manifest\" \"jupyterhub-sftp-ingress\" {\r\n[terraform]: \u2502 \r\n[terraform]: \u2502 The plugin returned an unexpected error from\r\n[terraform]: \u2502 plugin.(*GRPCProvider).ReadResource: rpc error: code = Unknown desc =\r\n[terraform]: \u2502 Unauthorized\r\n[terraform]: \u2575\r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: Plugin error\r\n[terraform]: \u2502 \r\n[terraform]: \u2502   with module.jupyterhub.kubernetes_manifest.jupyterhub,\r\n[terraform]: \u2502   on modules/kubernetes/services/jupyterhub/main.tf line 126, in resource \"kubernetes_manifest\" \"jupyterhub\":\r\n[terraform]: \u2502  126: resource \"kubernetes_manifest\" \"jupyterhub\" {\r\n[terraform]: \u2502 \r\n[terraform]: \u2502 The plugin returned an unexpected error from\r\n[terraform]: \u2502 plugin.(*GRPCProvider).ReadResource: rpc error: code = Unknown desc =\r\n[terraform]: \u2502 Unauthorized\r\n[terraform]: \u2575\r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: Plugin error\r\n[terraform]: \u2502 \r\n[terraform]: \u2502   with module.kubernetes-conda-store-server.module.minio.kubernetes_manifest.minio-api,\r\n[terraform]: \u2502   on modules/kubernetes/services/minio/ingress.tf line 1, in resource \"kubernetes_manifest\" \"minio-api\":\r\n[terraform]: \u2502    1: resource \"kubernetes_manifest\" \"minio-api\" {\r\n[terraform]: \u2502 \r\n[terraform]: \u2502 The plugin returned an unexpected error from\r\n[terraform]: \u2502 plugin.(*GRPCProvider).ReadResource: rpc error: code = Unknown desc =\r\n[terraform]: \u2502 Unauthorized\r\n[terraform]: \u2575\r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: Plugin error\r\n[terraform]: \u2502 \r\n[terraform]: \u2502   with module.monitoring[0].kubernetes_manifest.grafana-ingress-route,\r\n[terraform]: \u2502   on modules/kubernetes/services/monitoring/main.tf line 122, in resource \"kubernetes_manifest\" \"grafana-ingress-route\":\r\n[terraform]: \u2502  122: resource \"kubernetes_manifest\" \"grafana-ingress-route\" {\r\n[terraform]: \u2502 \r\n[terraform]: \u2502 The plugin returned an unexpected error from\r\n[terraform]: \u2502 plugin.(*GRPCProvider).ReadResource: rpc error: code = Unknown desc =\r\n[terraform]: \u2502 Unauthorized\r\n[terraform]: \u2575\r\nINFO:qhub.provider.terraform:terraform init directory=stages/06-kubernetes-keycloak-configuration\r\nINFO:qhub.provider.terraform: terraform at /tmp/terraform/1.0.5/terraform\r\n[terraform]: \r\n```\r\n\r\nSee https://github.com/Quansight/qhub-integration-test/runs/5311056863?check_suite_focus=true#step:6:1250 for example. This is not needed for 0.4.0. But should be resolved in 0.4.1\r\n\r\n```\r\n[terraform]: module.network.aws_vpc.main: Still destroying... [id=vpc-0d4af13fa907ed7bf, 4m20s elapsed]\r\n[terraform]: module.network.aws_vpc.main: Still destroying... [id=vpc-0d4af13fa907ed7bf, 4m30s elapsed]\r\n[terraform]: module.network.aws_vpc.main: Still destroying... [id=vpc-0d4af13fa907ed7bf, 4m40s elapsed]\r\n[terraform]: module.network.aws_vpc.main: Still destroying... [id=vpc-0d4af13fa907ed7bf, 4m50s elapsed]\r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: error deleting EC2 VPC (vpc-0d4af13fa907ed7bf): DependencyViolation: The vpc 'vpc-0d4af13fa907ed7bf' has dependencies and cannot be deleted.\r\n[terraform]: \u2502 \tstatus code: 400, request id: 467e3035-bbc1-400e-8880-a766392f1a9e\r\n[terraform]: \u2502 \r\n[terraform]: \u2502 \r\n[terraform]: \u2575\r\nINFO:qhub.provider.terraform:terraform init directory=stages/01-terraform-state/aws\r\n```\r\n\r\n### Actual behavior\r\n\r\nResources do not all properly delete\r\n\r\n### How to Reproduce the problem?\r\n\r\nRun qhub-integration-tests\r\n\r\n### Command output\r\n\r\n_No response_\r\n\r\n### Versions and dependencies used.\r\n\r\n_No response_\r\n\r\n### Compute environment\r\n\r\n_No response_\r\n\r\n### Integrations\r\n\r\n_No response_\r\n\r\n### Anything else?\r\n\r\n_No response_",
      "comments": [
        "I'm going to push this issue into 0.4.1 or later. I'll explain the rational. Currently the aws vpc does not cleanly delete with `qhub destroy`. There are two reasons for this. \r\n\r\n - when you delete the eks cluster any existing load balancers are not cleaned up https://github.com/hashicorp/terraform-provider-aws/issues/21863. We have solved this by running all the other stages and cleaning up the kubernetes service that was a load balancer. So this one is solved ... but really eks should be cleaning up after itself!\r\n - when you delete the eks cluster there is a stray security group that was associated with the eks cluster. Believe it is related to https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/eks_cluster#cluster_security_group_id. Related issues https://github.com/terraform-aws-modules/terraform-aws-eks/issues/1606\r\n\r\nSo this issue is a pain with no great solution on how to properly cleanup without AWS fixing this issue. Realistically this should not cause any problems aside from a stray vpc existing (no additional cost). If you want to delete the vpc simply go to the console and delete the vpc it should delete with it saying warning there is a security group still attached.",
        "Sounds like a plan to me.\n\nOn Thu, Feb 24, 2022 at 17:23 Christopher Ostrouchov <\n***@***.***> wrote:\n\n> I'm going to push this issue into 0.4.1 or later. I'll explain the\n> rational. Currently the aws vpc does not cleanly delete with qhub destroy.\n> There are two reasons for this.\n>\n>    - when you delete the eks cluster any existing load balancers are not\n>    cleaned up hashicorp/terraform-provider-aws#21863\n>    <https://github.com/hashicorp/terraform-provider-aws/issues/21863>. We\n>    have solved this by running all the other stages and cleaning up the\n>    kubernetes service that was a load balancer. So this one is solved ... but\n>    really eks should be cleaning up after itself!\n>    - when you delete the eks cluster there is a stray security group that\n>    was associated with the eks cluster. Believe it is related to\n>    https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/eks_cluster#cluster_security_group_id.\n>    Related issues terraform-aws-modules/terraform-aws-eks#1606\n>    <https://github.com/terraform-aws-modules/terraform-aws-eks/issues/1606>\n>\n> So this issue is a pain with no great solution on how to properly cleanup\n> without AWS fixing this issue. Realistically this should not cause any\n> problems aside from a stray vpc existing (no additional cost). If you want\n> to delete the vpc simply go to the console and delete the vpc it should\n> delete with it saying warning there is a security group still attached.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/Quansight/qhub/issues/1110#issuecomment-1050321991>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AABBB6NNUA6CSYS52A67DULU42VWHANCNFSM5PFV33PQ>\n> .\n> You are receiving this because you are subscribed to this thread.Message\n> ID: ***@***.***>\n>\n-- \niPhone\u2019d\n",
        "Hi @user I haven't found this recently, but how odd it would be if we add an extra removal step during destroying to use boto to check if the most painful resources were deleted?\r\n- EKS Lb\r\n- Elastic filesystem\r\n- S3 buckets (which can be deleted very cleanly using the python cli)\r\n- EKS clusters and VPC -- (deleting the vpcs seems to also remove the Security groups)",
        " @user @viniciusdc \ud83d\udc4b I found this thread by your link-back to the terraform-aws module issue I opened.\r\n\r\nYou might want to have a look at this terraform mini module I released awhile back and have been using internally for a couple months. During the terraform destroy, the module removes these Load Balancers that are stuck because of stray ENIs (Which creates the block in deleting subnets and security groups): https://github.com/webdog/terraform-kubernetes-delete-eni\r\n\r\nAt minimum, the shell script can be taken from the module, if the terraform module doesn't make sense to use. Cheers!"
      ]
    },
    "codes": [
      "networking",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/UrbanOS-Examples/scos-alm/issues/13",
    "content": {
      "title": "Update terraform-aws-ecs-cluster-1 module",
      "body": "As a government-funded project\r\nI don't want to maintain code I don't have to\r\nSo i'm cheaper and easier to run\r\n\r\nAC:\r\n- No longer uses SCOS fork \"terraform-aws-ecs-cluster-1\" and instead uses original/upstream\r\n- Fork is deleted\r\n- Jenkins state is preserved",
      "comments": [
        "Matt and I dug into this card. The original reason behind using a AWS cloud formation to set up the auto scaling group, vs using terraform, was that Terraform did not support applying rolling updates to the cluster. Looking at the master branch of aws_ecs_cluster, this feature is still not there, and the original issue with rolling updates would still persist. After speaking with Jeff, it may not be worth going after his card just yet.",
        "After discussion with Richard, moving this into the unprioritized backlog.  We will not play this for the time being."
      ]
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/jshcmpbll/Cloud-Mac-KVM/issues/16",
    "content": {
      "title": "Move MacHD disk storage to a Google Cloud disk rather than bucket.",
      "body": "It may be better to move the MacHD to a disk and have it mounted to the VM rather than copying over the image from a bucket each time (22GB of transfer takes a couple minutes and might be costly..)",
      "comments": [
        "Lol.. going back to a bucket so whateves."
      ]
    },
    "codes": [
      "saving",
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/jshcmpbll/Cloud-Mac-KVM/issues/33",
    "content": {
      "title": "Change boot disk size",
      "body": "First of all, I want to congratulate you on your great work with this. Absolutely brilliant!\r\nHowever, when I want to install the OS, the boot disk is about 10 GB so there isn't enough room to install macOS (I suppose it is around 20-30 GBs). So, I'd like to have in a new update an option to change the disk size, or you may help me with it tinkering around the actual config. Thanks in advance.\r\n\r\nEdit:\r\nI have seen in the variables file that you can change the size of the disk, although after changing it the boot disk still displays 10GB. Also, I've seen that after installing the System into an uninitialized disk of about 180GB that happened to be there, it get's stuck on %25 percent. I dunno how to continue. Any help is appreciated.",
      "comments": [
        " @user \n\r\nTotally valid point. When setting this project up I had made the default size much smaller to cut down on costs (I was rapidly spinning up and down the system and wasn't going through the actual install process). Being that the project is stable I should switch the default disk to an appropriate size.\r\n\r\nI'm going to redeploy this (haven't done it in some time) and see if I hit any issues. \r\n\r\nDid you rerun `terraform apply` after changing the disk size?",
        "Thank you for your fast reply. I have changed the 10GB variable in the variable.tf, but no luck so far. The macOS boot disk still shows ~5GB (I think). I recreated all the steps with a fresh GCP acc, but no luck neither (after changing the variables file again). So, I think that applying the `terraform apply` command, in this case, is unnecessary. If you need some screenshots or something please tell me.\r\nP.D.: every time I install it to the unknown drive with 180GB free space, it gets stuck on 17 minutes left. I can't install it to any other disk, even if I connect an external HDD (or maybe I haven't mounted it, so that's why it doesn't detect it, but I dunno how to mount it... so noob am I). However, it seems that the system isn't halted cuz I can connect to the VPS via VNC, but the mouse nor the kb responds to any action.",
        "Sorry I forgot to send an update once I finished last night. It looks that the disk size parameter wasnt actually populated in the disk resource creation. Im working on a PR to fix that and a couple other things.\r\n\r\nhttps://github.com/jshcmpbll/Cloud-Mac-KVM/pull/36\r\n",
        "Okay, I have seen the comments and also I've answered them, and added a couple more.\r\n\r\nBTW, happy new year!",
        "Happy new year! @user i added comments to your comments but I tested it and you should be good to rock.",
        " @user Let me know if you are still having issues bit I think you should be all set. I just spun up a VM with 4 cores and it was okay but I think increasing that to 8+ would be a much better experience. Feel free to adjust as needed on your end in the variables.tf file.",
        "Yeah, I've seen the comments. I'm trying the config right now, as I haven't had any spare time during the day. I'll let u catch up with any news.",
        "No worries, just wanted to check in. Happy new year!",
        "Okay, I've installed macOS, but I dunno why it gets frozen after installation. Also, after forced reboot, I can't connect to vnc anymore, and I dunno how to re-execute \"the script\" so to restart vnc sever.",
        "```MY_OPTIONS=\"+pcid,+ssse3,+sse4.2,+popcnt,+avx,+aes,+xsave,+xsaveopt,check\"\r\n  MEM=$(expr $(free -mt | grep Total | awk '{print $2}') - 2048)\r\n  CPU=$(nproc --all)\r\n  PASSWORD=password\r\n\r\n  printf \"change vnc password\\n%s\\n\" ${PASSWORD} | qemu-system-x86_64 -enable-kvm -m $MEM -cpu Penryn,kvm=on,vendor=GenuineIntel,+invtsc,vmware-cpuid-freq=on,$MY_OPTIONS\\\r\n    -machine q35 \\\r\n    -smp $CPU,cores=$CPU \\\r\n    -usb -device usb-kbd -device usb-tablet \\\r\n    -device isa-applesmc,osk=\"ourhardworkbythesewordsguardedpleasedontsteal(c)AppleComputerInc\"\\\r\n    -drive if=pflash,format=raw,readonly,file=/OSX-KVM/OVMF_CODE.fd \\\r\n    -drive if=pflash,format=raw,file=/OSX-KVM/OVMF_VARS-1024x768.fd \\\r\n    -smbios type=2 \\\r\n    -device ich9-intel-hda -device hda-duplex \\\r\n    -device ich9-ahci,id=sata \\\r\n    -drive id=OpenCore,if=none,snapshot=on,format=qcow2,file=/OSX-KVM/OpenCore-Catalina/OpenCore.qcow2 -device ide-hd,bus=sata.1,drive=OpenCore\\\r\n    -drive id=MacHDD,if=none,file=/user1,format=qcow2 -device ide-hd,bus=sata.3,drive=MacHDD \\\r\n    -netdev user,id=net0 -device e1000-82545em,netdev=net0,id=net0,mac=52:54:00:c9:18:27 \\\r\n    -monitor stdio -vnc :0,password\r\n ```    \r\nAs the root user - `sudo -i` you should be able to run that script above and restart the VM. I would double check to make sure its not running though - `ps aux | grep qemu`\r\n\r\nSounds odd that it keeps freezing though.. What VNC service are you using? Im using TigerVNC and it seems pretty reliable, my try switching to that and see if its acting better.",
        "Okay, it seems to be the last part of the init scrip... I'm using RealVNC as it is the default one in RasPi. (I'm not using raspi, but since I used it from the first time I haven't bothered to switch to tigervnc). I've reinstalled, and I'm just finishing the installation right now.",
        "Yeah, RealVNC should be fine. Feel free to ping me on discord and we can just on a call to debug if you want - Jsh#3038\r\n\r\nHopefully the install you are running works though!",
        "It's a pity... It seems that no luck this time neither! Discord tells me that you can't receive friend requests, so I think it's better if you add me - zallaevan#0322",
        "Okay, after tinkering around and with the guidance of @user  we've figured out what was happening:\r\n- There were some errors with some variables, but they were corrected quickly.\r\n- After installation, the screen froze, but to solve it we just needed to restart the qemu service with the last part of the initial script:\r\n> ```\r\n>   MEM=$(expr $(free -mt | grep Total | awk '{print $2}') - 2048)\r\n>   CPU=$(nproc --all)\r\n>   PASSWORD=password\r\n> \r\n>   printf \"change vnc password\\n%s\\n\" ${PASSWORD} | qemu-system-x86_64 -enable-kvm -m $MEM -cpu Penryn,kvm=on,vendor=GenuineIntel,+invtsc,vmware-cpuid-freq=on,$MY_OPTIONS\\\r\n>     -machine q35 \\\r\n>     -smp $CPU,cores=$CPU \\\r\n>     -usb -device usb-kbd -device usb-tablet \\\r\n>     -device isa-applesmc,osk=\"ourhardworkbythesewordsguardedpleasedontsteal(c)AppleComputerInc\"\\\r\n>     -drive if=pflash,format=raw,readonly,file=/OSX-KVM/OVMF_CODE.fd \\\r\n>     -drive if=pflash,format=raw,file=/OSX-KVM/OVMF_VARS-1024x768.fd \\\r\n>     -smbios type=2 \\\r\n>     -device ich9-intel-hda -device hda-duplex \\\r\n>     -device ich9-ahci,id=sata \\\r\n>     -drive id=OpenCore,if=none,snapshot=on,format=qcow2,file=/OSX-KVM/OpenCore-Catalina/OpenCore.qcow2 -device ide-hd,bus=sata.1,drive=OpenCore\\\r\n>     -drive id=MacHDD,if=none,file=/user1,format=qcow2 -device ide-hd,bus=sata.3,drive=MacHDD \\\r\n>     -netdev user,id=net0 -device e1000-82545em,netdev=net0,id=net0,mac=52:54:00:c9:18:27 \\\r\n>     -monitor stdio -vnc :0,password\r\n> ```\r\n\r\nNow everything seems to work flawlessly. Thank you again for your support, @jshcmpbll!"
      ]
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/terraform-google-modules/terraform-google-log-export/issues/6",
    "content": {
      "title": "Add billing-level sinks",
      "body": "Need to add support for billing-level logsinks (see https://www.terraform.io/docs/providers/google/r/logging_billing_account_sink.html)",
      "comments": [
        "Fixed by #15."
      ]
    },
    "codes": [
      "billing_mode",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/terraform-google-modules/terraform-google-log-export/issues/74",
    "content": {
      "title": "Please add exclusions",
      "body": "Please add exclusions as an input variable and pass it along to the resource.\r\n\r\nThe exclusion block is already available in resources as seen here:\r\n\r\nhttps://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/logging_project_sink#exclusions\r\n\r\nI believe it would just need to be an additional variable and reference it in the resource section of the sub-module",
      "comments": [
        "Hi @user \r\nThanks for the report. IIUC we can also use the [resource `google_logging_project_exclusion`](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/logging_project_exclusion) to achieve the same purpose?",
        " @user - I was trying to best understand how the exclusions block works on the project vs just on the log sink as I was reading the documentation. I was trying to understand if they are basically the same thing or if one only impacts the specific log sink.\r\n\r\nI can give that a try - I've got it working  using the standard google_logging_project_sink resource -  using the exclusions block.\r\n\r\n",
        " @user  - After a little trial, I think they actually do work differently.\r\n\r\nI cannot find the place where it actually displays my log exclusions on the project, except through the API (tried the UI and via gcloud command line)\r\n\r\nHere is the output of the log export with the project exclusion filter:\r\n\r\nlogging.sinks.get\r\n{\r\n  \"name\": \"********\",\r\n  \"destination\": \"pubsub.googleapis.com/projects/*****/topics/*****\",\r\n  \"filter\": \"Change_Me_to_Send_Additional_Logs_to_Splunk; Leave_Exclusions_in_Place_PLEASE\",\r\n  \"writerIdentity\": \"serviceAccount:p*****@gcp-sa-logging.iam.gserviceaccount.com\",\r\n  \"createTime\": \"2020-11-24T14:32:59.781680406Z\",\r\n  \"updateTime\": \"2020-11-24T19:14:43.336726821Z\"\r\n}\r\n\r\nlogging.exclusions.get\r\n{\r\n  \"exclusions\": [\r\n    {\r\n      \"name\": \"Aggregated_Logs_Exclusion\",\r\n      \"description\": \"Excluding logs already captured by aggregated log sink (at folder). Please DO NOT alter/remove.\",\r\n      \"filter\": \"LOG_ID(\\\"cloudaudit.googleapis.com/activity\\\") OR LOG_ID(\\\"externalaudit.googleapis.com/activity\\\") OR LOG_ID(\\\"cloudaudit.googleapis.com/system_event\\\") OR LOG_ID(\\\"externalaudit.googleapis.com/system_event\\\") OR LOG_ID(\\\"cloudaudit.googleapis.com/access_transparency\\\") OR LOG_ID(\\\"externalaudit.googleapis.com/access_transparency\\\")\",\r\n      \"createTime\": \"2020-11-24T19:14:43.313344920Z\",\r\n      \"updateTime\": \"2020-11-24T19:14:43.313344920Z\"\r\n    }\r\n  ]\r\n}\r\n\r\nHere is the output of the log export with the exclusions block in the google_logging_project_sink resource:\r\n\r\n{\r\n  \"name\": \"*****-log-export\",\r\n  \"destination\": \"pubsub.googleapis.com/projects/*****/topics/*****-logs-export\",\r\n  \"filter\": \"Change_Me_to_Send_Additional_Logs_to_Splunk; Leave_Exclusions_in_Place_PLEASE\",\r\n  \"writerIdentity\": \"serviceAccount:p*****@gcp-sa-logging.iam.gserviceaccount.com\",\r\n  \"createTime\": \"2020-11-24T14:32:59.781680406Z\",\r\n  \"updateTime\": \"2020-11-24T19:26:17.731837115Z\",\r\n  \"exclusions\": [\r\n    {\r\n      \"name\": \"Aggregated_Logs_Exclusion\",\r\n      \"description\": \"Excluding logs already captured by aggregated log sink (at folder). Please DO NOT alter/remove.\",\r\n      \"filter\": \"LOG_ID(\\\"cloudaudit.googleapis.com/activity\\\") OR LOG_ID(\\\"externalaudit.googleapis.com/activity\\\") OR LOG_ID(\\\"cloudaudit.googleapis.com/system_event\\\") OR LOG_ID(\\\"externalaudit.googleapis.com/system_event\\\") OR LOG_ID(\\\"cloudaudit.googleapis.com/access_transparency\\\") OR LOG_ID(\\\"externalaudit.googleapis.com/access_transparency\\\")\"\r\n    }\r\n  ]\r\n}\r\n\r\nHonestly, I do not know the specific differences between the two, other than it appears that the exclusion block placed on the google_logging_project_sink might only apply to that specific log-sink (which we are pushing through pub/sub) while the other looks like it might be more global - applies at the project level.\r\n\r\nAppreciate you looking into this.",
        "Hi @user \r\nAfter looking through the API docs it seems like `google_logging_project_exclusion ` operates on `_Default` sink, however defining exclusions with the log sink via `google_logging_project_sink` operates on the same sink.  `google_logging_*_sink`  resources also seems to supported. https://github.com/hashicorp/terraform-provider-google/issues/7880 for tracking doc update",
        "Hey @user - appreciate your help looking into this.\r\n\r\nAfter trying both the project exclusion and the individual sink exclusion, only the individual exclusion seems to fit what I'm trying to do.\r\n\r\nWe have an aggregated log sink on a folder that includes the children. We have a log sink in each project that will ship logs via pub/sub to splunk (who subscribes). \r\n\r\nWhen I try to use the project level exclusion with the aggregated log sink and use the same filters on both, I get duplicate messages in splunk (and obviously paying double via pub/sub).\r\n\r\nWhen I use the aggregated with the log exclusion specified on the individual log sink (and I can use the same filters for inclusion on the aggregated sink, inclusion on the specific log sink, and the exclusion filter applied in that same log sink (not via  the project logging.exclusion api call), I only get a single log - the one from the aggregated log sink at the folder level.\r\n\r\nAgain...really appreciate your help in looking into this.  I honestly think it's another variable on the logging module and exposing the exclusions on the sub-module - hope it's that simple.\r\n\r\nAlso...if we can get our CLA in place, we can try to do the pull request and help out when we things like this that we might be able to help on...sorry...",
        "This issue is stale because it has been open 60 days with no activity. Remove stale label or comment or this will be closed in 7 days",
        " @user can you take a look at #103? Or tell me who could?"
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/midl-dev/tezos-on-gke/issues/49",
    "content": {
      "title": "remove sentry node",
      "body": "I get many missed bakes and endorsements since florence activation or v9 release, even with v9.2 which was supposed to have a fix for it.\r\n\r\nI am trying now to run with \"naked nodes\" not protected by sentries, to see if it fixes it. I will post my findings.\r\n\r\nWe are an exception, no one else (kiln or big bakers) are using sentries for their baking operations.\r\n\r\nI will monitor for a few days and sees if it fixes it. If it does, I will push the code for sentry removal here.",
      "comments": [
        "Main problem for missed endorsements is that the private node is not reliably connecting to the public nodes (due to bad local DNS probably).\r\n\r\nIf you choose to remove sentry nodes:\r\n- is security compromised? can it be still set as private node, with about 50 public trusted nodes as peers?\r\n- you could also \"lower\" the machine type (right now it costs 200 euro/month).",
        "When the public/private node link disconnects, I get log messages \"too few\nconnections (1)\". The times I had missed endorsements, I didn't see these\nmessages, so I assume the connectivity was ok.\nThere is a long-standing issue with dns (\nhttps://gitlab.com/tezos/tezos/-/issues/1382) but I don't think it makes a\ndifference here, except maybe at first boot.\nComparing the timestamps, I did see 5-10 second lag between the time the\npublic node imports the block and the time it propagates to the private\nnode.\nNo, it can't be a private node with public trusted nodes. It has to open\nconnections to a large number of peers and advertise its node id. Note that\nit does not accept incoming connections, which is good and bad. (bad for\nthe network, but good for us in terms of security)\nSecurity-wise it's not as good as sentries, but if sentries cause\nendorsement to be missed, it's not worth it. I will leave it running for a\nfew more days to see if it makes a difference...\n\nOn Wed, Jun 2, 2021 at 11:22 PM denver-s ***@***.***> wrote:\n\n> Main problem for missed endorsements is that the private node is not\n> reliably connecting to the public nodes (due to bad local DNS probably).\n>\n> If you choose to remove sentry nodes:\n>\n>    - is security compromised? can it be still set as private node, with\n>    about 50 public trusted nodes as peers?\n>    - you could also \"lower\" the machine type (right now it costs 200\n>    euro/month).\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/midl-dev/tezos-on-gke/issues/49#issuecomment-853601317>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAAWXC5VRBJ4PJU7L3COHYDTQ4NSBANCNFSM457YGBYA>\n> .\n>\n",
        "How's it going?",
        "I still have missed endorsements occasionally despite removing sentry nodes.",
        "Nodes have plenty of cpu and ram, but it takes about 5 seconds to validate each block (`completed in 7.650s `):\r\n\r\n```\r\n\u2502 Jun  7 16:12:14.101 - validator.chain: Request pushed on 2021-06-07T16:12:13.846-00:00, treated in 1.351ms, completed in 251ms                                                                                                                             \u2502\r\n\u2502 Jun  7 16:13:10.632 - validator.block: block BL1DAnEVpT7L1UAzTpBbPQvWmgGGA7RbxPa9oEbBTync4TchpWG successfully validated                                                                                                                                    \u2502\r\n\u2502 Jun  7 16:13:10.632 - validator.block: Request pushed on 2021-06-07T16:13:02.982-00:00, treated in 7.187us, completed in 7.650s                                                                                                                            \u2502\r\n\u2502 Jun  7 16:13:10.816 - prevalidator.NetXdQprcVkpa.PsFLorenaUUu: switching to new head BL1DAnEVpT7L1UAzTpBbPQvWmgGGA7RbxPa9oEbBTync4TchpWG                                                                                                                   \u2502\r\n\u2502 Jun  7 16:13:10.816 - prevalidator.NetXdQprcVkpa.PsFLorenaUUu:  Request pushed on 2021-06-07T16:13:10.644-00:00, treated in 376us, completed in 171ms                                                                                                      \u2502\r\n\u2502 Jun  7 16:13:10.925 - validator.chain: Update current head to Block Hash BL1DAnEVpT7L1UAzTpBbPQvWmgGGA7RbxPa9oEbBTync4TchpWG (level 1505379, timestamp 2021-06-07T16:12:58-00:00, fitness 01::00000000000cf863), same branch                               \u2502\r\n\u2502 Jun  7 16:13:10.925 - validator.chain: Request pushed on 2021-06-07T16:13:10.631-00:00, treated in 1.958ms, completed in 289ms                                                                                                                             \u2502\r\n\u2502 Jun  7 16:14:24.541 - validator.block: block BM5UfHWamYapeUANsVfpkyJNVWGboEGyohNFJEa2kG8tRqEWn8Q successfully validated                                                                                                                                    \u2502\r\n\u2502 Jun  7 16:14:24.541 - validator.block: Request pushed on 2021-06-07T16:14:19.223-00:00, treated in 11.776us, completed in 5.317s                                                                                                                           \u2502\r\n\u2502 Jun  7 16:14:25.329 - prevalidator.NetXdQprcVkpa.PsFLorenaUUu: switching to new head BM5UfHWamYapeUANsVfpkyJNVWGboEGyohNFJEa2kG8tRqEWn8Q                                                                                                                   \u2502\r\n\u2502 Jun  7 16:14:25.329 - prevalidator.NetXdQprcVkpa.PsFLorenaUUu:  Request pushed on 2021-06-07T16:14:24.565-00:00, treated in 2.302ms, completed in 761ms                                                                                                    \u2502\r\n\u2502 Jun  7 16:14:25.461 - validator.chain: Update current head to Block Hash BM5UfHWamYapeUANsVfpkyJNVWGboEGyohNFJEa2kG8tRqEWn8Q (level 1505380, timestamp 2021-06-07T16:13:58-00:00, fitness 01::00000000000cf864), same branch                               \u2502\r\n\u2502 Jun  7 16:14:25.461 - validator.chain: Request pushed on 2021-06-07T16:14:24.540-00:00, treated in 5.146ms, completed in 912ms                                                                                                                             \u2502\r\n\u2502 Jun  7 16:15:12.834 - validator.block: block BM2VtPURmfixu7F6t9Sgb5mDfHamQ18WTryDi28d1RMWpS4VtSa successfully validated                                                                                                                                    \u2502\r\n\u2502 Jun  7 16:15:12.834 - validator.block: Request pushed on 2021-06-07T16:15:06.737-00:00, treated in 4.495us, completed in 6.96s                                                                                                                             \u2502\r\n\u2502 Jun  7 16:15:13.081 - prevalidator.NetXdQprcVkpa.PsFLorenaUUu: switching to new head BM2VtPURmfixu7F6t9Sgb5mDfHamQ18WTryDi28d1RMWpS4VtSa                                                                                                                   \u2502\r\n\u2502 Jun  7 16:15:13.081 - prevalidator.NetXdQprcVkpa.PsFLorenaUUu:  Request pushed on 2021-06-07T16:15:12.847-00:00, treated in 282us, completed in 233ms                                                                                                      \u2502\r\n\u2502 Jun  7 16:15:13.183 - validator.chain: Update current head to Block Hash BM2VtPURmfixu7F6t9Sgb5mDfHamQ18WTryDi28d1RMWpS4VtSa (level 1505381, timestamp 2021-06-07T16:14:58-00:00, fitness 01::00000000000cf865), same branch                               \u2502\r\n\u2502 Jun  7 16:15:13.183 - validator.chain: Request pushed on 2021-06-07T16:15:12.834-00:00, treated in 877us, completed in 346ms    \r\n```\r\n\r\nOn Florencenet it's much faster.",
        "I noticed an improvement without sentry nodes, so I'm in favor of removing them.",
        "OK, will do.\r\n\r\nI opened a ticket on tezos repo where they claim that it will get better once more bakers upgrade to 9.2\r\n\r\nhttps://gitlab.com/tezos/tezos/-/issues/1446"
      ]
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/relaycorp/cloud-gateway/issues/86",
    "content": {
      "title": "Replace MongoDB with GCP Firestore as backend for public keys and certificates",
      "body": "# Describe the problem\r\n\r\nA HA MongoDB cluster is too complicated and expensive for what it's used for today, which is mostly to store public keys and certificates.\r\n\r\n# Describe the solution you'd like\r\n\r\nUse [GCP Firestore](https://cloud.google.com/firestore) instead, which is fully managed by GCP and doesn't require any capacity planning/monitoring.\r\n\r\nTODO\r\n\r\n- [ ] Implement `FirestorePublicKeyStore`.\r\n- [ ] Implement `FirestoreCertificateStore`.\r\n- [ ] Integrate `FirestorePublicKeyStore` and `FirestoreCertificateStore` in the gateway.\r\n- [ ] Provision GCP resources (including Google Service Accounts using Workload Identity).\r\n- [ ] Configure Firestore in the gateway.\r\n\r\n# Additional context\r\n\r\nSee also:\r\n\r\n- https://github.com/relaycorp/cloud-gateway/issues/85\r\n",
      "comments": []
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/relaycorp/cloud-gateway/issues/84",
    "content": {
      "title": "Replace NATS Streaming with GCP PubSub",
      "body": "# The problem\r\n\r\nNATS Streaming is a delight to use in development, but less so to deploy and operate (especially when using Kubernetes + GitOps):\r\n\r\n- It requires two backing services: NATS server and a fault-tolerant, persistence backend. That means: Increased costs (labour + hosting), and more components that could fail.\r\n- I'm having to deploy a highly-available PostreSQL instance just for NATS Streaming, which is insane. We already use MongoDB, but it isn't supported. (MySQL and a distributed file storage are also supported, but neither offers any advantage over PostgreSQL)\r\n- I'm uncomfortable using the official Helm charts in production as I don't think they're production-ready, based on the issued I've found (and submitted PRs for), the workarounds I've had to implement, the overall brittleness of the resources in the chart (e.g., provisioning DB tables via the `initdb` script) and the overall insecurity of the chart (e.g., storing sensitive values in the clear instead of using secrets).\r\n\r\n# The solution\r\n\r\nKeep NATS Streaming in development but use GCP PubSub in production. However, this requires making the gateway broker-agnostic:\r\n\r\n- [ ] https://github.com/relaycorp/relaynet-internet-gateway/issues/759\r\n- [ ] https://github.com/relaycorp/relaynet-internet-gateway/issues/760\r\n\r\n# Alternatives considered\r\n\r\nWe could use a GCP-managed Redis instance, but that's still a server/cluster we have to provision and ensure has the appropriate capacity at all times. Plus, [persistence is Redis](https://redis.io/topics/persistence) isn't adequate here because we can't lose messages.\r\n\r\n[Kafka](https://cloud.google.com/confluent) is too expensive at +$1.2k/month for the simplest possible HA cluster. Kafka also feels like a massive overkill for what it'd be used for.",
      "comments": []
    },
    "codes": [
      "networking",
      "cluster",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/lean-delivery/terraform-module-aws-core/issues/2",
    "content": {
      "title": "NAT cost optimization",
      "body": "I'd like to have opportunity to reduce core costs with replacing NAT gateway service with ec2 instance (shape should be chosen from variables, default t3.nano).",
      "comments": [
        "Hi all! I've created [PR ](https://github.com/lean-delivery/tf-module-aws-core/pull/3 )for this issue, plz review it.\r\nI'm new to Terraform and IaC AWS and don't expect to solve this issue from the first time",
        "Not works, as expected. ",
        "please recheck. connection problem solved. \r\nNow we can connect to Nat instance with key. added new line for internet connection \"source_dest_check      = false \". tested on own AWS cloud platform all work fine\r\n\r\nhttps://github.com/javid87/ld_tf_projects.git"
      ]
    },
    "codes": [
      "networking",
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/equinix/terraform-metal-anthos-on-vsphere/issues/15",
    "content": {
      "title": "Implement code to clean up assigned IP blocks",
      "body": "Now that we have the IP block assignment fix in, we can implement code to give back the ip blocks after the esxi hosts are converted to layer to give back the IPs.\r\n\r\nThis would reduce costs ($0.04 per hour!!) and more importantly give back the IPs to be used on different projects.\r\n\r\nI've tested manually deleting the IP blocks after the project was created and it had no impacted on the environment or on the destroy procedure. ",
      "comments": [
        "I fixed this in the VMware only version of this repo.\r\nI'll try and get this turned into a PR soon.\r\nHere are the changes: \r\nhttps://github.com/c0dyhi11/vmware-on-packet/blob/bd368a0f70f4abb0aa65c4593580560bc85a842e/08-esx-host-networking.tf#L59\r\nhttps://github.com/c0dyhi11/vmware-on-packet/blob/bd368a0f70f4abb0aa65c4593580560bc85a842e/templates/esx_host_networking.py#L153\r\nhttps://github.com/c0dyhi11/vmware-on-packet/blob/bd368a0f70f4abb0aa65c4593580560bc85a842e/templates/esx_host_networking.py#L159\r\nhttps://github.com/c0dyhi11/vmware-on-packet/blob/bd368a0f70f4abb0aa65c4593580560bc85a842e/templates/esx_host_networking.py#L162\r\nhttps://github.com/c0dyhi11/vmware-on-packet/blob/bd368a0f70f4abb0aa65c4593580560bc85a842e/templates/esx_host_networking.py#L344"
      ]
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/equinix/terraform-metal-anthos-on-vsphere/issues/21",
    "content": {
      "title": "Change router to c3.small.x86 ",
      "body": "Adjust the router to c3.small.x86 which also requires a change to ubuntu 18.04. This is more right sized and reduces costs by $0.50 per hour.\r\n\r\nWe can borrow heavily from @user existing work (https://github.com/c0dyhi11/vmware-on-packet).\r\n\r\n",
      "comments": [
        "I've got code ready, just waiting on [pr#20 ](https://github.com/packet-labs/google-anthos/pull/20) before I merge, test, and submit a pr.",
        " @user The c3.small with ubuntu 18.04 may not be ready for external users. I see: \r\n`Error: ubuntu_18_04 is not a valid operating system for the plan c3.small.x86 and facility [\"dfw2\"]`\r\n\r\nThe portal only lists custom iPXE as an option.\r\n",
        "Hmm... That's interesting. I know another customer can use it. Maybe the google entitlement needs to be tweaked. ",
        "Understood, the limitation is something specific to my account set up. Will continue this work once the entitlement gets resolved.",
        "I tried this before on my account and had the same result, could not spin\nup c3.small with ubuntu\n\nOn Wed, Mar 11, 2020 at 10:52 PM Paul Mason <notifications@github.com>\nwrote:\n\n> Understood, the limitation is something specific to my account set up.\n> Will continue this work once the entitlement gets resolved.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/packet-labs/google-anthos/issues/21#issuecomment-597896263>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAJHEURHURH224JZ4XAEEWDRHABZFANCNFSM4LFY2IAQ>\n> .\n>\n\n\n-- \nBest Regards\n\nAbdelfettah | Strategic Cloud Engineer | abdelfettah@google.com | +46 70\n534 68 78\n",
        "The latest update here is that the c3.small.x86 is not ready for prime time. We have 2x outstanding issues which make it difficult to provision. We are addressing these and will then get back to this issue.",
        "Was able successfully test with the s3.small today. Speaking with @user  the c3.small may not be deployed widely enough to make this the default router.\r\n\r\nHowever, if someone wants to try it out, they can add the following to their tfvars file:\r\n`router_size = c3.small.x86`",
        "This is available now by using gen 3 hardware (note, the c3.small are in high demand and it is wise to run `check_capacity.py` prior attempting to use a c3.small to confirm availability), see: https://github.com/packet-labs/google-anthos#using-packet-gen-3-hardware-and-esxi-67"
      ]
    },
    "codes": [
      "saving",
      "instance"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/GSA/datagov-brokerpak-eks/issues/8",
    "content": {
      "title": "Ensure all inter-pod traffic uses TLS",
      "body": "## User Story\r\n\r\nIn order to have TLS on every network hop between the outside world and individual pods, we want [EKS clusters configured to use AWS App Mesh and cert-manager](https://aws.amazon.com/blogs/containers/securing-kubernetes-applications-with-aws-app-mesh-and-cert-manager/). \r\n\r\n## Acceptance Criteria\r\n- [ ] GIVEN I have provisioned an EKS instance \\\r\n  AND I have deployed the 2048 fixture \\\r\n  AND I have accessed the 2048 application using my browser \\\r\n  WHEN I run `kubectl -n default exec -it ${2048_POD_NAME} -c envoy -- curl -s localhost:9901/stats | grep ssl.handshake` \\\r\n  THEN I see a non-zero count of `ssl_handshake` entries between the 2048 pod and the nginx-ingress pod.\r\n\r\n## Background\r\n[Any helpful contextual notes or links to artifacts/evidence, if needed]\r\n\r\n## Security Considerations ([required](https://nvd.nist.gov/800-53/Rev4/control/CM-4))\r\n[comment]: # \"Our SSP says 'The Data.gov team ensures security implications are considered as part of the agile requirements refinement process by including a section in the issue template used as a basis for new work.' so please don't remove this section without care.\"\r\nThis work will help us meet our compliance requirements. See [section 10.9.6](https://docs.google.com/document/d/15bP07LtTXY2gvNHlM_reehTp4_k3tMZh/edit#heading=h.9agzit9csn1a).\r\n\r\n## Sketch\r\nFor this story, we only need to work up through step 4.1 of the referenced blog post... That is, we want to demonstrate mTLS between the `nginx-ingress` pod and the `2048` pod.\r\n\r\nWe can work up through step 5 (TLS between the ALB controller and `nginx-ingress` controller) in a separate/future story.\r\n\r\nWe're now considering 4 options going forward:\r\n1. Remove nginx-ingress to get as close to the AWS-supported configuration as possible (adds ALB costs)\r\n2. Try the new [solr-operator support for inter-node TLS](https://github.com/apache/solr-operator/blob/main/docs/solr-cloud/solr-cloud-crd.md#enable-tls-between-solr-pods) (solves for Solr, further work needed in future for other k8s services) \r\n3. Try [the AWS+Kong documented method that uses Kong as the ingress controller](https://aws.amazon.com/blogs/containers/running-microservices-in-amazon-eks-with-aws-app-mesh-and-kong/) (keeps single ALB)\r\n4. Keep trying to debug existing path\r\n\r\nSee also https://docs.aws.amazon.com/app-mesh/latest/userguide/getting-started-kubernetes.html",
      "comments": [
        "We have to figure out why our annotations aren't making it all the way from the `helm_release` resource to the pods.",
        "As we're re-evaluating our approach, I wanted to mention another option, the Solr Operator has support for [mutual TLS](https://github.com/apache/solr-operator/blob/main/docs/solr-cloud/solr-cloud-crd.md#enable-tls-between-solr-pods). It doesn't look simple to implement and it doesn't look like it covers ZooKeeper but it might end up being an overall simpler solution that meets the compliance requirements.",
        "It appears this is new functionality as of version 0.3.0 of `solr-operator`, which is not yet released. However, it's due any day now! Here's [the 6-day-old ChangeLog](https://github.com/apache/solr-operator/commit/32ad1c87b8d3dcf0a6df5ac2395f98b0c86cd7a5).",
        "BTW, I think the inter-pod TLS functionality is relatively straightforward if `cert-manager` is installed in the k8s cluster. It's the mTLS (client-certificate) part that is more complex.",
        "Srini is examining [this alternate approach](https://aws.amazon.com/blogs/containers/running-microservices-in-amazon-eks-with-aws-app-mesh-and-kong/).",
        "For the record: it turns out that if you're using Fargate, AWS AppMesh is the only viable service mesh option for now. Brought to you after I read up on linkerd and got excited about it being simpler, only to find [this post](https://github.com/aws/containers-roadmap/issues/682#issuecomment-884593011) saying Istio/App Mesh is the only game in Fargate town.\r\n"
      ]
    },
    "codes": [
      "awareness",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/GSA/datagov-brokerpak-eks/issues/2",
    "content": {
      "title": "Optimize ingress to use just one ALB per cluster",
      "body": "## User Story\r\n\r\nIn order to reduce the cost of operating EKS cluster instances, and reduce dependence on a single k8s provider, the team would like to provision just a single ALB per AWS EKS cluster, rather than one per individual ingress.\r\n\r\n## Acceptance Criteria\r\n[ACs should be clearly demoable/verifiable whenever possible. Try specifying them using [BDD](https://en.wikipedia.org/wiki/Behavior-driven_development#Behavioral_specifications).]\r\n- [x] GIVEN I have provisioned an EKS instance \\\r\n  AND I have deployed two ingresses \\\r\n  AND I am authenticated with the AWS console \\\r\n  WHEN I look at the AWS EC2 \"Load Balancer\" list \\\r\n  THEN I see just one LB associated with the EKS cluster\r\n\r\n## Background\r\nPeople using cloud.gov to provision and bind k8s service should, as much as possible, not care about and not refer to the underlying implementation when they use the service. This leaves the provider of the k8s service flexibility to use a different implementation (eg GCP or Azure instead of AWS) without customers of the service preventing migration. \r\n\r\nNormally, using AWS EKS requires customers to know about and use the AWS-specific ingress annotations in order to make their deployments accessible to the outside world. By using a second level ingress controller based on the widely used and documented nginx-ingress, we can enable cross-provider specifications on ingress using labels and tags that will not need to be changed.\r\n\r\nUsing a secondary controller has the added benefit of requiring just a single AWS Load Balancer instance for all the workloads in the cluster, no matter how many. This in turn cuts down on the government's cost to run the service.\r\n\r\n## Security Considerations ([required](https://nvd.nist.gov/800-53/Rev4/control/CM-4))\r\n[comment]: # \"Our SSP says 'The Data.gov team ensures security implications are considered as part of the agile requirements refinement process by including a section in the issue template used as a basis for new work.' so please don't remove this section without care.\"\r\nThe secondary nginx-ingress controller is only accessible from within Fargate, and for traffic to reach that service, it must traverse the AWS ALB first. So no additional network exposure is implied here.\r\n\r\nBecause customers of the service cannot specify tags or labels that the ALB controller will act on, there is no way for customers to introduce a separate ingress to the cluster. \r\n\r\n## Sketch\r\nThere is [precedent for setting up this architecture with AWS EKS and Fargate](https://github.com/rajeshwrn/alb-nginx-controller).",
      "comments": []
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/segmentio/stack/issues/135",
    "content": {
      "title": "Add s3 vpc endpoint",
      "body": "Didn't see anything discussing this before, but having a S3 VPC endpoint is something to have as a default?",
      "comments": [
        "One can whitelist EIP on S3, or control via IAM. Have you seen performance benefits? Can you share your findings?",
        "There is definitely cost benefits, no?"
      ]
    },
    "codes": [
      "networking",
      "saving",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/patheard/terraform-cantrill-aws-associate/issues/1",
    "content": {
      "title": "Create billing alerts for general/prod accounts",
      "body": "",
      "comments": [
        "Closed by 389757a7, 29492c8b"
      ]
    },
    "codes": [
      "alert",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/patheard/terraform-cantrill-aws-associate/issues/38",
    "content": {
      "title": "Lab: deploy container to ECS (Fargate mode)",
      "body": "1. Create VPC w/ 2 public subnets\r\n2. Create Fargate ECS cluster\r\n3. Task definition\r\n    - No role required\r\n    - Memory: `1GB`\r\n    - vCPU: `0.5`\r\n 4. Container definition:\r\n    - Image: `docker.io/acantrill/containerofcats`\r\n    - Memory: `1024MiB (soft)`\r\n    - Port: `80 (tcp)`\r\n 5. Deploy task to Fargate cluster in both subnets\r\n 6. Need SG that allows incoming 80 tcp to tasks\r\n 7. Delete it all\r\n \r\nOptional: investigate service definition for ECS.\r\n\r\n### Concepts\r\n- `Container definition`: image and ports\r\n- `Task definition`: Security role (assume) for containers, container(s), resources\r\n- `Service`: How many tasks, HA, restarts\r\n- `EC2 mode`:\r\n   - you manage an ECS cluster's EC2 instances\r\n   - more control over instance sizing, pricing, but you pay for those EC2 instances + maintenance overhead\r\n- `Fargate mode`: \r\n   - shared ECS cluster infra managed by Amazon\r\n   - tasks are given Elastic Network Interface (ENI) injected into your subnets, and behave like any other aws resource with an ENI in your subnet (e.g. public IP allocation, ability to egress to internet based on SGs and NACLs, etc)",
      "comments": []
    },
    "codes": [
      "instance",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/covidapihub/terraform-covidapihub/issues/35",
    "content": {
      "title": "Try to deploy a smaller Kafka",
      "body": "Our current deployment is here: https://console.aws.amazon.com/msk/home?region=us-east-1#/cluster/arn%3Aaws%3Akafka%3Aus-east-1%3A153757265334%3Acluster%2Fcovidapihub-observables%2Fd79bdc4f-fd82-4c71-a731-710d0c41ca43-6/view?tabId=details\r\n\r\nManaged is awesome, but expensive, and we're overprovisioned.\r\n\r\nWe will likely need to deploy a new cluster alongside our existing one, then change our configs.\r\n\r\nPricing: https://aws.amazon.com/msk/pricing/\r\n",
      "comments": []
    },
    "codes": [
      "cluster",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/VikeSec/CTF-Infrastructure/issues/25",
    "content": {
      "title": "Review apps",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nHaving to pull, setup a `k3d` cluster, then check to ensure k8s changes are reflected is a waste of time\r\n\r\n**Describe the solution you'd like**\r\nAn automated review application spun up upon every push to a pull request. Running within GH Actions in `k3d`.\r\n\r\n**Describe alternatives you've considered**\r\nSetup a full AWS stack, but that is costly and worrying if it isn't all torn down properly\r\n\r\n**Additional context**\r\nThere might be issues with the review app running and not allowing the pull request to be merged. Some sort of \"gracefully exit\" might be required\r\n",
      "comments": []
    },
    "codes": [
      "awareness",
      "provider"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Eximchain/terraform-aws-dappbot/issues/16",
    "content": {
      "title": "Scaling Analysis",
      "body": "Enumerate all possible scaling bottlenecks and design a plan to work around it.  We should have a path to scale to at least 10,000 users.",
      "comments": [
        "Per Dapp Resources:\r\n\r\n- S3 bucket\r\n- Dappseed in Dappseed bucket\r\n- CodePipeline\r\n- Cloudfront Distribution\r\n- DNS Record\r\n- DynamoDB Item\r\n\r\nRelevant Limits:\r\n\r\n- S3 buckets: 100 default, max limit 1,000\r\n- CodePipelines: 300 default, only 60 if periodically checking for source changes\r\n- Cloudfront Distributions: 200 default\r\n- DNS Records: 10,000 default\r\n\r\nSeems like to scale to 10,000 and beyond, we need to avoid creating an S3 bucket, CodePipeline, and Cloudfront Distribution per-dapp.",
        "The idea we have for the cheapest offering should scale pretty much as far as we want.\r\n\r\nWe could serve Dapps from a single domain, from an API Gateway possibly hooked up to Lambda, used to serve the Dapps from a single S3 bucket.\r\n\r\n- This architecture only uses a single S3 bucket, API Gateway, and (possibly) Lambda function\r\n- This architecture takes the Cloudfront Distribution out of the loop\r\n- This architecture allows CodePipelines to be deleted afterwards\r\n\r\nEffectively, the only per-Dapp items are some S3 objects and the DynamoDB Item, which has no limits. We just have to pay for our usage."
      ]
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Eximchain/terraform-aws-dappbot/issues/52",
    "content": {
      "title": "Extract ABI from Dynamo",
      "body": "As an example, the weyl governance dapp has an ABI that's around 20 KB long, meaning each request to the Dapp table is probably metered at 5 or 6 requests for the most part. (They're metered in 4 KB chunks)\r\n\r\nThat means there's probably cost savings to be had if we can offload the ABI to S3, and instead store a key name in our Dynamo table",
      "comments": []
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Eximchain/terraform-aws-dappbot/issues/108",
    "content": {
      "title": "Spec out technical metrics we'd like to track",
      "body": "As a product stakeholder, I want to be able to retrieve business-relevant data about our customers.  As a service operator, I want to be able to retrieve metrics regarding our technical performance.\r\n\r\nThese concerns both require us to set up a system of metrics and logging.  In general, we want to instrument customer operations and errors.  This issue is for specifying\r\n\r\n- What business metrics we want to track\r\n- What technical metrics we want to track\r\n- A high-level implementation plan, in particular what tools and services we will use and what we want the \"output\" to look like",
      "comments": [
        "For technical metrics we may want to set up a Cloudwatch dashboard. Lambda functions also have their own dashboards, should be able to pick some of those graphs to import on our dashboard.\r\n\r\nFor business metrics we have a few options.\r\n\r\nData Warehouses and Data Lakes are the two models of service built for business analytics.  Data Warehouses are for heavily processed data that we want to be able to do fast and complex queries on.  Data Lakes are more free-form and lower cost.\r\n\r\n<img width=\"1192\" alt=\"Screen Shot 2019-09-27 at 1 32 02 PM\" src=\"https://user-images.githubusercontent.com/4107447/65789487-7e47f880-e12b-11e9-86e2-3482191e83a7.png\">\r\n\r\nDepending on exactly what we want it might not make sense to use those products.  We could roll something ourselves using more basic services if what we want is simple enough.\r\n\r\nFinally, we could purchase a third party solution, for example: https://amplitude.com",
        "We ended up deciding to go with Amplitude, and there's now a new issue for collecting the set of business metrics we want to track ().  I've updated the title of this issue to reflect that its scope has been narrowed to technical metrics, rather than product/business ones."
      ]
    },
    "codes": [
      "saving",
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/RootPrivileges/terragrunt-aws-modules/issues/8",
    "content": {
      "title": "Host PrivateLink AWS in distinct VPC",
      "body": "After adding [VPC peering](https://github.com/RootPrivileges/terragrunt-aws-modules/issues/5) and [changing the staging account to be preprod](https://github.com/RootPrivileges/terragrunt-aws/issues/2), move to host the AWS Endpoint and Interface Gateways in their own VPC within each account.\r\n\r\nIn particular, this will allow a single set of Gateways to be reused within the account (e.g. sharing the same gateways between staging, dev and test VPCs hosted in the preprod account) for cost purposes.",
      "comments": []
    },
    "codes": [
      "networking",
      "saving",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/RootPrivileges/terragrunt-aws-modules/issues/20",
    "content": {
      "title": "Add Lambda function to periodically aggregate all log files into a single archive",
      "body": "As part of https://github.com/RootPrivileges/terragrunt-aws-modules/issues/19, files will no longer be moved to the cheaper S3 Glacier storage class. To make use of these cost savings, add a Lambda function to group the large number of small files into a single archive, which can be more cost-effectively stored on Glacier.",
      "comments": [
        "https://iolap.com/2020/04/06/how-to-avoid-hidden-amazon-glacier-charges/",
        "https://dev.to/michabahr/archive-your-aws-data-to-reduce-storage-cost-364c"
      ]
    },
    "codes": [
      "saving",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/RootPrivileges/terragrunt-aws-modules/issues/19",
    "content": {
      "title": "Don't use S3 Glacier for logging and cloudtrail buckets",
      "body": "The high cost of transitioning numerous small objects to Glacier far outweighs the benefit of the cheaper storage, especially with the additional 40KB added to each object (when the average object pre-transition appears to be ~6.5KB). Instead, move to Standard-IA after a short time period, and leave files indefinitely.\r\n\r\nEventually, it would probably be much better to have a Lambda function aggregate a large number of historical files into a single archive, and put those objects into Glacier instead.",
      "comments": [
        "http://pragmaticnotes.com/2020/04/22/s3-to-glacier-lifecycle-transition-see-if-its-worth-it/"
      ]
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/GoogleCloudPlatform/terraform-google-nat-gateway/issues/73",
    "content": {
      "title": "Zonal tag does not work",
      "body": "```\r\nresource \"google_compute_route\" \"nat-gateway\" {\r\n  count                  = \"${var.module_enabled ? 1 : 0}\"\r\n  name                   = \"${var.name}nat-${var.zone == \"\" ? lookup(var.region_params[\"${var.region}\"], \"zone\") : var.zone}\"\r\n  project                = \"${var.project}\"\r\n  dest_range             = \"${var.dest_range}\"\r\n  network                = \"${data.google_compute_network.network.self_link}\"\r\n  next_hop_instance      = \"${element(split(\"/\", element(module.nat-gateway.instances[0], 0)), 10)}\"\r\n  next_hop_instance_zone = \"${var.zone == \"\" ? lookup(var.region_params[\"${var.region}\"], \"zone\") : var.zone}\"\r\n  tags                   = [\"${compact(concat(list(\"${var.name}nat-${var.region}\"), var.tags))}\"]\r\n  priority               = \"${var.route_priority}\"\r\n}\r\n```\r\n\r\nThe only source tag being created here is \"${var.name}nat-${var.region}\", there is no zonal tag being created.",
      "comments": [
        "Good point, what is your use case for the zonal tag?",
        "My set of instances is small (all in one zone), and there is a non zero\r\ncost to inter zone traffic.",
        "Are you trying to put a nat gateway in multiple zones within the same region?\r\n\r\nWhy does not just setting the zone variable and using the regional tag not work?",
        "I think it creates the gateway in us-central1-f by default looking at the zone map in variables, which would incur that nonzero cost if my resources are all in us-central1-a.\r\n\r\nDon't know if it's important to officially support as the cost isn't very high but as it is right now the documentation / usage examples in the README are inconsistent with the outputs that are actually produced.",
        "In your case I would say use the `zone` parameter to set the zone to `us-central1-a` the default zone map is just there for convenience.  \r\n\r\nThe README does need to be updated, thanks for pointing that out. ",
        "Switched over to using regional tag and all is working now, closing this issue."
      ]
    },
    "codes": [
      "area",
      "awareness"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/khuedoan/homelab/issues/23",
    "content": {
      "title": "Automated offsite backup",
      "body": "- [x] Evaluate object storage providers (cost is a major factor, S3 compatible is a plus):\r\n  - [x] AWS S3 Glacier ([pricing](https://aws.amazon.com/s3/glacier/pricing))\r\n  - [ ] ~Backblaze B2 ([pricing](https://www.backblaze.com/b2/cloud-storage-pricing.html))~\r\n  - [ ] ~Oracle Archive Storage ([pricing](https://www.oracle.com/cloud/storage/pricing))~\r\n  - [ ] ~Wasabi ([pricing](https://wasabi.com/cloud-storage-pricing))~\r\n  - [ ] ~Install Minio on my other cluster at [khuedoan/horus](https://github.com/khuedoan/horus) (free, but limited storage at about 100GB)~\r\n- [ ] Evaluate backup tool:\r\n  - [ ] Built in backup feature in Longhorn\r\n  - [ ] [K8up](https://k8up.io)\r\n  - [ ] [Velero](https://velero.io)\r\n- [ ] Implement the above\r\n- [ ] Update docs",
      "comments": [
        " @user  i would recommend wasabi [pricing](https://wasabi.com/cloud-storage-pricing/) \r\nBy the way very nice project!",
        "Thanks, added to the list.",
        "After comparing the prices and consider my _current_ usage (a few hundreds GB), I decided to go with **AWS S3 Glacier Deep Archive**.\r\n\r\nBackup cost:\r\n\r\n- Storage cost: $1.01376/TB/month\r\n- Inbound data transfer: Free\r\n\r\nRestore cost:\r\n\r\n- Retrieval (bulk, within 48 hours): $2.56/TB\r\n- Outbound data transfer: $0.09/GB or $92.16/TB\r\n\r\nFull calculation:\r\n\r\n```\r\n    1,024 GB per month / 10 GB average item size = 102.40 unrounded number of items\r\n    Round up by 1 (102.4000) = 103 number of items\r\n    103 number of items x 32 KB = 3,296.00 KB overhead\r\n    3,296.00 KB overhead / 1048576 KB in a GB = 0.003143 GB overhead\r\n    0.003143 GB overhead x 0.00099 USD = 0.0000031 USD (Glacier Deep Archive storage overhead cost)\r\n    Glacier Deep Archive storage overhead cost: 0.0000031 USD\r\n    103 number of items x 8 KB = 824.00 KB overhead\r\n    824.00 KB overhead / 1048576 KB in a GB = 0.000786 GB overhead\r\n    Tiered price for: 0.000786 GB\r\n    0.000786 GB x 0.0230000000 USD = 0.00 USD\r\n    Total tier cost = 0.0000181 USD (S3 Standard storage overhead cost)\r\n    S3 Standard storage overhead cost: 0.0000181 USD\r\n    1,024 GB per month x 0.00099 USD = 1.01376 USD (Glacier Deep Archive storage cost)\r\n    Glacier Deep Archive storage cost: 1.01376 USD\r\n    0.0000031 USD + 0.0000181 USD + 1.01376 USD = 1.013781 USD (total Glacier Deep Archive storage cost)\r\n    1,000 requests x 0.00005 USD = 0.05 USD (Cost for PUT, COPY, POST, LIST requests)\r\n    1 requests x 0.000025 USD = 0.00 USD (Cost for Restore requests (Bulk))\r\n    1,024 GB per month x 0.0025 USD = 2.56 USD (Cost for Glacier Deep Archive Data Retrieval (Bulk))\r\n    1.013781 USD + 0.05 USD + 2.56 USD = 3.62 USD (S3 Glacier Deep Archive cost)\r\n\r\n    **S3 Glacier Deep Archive cost (monthly): 3.62 USD**\r\n\r\n    Inbound:\r\n    Internet: 1024 GB x 0 USD per GB = 0.00 USD\r\n    Outbound:\r\n    Internet: 1024 GB x 0.09 USD per GB = 92.16 USD \r\n\r\n    **Data Transfer cost (monthly): 92.16 USD**\r\n```\r\n\r\nSo data will be backed up in 2 places:\r\n\r\n- Local Minio on my NAS\r\n- AWS S3 Glacier\r\n\r\nDistributed storage also improve the resilience with 2 (or 3) replicas.",
        "Accidentally closed. Still need to implement the backup.",
        "Probably you already know this: we should test the backup."
      ]
    },
    "codes": [
      "awareness",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/maddevsio/aws-eks-base/issues/263",
    "content": {
      "title": "bug: Cluster autoscaler can't scale managed nodepools from 0",
      "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue. **YOU MAY DELETE THE PREREQUISITES SECTION.**\r\n\r\n- [x] I am running the latest version\r\n- [x] I checked the documentation and found no answer\r\n- [x] I checked to make sure that this issue has not already been filed\r\n\r\n# Expected Behavior\r\n\r\nIt's possible to have nodepools with 0 nodes at launch\r\n\r\n# Current Behavior\r\n\r\nCA can't scale managed nodepools if their size is 0 (ci nodepool, bottlerocket nodepool)\r\n\r\nSome notes from EKS cluster autoscaling documentation:\r\n```\r\nScaling from zero\r\n\r\nCluster Autoscaler can scale node groups to and from zero. This might result in a significant cost savings. The Cluster Autoscaler detects the CPU, memory, and GPU resources of an Auto Scaling group by inspecting the InstanceType that is specified in its LaunchConfiguration or LaunchTemplate. Some pods require additional resources such as WindowsENI or PrivateIPv4Address. Or they might require specific NodeSelectors or Taints. These latter two can't be discovered from the LaunchConfiguration. However, the Cluster Autoscaler can account for these factors by discovering them from the following tags on the Auto Scaling group.\r\n\r\nKey: k8s.io/cluster-autoscaler/node-template/resources/$RESOURCE_NAME\r\nValue: 5\r\nKey: k8s.io/cluster-autoscaler/node-template/label/$LABEL_KEY\r\nValue: $LABEL_VALUE\r\nKey: k8s.io/cluster-autoscaler/node-template/taint/$TAINT_KEY\r\nValue: NoSchedule\r\n```\r\n\r\nRelated issues:\r\nhttps://github.com/aws/containers-roadmap/issues/608\r\nhttps://github.com/aws/containers-roadmap/issues/724\r\n\r\n\r\n",
      "comments": [
        "How we can manually add necessary tags https://github.com/aws/containers-roadmap/issues/724#issuecomment-1095241320"
      ]
    },
    "codes": [
      "saving",
      "cluster",
      "feature"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/thoughtbot/flightdeck/issues/26",
    "content": {
      "title": "Improve logging experience",
      "body": "We deploy Fluent Bit and we're sending logs to Cloudwatch in the AWS platform. Cloudwatch is hard to search and expensive for data retention. It's also only viewable from workload accounts.",
      "comments": [
        "We may be able to improve the experience with Cloudwatch:\r\n\r\n* Searching via Insights is much easier. We can use structured logs and pre-configured Insights queries to simplify this.\r\n* It's not actually data retention that's expensive, but data ingestion. Reducing the default logging volume helps with this.\r\n* We can either send logs to a centralized monitoring account or enable cross-account access."
      ]
    },
    "codes": [
      "awareness",
      "feature",
      "storage"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/openshift/okd/issues/40",
    "content": {
      "title": "Several errors when installing OKD 4.4.0 on AWS",
      "body": "When I'm trying to install OKD on AWS the installation will crash with several errors.\r\n\r\n```console\r\n$ openshift-install version\r\nopenshift-install unreleased-master-2054-g7e1413f19c0af31bd7919d3067080a9ec787e135-dirty\r\nbuilt from commit 7e1413f19c0af31bd7919d3067080a9ec787e135\r\nrelease image registry.svc.ci.openshift.org/origin/release@sha256:3b68e0f037c0ef3359ba3707d623a00402d0467d5ebafa8fea34ad326a27ed30\r\n```\r\n\r\n### Installer Console Output\r\n<details>\r\n  <summary>Click to expand</summary>\r\n\r\n```console\r\n$ openshift-install create cluster\r\nINFO Consuming Install Config from target directory \r\nINFO Creating infrastructure resources...         \r\nINFO Waiting up to 30m0s for the Kubernetes API at https://api.dev.example.com:6443... \r\nINFO API v1.17.0 up                               \r\nINFO Waiting up to 30m0s for bootstrapping to complete... \r\nINFO Destroying the bootstrap resources...        \r\nERROR                                              \r\nERROR Warning: Resource targeting is in effect     \r\nERROR                                              \r\nERROR You are creating a plan with the -target option, which means that the result \r\nERROR of this plan may not represent all of the changes requested by the current \r\nERROR configuration.                               \r\nERROR                                                      \r\nERROR The -target option is not for routine use, and is provided only for \r\nERROR exceptional situations such as recovering from errors or mistakes, or when \r\nERROR Terraform specifically suggests to use it as part of an error message. \r\nERROR                                              \r\nERROR                                              \r\nERROR Warning: Applied changes may be incomplete   \r\nERROR                                              \r\nERROR The plan was created with the -target option in effect, so some changes \r\nERROR requested in the configuration may have been ignored and the output values may \r\nERROR not be fully updated. Run the following command to verify that no other \r\nERROR changes are pending:                         \r\nERROR     terraform plan                           \r\nERROR                                               \r\nERROR Note that the -target option is not suitable for routine use, and is provided \r\nERROR only for exceptional situations such as recovering from errors or mistakes, or \r\nERROR when Terraform specifically suggests to use it as part of an error message. \r\nERROR\r\nINFO Waiting up to 30m0s for the cluster at https://api.dev.example.com:6443 to initialize... \r\nE0121 14:09:10.603015   29692 reflector.go:280] k8s.io/client-go/tools/watch/informerwatcher.go:146: Failed to watch *v1.ClusterVersion: Get https://api.dev.example.com:6443/apis/config.openshift.io/v1/clusterversions?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dversion&resourceVersion=6227&timeoutSeconds=434&watch=true: dial tcp 3.125.232.148:6443: connect: connection refused\r\nE0121 14:09:11.670914   29692 reflector.go:280] k8s.io/client-go/tools/watch/informerwatcher.go:146: Failed to watch *v1.ClusterVersion: Get https://api.dev.example.com:6443/apis/config.openshift.io/v1/clusterversions?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dversion&resourceVersion=6227&timeoutSeconds=501&watch=true: dial tcp 3.125.227.80:6443: connect: connection refused\r\nE0121 14:23:19.657660   29692 reflector.go:280] k8s.io/client-go/tools/watch/informerwatcher.go:146: Failed to watch *v1.ClusterVersion: Get https://api.dev.example.com:6443/apis/config.openshift.io/v1/clusterversions?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dversion&resourceVersion=12851&timeoutSeconds=428&watch=true: dial tcp 3.124.42.151:6443: connect: connection refused\r\nE0121 14:23:26.410308   29692 reflector.go:280] k8s.io/client-go/tools/watch/informerwatcher.go:146: Failed to watch *v1.ClusterVersion: the server is currently unable to handle the request (get clusterversions.config.openshift.io)\r\nI0121 14:23:37.751957   29692 trace.go:116] Trace[1562460260]: \"Reflector ListAndWatch\" name:k8s.io/client-go/tools/watch/informerwatcher.go:146 (started: 2020-01-21 14:23:27.411104 +0100 CET m=+2282.244719334) (total time: 10.340616233s):\r\nTrace[1562460260]: [10.340594715s] [10.340594715s] Objects listed\r\nE0121 14:25:45.974279   29692 reflector.go:280] k8s.io/client-go/tools/watch/informerwatcher.go:146: Failed to watch *v1.ClusterVersion: Get https://api.dev.example.com:6443/apis/config.openshift.io/v1/clusterversions?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dversion&resourceVersion=12958&timeoutSeconds=453&watch=true: dial tcp 3.125.232.148:6443: connect: connection refused\r\nE0121 14:25:47.045923   29692 reflector.go:280] k8s.io/client-go/tools/watch/informerwatcher.go:146: Failed to watch *v1.ClusterVersion: Get https://api.dev.example.com:6443/apis/config.openshift.io/v1/clusterversions?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dversion&resourceVersion=12958&timeoutSeconds=563&watch=true: dial tcp 3.124.42.151:6443: connect: connection refused\r\nE0121 14:25:48.109525   29692 reflector.go:280] k8s.io/client-go/tools/watch/informerwatcher.go:146: Failed to watch *v1.ClusterVersion: Get https://api.dev.example.com:6443/apis/config.openshift.io/v1/clusterversions?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dversion&resourceVersion=12958&timeoutSeconds=549&watch=true: dial tcp 3.125.232.148:6443: connect: connection refused\r\nE0121 14:25:55.373676   29692 reflector.go:280] k8s.io/client-go/tools/watch/informerwatcher.go:146: Failed to watch *v1.ClusterVersion: the server is currently unable to handle the request (get clusterversions.config.openshift.io)\r\nI0121 14:26:12.074432   29692 trace.go:116] Trace[1614350600]: \"Reflector ListAndWatch\" name:k8s.io/client-go/tools/watch/informerwatcher.go:146 (started: 2020-01-21 14:25:56.37865 +0100 CET m=+2431.209468384) (total time: 15.695454553s):\r\nTrace[1614350600]: [15.695429463s] [15.695429463s] Objects listed\r\nERROR Cluster operator authentication Degraded is True with IngressStateEndpoints_MissingSubsets::RouteStatus_FailedHost: IngressStateEndpointsDegraded: No subsets found for the endpoints of oauth-server\r\nRouteStatusDegraded: route is not available at canonical host oauth-openshift.apps.dev.example.com: [] \r\nINFO Cluster operator authentication Progressing is Unknown with NoData:  \r\nINFO Cluster operator authentication Available is Unknown with NoData:  \r\nINFO Cluster operator console Progressing is True with RouteSyncProgressingFailedHost: RouteSyncProgressing: route is not available at canonical host [] \r\nINFO Cluster operator console Available is Unknown with NoData:  \r\nINFO Cluster operator image-registry Available is False with NoReplicasAvailable: The deployment does not have available replicas \r\nINFO Cluster operator image-registry Progressing is True with DeploymentNotCompleted: The deployment has not completed \r\nERROR Cluster operator ingress Degraded is True with IngressControllersDegraded: Some ingresscontrollers are degraded: default \r\nINFO Cluster operator ingress Progressing is True with Reconciling: Not all ingress controllers are available.\r\nMoving to release version \"4.4.0-0.okd-2020-01-20-231545\".\r\nMoving to ingress-controller image version \"registry.svc.ci.openshift.org/origin/4.4-2020-01-20-231545@sha256:c5aa779b80bf6b7f9e98a4f85a3fec5a17543ce89376fc13e924deedcd7298cf\". \r\nINFO Cluster operator ingress Available is False with IngressUnavailable: Not all ingress controllers are available. \r\nINFO Cluster operator insights Disabled is True with Disabled: Health reporting is disabled \r\nINFO Cluster operator kube-storage-version-migrator Available is False with _NoMigratorPod: Available: deployment/migrator.openshift-kube-storage-version-migrator: no replicas are available \r\nINFO Cluster operator monitoring Progressing is True with RollOutInProgress: Rolling out the stack. \r\nERROR Cluster operator monitoring Degraded is True with UpdatingAlertmanagerFailed: Failed to rollout the stack. Error: running task Updating Alertmanager failed: waiting for Alertmanager Route to become ready failed: waiting for RouteReady of alertmanager-main: no status available for alertmanager-main \r\nINFO Cluster operator monitoring Available is False with :  \r\nINFO Cluster operator support Disabled is True with : Health reporting is disabled \r\nFATAL failed to initialize the cluster: Working towards 4.4.0-0.okd-2020-01-20-231545: 99% complete\r\n```\r\n</details>\r\n\r\n### install-config.yaml\r\nI set the replicas to 1 and just for this issue I also anonymized the domain.\r\n<details>\r\n  <summary>Click to expand</summary>\r\n  \r\n  ```yaml\r\napiVersion: v1\r\nbaseDomain: example.com\r\ncompute:\r\n- hyperthreading: Enabled\r\n  name: worker\r\n  platform: {}\r\n  replicas: 2\r\ncontrolPlane:\r\n  hyperthreading: Enabled\r\n  name: master\r\n  platform: {}\r\n  replicas: 3\r\nmetadata:\r\n  creationTimestamp: null\r\n  name: dev\r\nnetworking:\r\n  clusterNetwork:\r\n  - cidr: 10.128.0.0/14\r\n    hostPrefix: 23\r\n  machineCIDR: 10.0.0.0/16\r\n  networkType: OVNKubernetes\r\n  serviceNetwork:\r\n  - 172.30.0.0/16\r\nplatform:\r\n  aws:\r\n    region: eu-central-1\r\npublish: External\r\npullSecret: '{\"auths\":{\"fake\":{\"auth\": \"bar\"}}}'\r\nsshKey: |\r\n  ssh-rsa ...\r\n```\r\n</details>\r\n\r\n### Gathered data about cluster\r\nhttps://drive.google.com/file/d/11mi2Z_3Oye3bErvQwHJ4f-2pRrPebVAZ/view?usp=sharing",
      "comments": [
        "Does it work with 3 masters and 2 workers? \r\n\r\nOne master and one node won't work - ingress, image registry attempt to place on different nodes (and master is tainted). So now many components in monitoring throw `0/1 nodes are available: 1 node(s) had taints that the pod didn''t tolerate`.\r\n\r\nAlternatively try https://github.com/openshift/enhancements/blob/master/enhancements/compact-clusters.md",
        "Ok, I did it and got some other errors. Do you need the gathered data again? You can find the console output below.\r\n\r\n### Installer Console Output\r\n<details>\r\n  <summary>Click to expand</summary>\r\n\r\n```console\r\n$ openshift-install create cluster\r\nINFO Consuming Install Config from target directory \r\nINFO Creating infrastructure resources...         \r\nINFO Waiting up to 30m0s for the Kubernetes API at https://api.dev.example.com:6443... \r\nINFO API v1.17.0 up                               \r\nINFO Waiting up to 30m0s for bootstrapping to complete... \r\nINFO Destroying the bootstrap resources...        \r\nERROR                                              \r\nERROR Warning: Resource targeting is in effect     \r\nERROR                                              \r\nERROR You are creating a plan with the -target option, which means that the result \r\nERROR of this plan may not represent all of the changes requested by the current \r\nERROR configuration.                               \r\nERROR                                                      \r\nERROR The -target option is not for routine use, and is provided only for \r\nERROR exceptional situations such as recovering from errors or mistakes, or when \r\nERROR Terraform specifically suggests to use it as part of an error message. \r\nERROR                                              \r\nERROR                                              \r\nERROR Warning: Applied changes may be incomplete   \r\nERROR                                              \r\nERROR The plan was created with the -target option in effect, so some changes \r\nERROR requested in the configuration may have been ignored and the output values may \r\nERROR not be fully updated. Run the following command to verify that no other \r\nERROR changes are pending:                         \r\nERROR     terraform plan                           \r\nERROR                                               \r\nERROR Note that the -target option is not suitable for routine use, and is provided \r\nERROR only for exceptional situations such as recovering from errors or mistakes, or \r\nERROR when Terraform specifically suggests to use it as part of an error message. \r\nERROR                                              \r\nINFO Waiting up to 30m0s for the cluster at https://api.dev.example.com:6443 to initialize... \r\nINFO Cluster operator insights Disabled is True with Disabled: Health reporting is disabled \r\nINFO Cluster operator monitoring Progressing is True with RollOutInProgress: Rolling out the stack. \r\nERROR Cluster operator monitoring Degraded is True with UpdatingPrometheusK8SFailed: Failed to rollout the stack. Error: running task Updating Prometheus-k8s failed: waiting for Prometheus object changes failed: waiting for Prometheus: expected 2 replicas, updated 1 and available 1 \r\nINFO Cluster operator monitoring Available is False with :  \r\nINFO Cluster operator support Disabled is True with : Health reporting is disabled \r\nFATAL failed to initialize the cluster: Cluster operator monitoring is still updating\r\n```\r\n</details>",
        "Hmm, only one replicas of Prometheus rolled out. Check pod status in `openshift-monitoring` namespace?\r\n\r\nFor more info it would be best to attach `oc adm must-gather` output",
        "### Gathered data about cluster\r\nhttps://drive.google.com/file/d/1G0M3tLDiK_kCYBFitDjQerxgO9WItVuV/view?usp=sharing\r\n\r\n### Pods in openshift-monitoring namespace\r\n<details>\r\n  <summary>Click to expand</summary>\r\n\r\n```console\r\nNAME                                           READY   STATUS    RESTARTS   AGE\r\nalertmanager-main-0                            3/3     Running   0          173m\r\nalertmanager-main-1                            3/3     Running   0          173m\r\nalertmanager-main-2                            3/3     Running   0          174m\r\ncluster-monitoring-operator-56b8bb6dff-nq6pw   1/1     Running   0          179m\r\ngrafana-6dfbbd887d-mpwph                       2/2     Running   0          174m\r\nkube-state-metrics-5d9d6cc457-jc7fk            3/3     Running   0          179m\r\nnode-exporter-5nk5r                            2/2     Running   0          177m\r\nnode-exporter-ktmr6                            2/2     Running   0          179m\r\nnode-exporter-l2gbb                            2/2     Running   0          179m\r\nnode-exporter-s58f6                            2/2     Running   0          179m\r\nnode-exporter-z8gkz                            2/2     Running   0          174m\r\nopenshift-state-metrics-5f8d4bdb56-d8p98       3/3     Running   0          179m\r\nprometheus-adapter-688f556698-6fkn2            1/1     Running   0          174m\r\nprometheus-adapter-688f556698-fqdnf            1/1     Running   0          174m\r\nprometheus-k8s-0                               7/7     Running   1          173m\r\nprometheus-k8s-1                               0/7     Pending   0          172m\r\nprometheus-operator-745f8c55f8-6kmn5           1/1     Running   0          174m\r\nthanos-querier-597c78cb95-9fd87                4/4     Running   0          172m\r\nthanos-querier-597c78cb95-v9wpw                4/4     Running   0          172m\r\n```\r\n</details>",
        "Interesting, all the nodes seems healthy and yet `cluster-monitoring-operator` is not happy.\r\n\r\nPlease create an issue at https://github.com/openshift/cluster-monitoring-operator, attach the must-gather and link it here please",
        "https://github.com/openshift/cluster-monitoring-operator/issues/620",
        "> ERROR Cluster operator ingress Degraded is True with IngressControllersDegraded: Some ingresscontrollers are degraded: default \r\n\r\nSeems like this happened before monitoring error, so seems like a networking issue?",
        "Seems 2 worker nodes is not sufficient: https://github.com/openshift/cluster-monitoring-operator/issues/620#issuecomment-577088556\r\n\r\nCompact cluster would probably be the safest option to save costs. Alternatively Prometheus operator can be tweaked to run just one replica (note that this would make your metrics non-HA) ",
        "cluster-monitoring-operator would reconcile any tweaking to Prometheus operator, you can tweak it via the ConfigMap but I don't think we expose the replicas there.",
        "the must-gather outlines in the last event that worker nodes have insufficient CPUs to schedule prometheus:\r\n```\r\nkind: Event\r\n  lastTimestamp: null\r\n  message: '0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had taints that\r\n    the pod didn''t tolerate.'\r\n  metadata:\r\n    creationTimestamp: \"2020-01-21T16:30:14Z\"\r\n```\r\n\r\nHence, please add another worker node. In OCP, by default we install three worker nodes.",
        "You're right. Everything works fine using 3 workers.\r\n\r @user  I think the best option to save costs would be if the installer could use spot instances \ud83d\ude09 https://github.com/openshift/installer/issues/1287"
      ]
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/openshift/okd/issues/250",
    "content": {
      "title": "Documentation missing for All-in-one OKD 4",
      "body": "**Describe the bug**\r\nThe documentation for OKD 4 doesn't explain how to deploy an all in one deployment. We were planning to use something like that in the CI pipeline of the submariner project http://github.com/submariner-io to reduce the cost of deploying a fully fledged OKD 4\r\n\r\n**Version**\r\nOKD 4 documentation\r\n\r\n**How reproducible**\r\nN/A\r\n\r\n**Log bundle**\r\nN/A",
      "comments": [
        "It should be as simple as setting \r\n```\r\ncompute:\r\n- name: worker\r\n  replicas: 0\r\ncontrolPlane:\r\n  name: master\r\n  replicas: 1\r\n```\r\nin `install-config.yaml`, but the docs should also mention that the host cannot be modified via MCO - and the cluster cannot be updated",
        "Again, documentation about this is important. Currently, the docs state that you need at least three master notes ...\r\nI am also waiting for non-HA setups for some cases, so I'm keen on that.\r\nI am also wondering if setting the # of masters to 1 and no workers if then the deployment of monitoring will work?\r\nBecause that currently installs with multiple instances.\r\nI have already tried installing cluster-logging with only 1 instance (only one instance of Elasticsearch) and at least that failed.",
        "> and the cluster cannot be updated\r\n\r\nThis will probably remain a fundamental limit? As there is no redundancy to perform a rolling updae?\r\n",
        ">I am also wondering if setting the # of masters to 1 and no workers if then the deployment of monitoring will work?\r\n\r\nYes. Master would become schedulable and run all pods\r\n\r\n>This will probably remain a fundamental limit? As there is no redundancy to perform a rolling updae?\r\n\r\nCorrect. MCO requires maxUnavalable to be set for a pool (minimum 1) during upgrade, which is impossible to satisfy in single master deployment",
        "I'll give it a try one of these days.",
        "Fixed by https://github.com/openshift/okd/pull/253"
      ]
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/openshift/okd/issues/48",
    "content": {
      "title": "Azure: Upload of decompressed FCOS image is very slow",
      "body": "Hi,\r\n\r\nI couldn't manage to get the new Azure installer working because the upload of the decompressed (fcos image is xz compressed) takes so long (more than 30 Minutes on my PC) that this procedure seems not to be feasable to me.\r\n\r\nPlease support a procedure where users can upload the fcos image on their own to a storage account which is in a different resource group and will be referenced from all cluster VMs. \r\n\r\nA method to configure this storage blob container is necessary (through env variables which are fed to the installer?).\r\n\r\nThe time for the upload would be necessary only once for all OKD clusters and must not be spent for each cluster again and again.\r\n\r\nBest regards,\r\n\r\nJosef\r\n",
      "comments": [
        "I am not familiar with Azure, what's the recommendation here? \r\n\r\nIf Azure doesn't give us tools to upload the image faster there isn't much we can do about it",
        "The problem is in my opinion, that it takes almost half an hour every time the image is uploaded before the VMs get started. Every time we create a cluster.\r\n\r\nThe upload is limited by the upload speed of your provider.\r\n\r\nWe could ...\r\n\r\n1. create a VM in Azure which downloads, decompressed and uploads the image to the blob storage. Because we still are on Azure everything works much faster.\r\n\r\n2. Manually upload the vhd file independently from the installer in a dedicated resource group and use it for multiple clusters. In this case we need to override the location from where the image gets its VHD file. Maybe that\u2019s already possible through environment variables or should that be asked by the installer for Azure?\r\n\r\n3. Get the Image in the Azure Marketplace\r\n\r\nThe 3rd option would be long term, the 2nd option short term solution in my opinion.\r\n\r\n",
        "https://github.com/openshift/installer/pull/3033",
        "Just tried to deploy a cluster to azure, I'm running into the same problems. \r @user How do I use your workaround? \r\nopenshift/installer#3033",
        "@sanderkooger:\r\nIn my video I explain how you can modify the openshift installer to get OKD 4.4 installed on Azure:\r\n\r\nhttps://www.youtube.com/watch?v=PXZn87FCIYk\r\n\r\nHave fun (and don't forget to like my video if it helped you :-) ).",
        "It's on minute 14:13",
        "One generic thing you can do is just boot up any Linux machine in Azure and run `openshift-install` from there over ssh or whatever.\r\n\r\n(There's also https://github.com/openshift/hive/ once you have one cluster running)\r\n",
        "I just created a fresh Ubuntu box and tried from within azure. No joy... \r\n\r\n```\r\nDEBUG azurerm_storage_blob.rhcos_image: Still creating... [50s elapsed]\r\nDEBUG azurerm_storage_blob.rhcos_image: Still creating... [1m0s elapsed]\r\nDEBUG azurerm_storage_blob.rhcos_image: Still creating... [1m10s elapsed]\r\nDEBUG azurerm_storage_blob.rhcos_image: Still creating... [1m20s elapsed]\r\nDEBUG azurerm_storage_blob.rhcos_image: Still creating... [1m30s elapsed]\r\nDEBUG azurerm_storage_blob.rhcos_image: Creation complete after 1m31s [id=https://cluster3iydm.blob.core.windows.net/vhd/rhcos3iydm.vhd]\r\nDEBUG azurerm_image.cluster: Creating...\r\nERROR\r\nERROR Warning: \"resource_group_name\": [DEPRECATED] This field has been deprecated and is no longer used - will be removed in 2.0 of the Azure Provider\r\nERROR\r\nERROR   on ../../tmp/openshift-install-362141893/main.tf line 163, in resource \"azurerm_storage_container\" \"vhd\":\r\nERROR  163: resource \"azurerm_storage_container\" \"vhd\" {\r\nERROR\r\nERROR (and 3 more similar warnings elsewhere)\r\nERROR\r\nERROR\r\nERROR Error: compute.ImagesClient#CreateOrUpdate: Failure sending request: StatusCode=400 -- Original Error: Code=\"InvalidParameter\" Message=\"The source blob https://cluster3iydm.blob.core.windows.net/vhd/rhcos3iydm.vhd is not a page blob.\" Target=\"disks\"\r\nERROR\r\nERROR   on ../../tmp/openshift-install-362141893/main.tf line 179, in resource \"azurerm_image\" \"cluster\":\r\nERROR  179: resource \"azurerm_image\" \"cluster\" {\r\nERROR\r\nERROR\r\nFATAL failed to fetch Cluster: failed to generate asset \"Cluster\": failed to create cluster: failed to apply using Terraform\r\n```\r\n\r\nIts my first cluster so hyve is not an option. \r\n\r\n",
        " @user I'm trying your YouTube vid but on cherry-pick item give me an error.\r\n\r\n```\r\ngit cherry-pick 05a7dff57a68134a5457e98864e62783221a5aa\r\nfatal: bad revision '05a7dff57a68134a5457e98864e62783221a5aa'\r\n```\r\n ",
        "@sanderkooger:\r\nHm ... I just tried it out again and on my site it works (git version 2.17.1)",
        " @user  \r\nI reported this \"is not a page blob\" several times. My hack also takes care about that.",
        " @user Thats weird as I can cherrypick the other commits. Its just the last one i cant pick. \r\n\r\n",
        " @user \r\nYou can leave that one away. With this commit the installation will do a little bit faster but it should work without it for the moment.\r\n",
        " @user \r\nCould you try out this and confirm that it works, please?\r\n\r\nhttps://github.com/openshift/installer/pull/3563\r\n\r\nYou still must run this installer in an Azure Cloud Shell. I tried it several times and it works now.",
        " @user Is there a place where we can chat privately? Answered in your new issue. Having issues using your YouTube as i have to copy the identifiers for the commits by hand.  Mail me at sander@thisisfashion.tv please. ",
        " @user \r\nMeet me in the kubernetes/#openshift-dev Slack channel.",
        "If you put the latest nightly in an Azure VM or Cloud Shell, the Installation will succeed without further patches.",
        "Maybe ill try, I actually have been doing the math, The infrastructure is meant for a small startup ( unfunded) and I kind of figured that maybe its better to run AKS instead of openshift, It\u2019s a little less developer friendly in terms of automates stream2images, but on the other hand During development I only have to run one workernode and a mysql db.\r\n\r\nRunning 2 4core masternodes, 2 4core infranodes, and the workernodes gets a little too expensive too fast.\r\n\r\nThank you for the support though! And good work!\r\n\r\nFrom: Josef Meier <notifications@github.com>\r\nSent: Friday, 8 May 2020 17:09\r\nTo: openshift/okd <okd@noreply.github.com>\r\nCc: Sander Kooger <sander@thisisfashion.tv>; Mention <mention@noreply.github.com>\r\nSubject: Re: [openshift/okd] Azure: Upload of decompressed FCOS image is very slow (#48)\r\n\r\n\r\nIf you put the latest nightly in an Azure VM or Cloud Shell, the Installation will succeed without further patches.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/openshift/okd/issues/48#issuecomment-625861824>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AET7V2S4H27PCBS2ME3CTCDRQQOAJANCNFSM4KM6CWPQ>.\r\n\r\nNote: This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. You cannot use or forward any attachments in the email. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. The integrity and security of this message cannot be guaranteed on the Internet.\r\n",
        "Issues go stale after 90d of inactivity.\n\nMark the issue as fresh by commenting `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\nExclude this issue from closing by commenting `/lifecycle frozen`.\n\nIf this issue is safe to close now please do so with `/close`.\n\n/lifecycle stale",
        "Stale issues rot after 30d of inactivity.\n\nMark the issue as fresh by commenting `/remove-lifecycle rotten`.\nRotten issues close after an additional 30d of inactivity.\nExclude this issue from closing by commenting `/lifecycle frozen`.\n\nIf this issue is safe to close now please do so with `/close`.\n\n/lifecycle rotten\n/remove-lifecycle stale",
        "Rotten issues close after 30d of inactivity.\n\nReopen the issue by commenting `/reopen`.\nMark the issue as fresh by commenting `/remove-lifecycle rotten`.\nExclude this issue from closing again by commenting `/lifecycle frozen`.\n\n/close",
        "@openshift-bot: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/openshift/okd/issues/48#issuecomment-780002024):\n\n>Rotten issues close after 30d of inactivity.\n>\n>Reopen the issue by commenting `/reopen`.\n>Mark the issue as fresh by commenting `/remove-lifecycle rotten`.\n>Exclude this issue from closing again by commenting `/lifecycle frozen`.\n>\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"
      ]
    },
    "codes": [
      "instance",
      "awareness",
      "increase"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/openshift/okd/issues/619",
    "content": {
      "title": "OKD 4.5 in AWS - Cluster did not started after stopping master instances for more than 24 hours ",
      "body": "Hi,\r\n\r\nI have installed a new OKD cluster in AWS by following below installation guide and able to login to the cluster. Also converted all worker nodes to SPOT instances. Cluster was working fine as ecpected.\r\nhttps://docs.okd.io/latest/installing/installing_aws/installing-aws-customizations.html\r\nOKD Version: openshift-install-linux-4.5.0-0.okd-2020-10-15-235428\r\n\r\nTo save the cost I have deleted worker nodes(SPOT instances) and stopped master nodes 24 hours back and now I have started master nodes in AWS. However not able to login to cluster and getting below error.\r\n\r\n```\r\nubuntu@machine:~/OKDeployments$ oc login --loglevel=10\r\nI0505 13:09:23.691871   98512 loader.go:375] Config loaded from file:  /home/ubuntu/.kube/config\r\nI0505 13:09:23.692159   98512 round_trippers.go:423] curl -k -v -XHEAD  'https://api.okdqaf.example.com:6443/'\r\nI0505 13:09:23.738409   98512 round_trippers.go:443] HEAD https://api.okdqaf.example.com:6443/  in 46 milliseconds\r\nI0505 13:09:23.738447   98512 round_trippers.go:449] Response Headers:\r\nI0505 13:09:23.738499   98512 round_trippers.go:423] curl -k -v -XHEAD  'https://api.okdqaf.example.com:6443/'\r\nI0505 13:09:23.745916   98512 round_trippers.go:443] HEAD https://api.okdqaf.example.com:6443/ 403 Forbidden in 7 milliseconds\r\nI0505 13:09:23.745943   98512 round_trippers.go:449] Response Headers:\r\nI0505 13:09:23.745949   98512 round_trippers.go:452]     X-Content-Type-Options: nosniff\r\nI0505 13:09:23.745955   98512 round_trippers.go:452]     X-Kubernetes-Pf-Flowschema-Uid: a3d196ac-edd1-4d6d-8a6f-62bb63a9aa19\r\nI0505 13:09:23.745960   98512 round_trippers.go:452]     X-Kubernetes-Pf-Prioritylevel-Uid: ef2615db-8394-40b4-8c42-13556789a6f1\r\nI0505 13:09:23.745966   98512 round_trippers.go:452]     Content-Length: 186\r\nI0505 13:09:23.745971   98512 round_trippers.go:452]     Date: Wed, 05 May 2021 13:09:23 GMT\r\nI0505 13:09:23.745982   98512 round_trippers.go:452]     Audit-Id: 61b84b60-ddba-41fb-b837-1ba3f6ceb7b0\r\nI0505 13:09:23.746000   98512 round_trippers.go:452]     Cache-Control: no-cache, private\r\nI0505 13:09:23.746011   98512 round_trippers.go:452]     Content-Type: application/json\r\nI0505 13:09:23.746035   98512 request_token.go:86] GSSAPI Enabled\r\nI0505 13:09:23.746077   98512 round_trippers.go:423] curl -k -v -XGET  -H \"X-Csrf-Token: 1\" 'https://api.okdqaf.example.com:6443/.well-known/oauth-authorization-server'\r\nI0505 13:09:23.747855   98512 round_trippers.go:443] GET https://api.okdqaf.example.com:6443/.well-known/oauth-authorization-server 200 OK in 1 milliseconds\r\nI0505 13:09:23.747876   98512 round_trippers.go:449] Response Headers:\r\nI0505 13:09:23.747882   98512 round_trippers.go:452]     X-Kubernetes-Pf-Flowschema-Uid: a3d196ac-edd1-4d6d-8a6f-62bb63a9aa19\r\nI0505 13:09:23.747893   98512 round_trippers.go:452]     X-Kubernetes-Pf-Prioritylevel-Uid: ef2615db-8394-40b4-8c42-13556789a6f1\r\nI0505 13:09:23.747904   98512 round_trippers.go:452]     Content-Length: 624\r\nI0505 13:09:23.747915   98512 round_trippers.go:452]     Date: Wed, 05 May 2021 13:09:23 GMT\r\nI0505 13:09:23.747928   98512 round_trippers.go:452]     Audit-Id: 87dde1ba-d4e0-408b-8ea4-2e81431e6213\r\nI0505 13:09:23.747934   98512 round_trippers.go:452]     Cache-Control: no-cache, private\r\nI0505 13:09:23.747939   98512 round_trippers.go:452]     Content-Type: application/json\r\nI0505 13:09:23.750477   98512 round_trippers.go:423] curl -k -v -XGET  -H \"Accept: application/json, */*\" -H \"User-Agent: oc/v4.2.0 (linux/amd64) kubernetes/3f6a83f\" 'https://api.okdqaf.example.com:6443/api/v1/namespaces/openshift/configmaps/motd'\r\nI0505 13:09:23.751639   98512 round_trippers.go:443] GET https://api.okdqaf.example.com:6443/api/v1/namespaces/openshift/configmaps/motd 403 Forbidden in 1 milliseconds\r\nI0505 13:09:23.751661   98512 round_trippers.go:449] Response Headers:\r\nI0505 13:09:23.751672   98512 round_trippers.go:452]     Content-Type: application/json\r\nI0505 13:09:23.751684   98512 round_trippers.go:452]     X-Content-Type-Options: nosniff\r\nI0505 13:09:23.751694   98512 round_trippers.go:452]     X-Kubernetes-Pf-Flowschema-Uid: a3d196ac-edd1-4d6d-8a6f-62bb63a9aa19\r\nI0505 13:09:23.751705   98512 round_trippers.go:452]     X-Kubernetes-Pf-Prioritylevel-Uid: ef2615db-8394-40b4-8c42-13556789a6f1\r\nI0505 13:09:23.751711   98512 round_trippers.go:452]     Content-Length: 303\r\nI0505 13:09:23.751721   98512 round_trippers.go:452]     Date: Wed, 05 May 2021 13:09:23 GMT\r\nI0505 13:09:23.751731   98512 round_trippers.go:452]     Audit-Id: 0c7e5d27-e92e-46ed-9e5b-987fce2cd930\r\nI0505 13:09:23.751743   98512 round_trippers.go:452]     Cache-Control: no-cache, private\r\nI0505 13:09:23.751761   98512 request.go:1068] Response Body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"configmaps \\\"motd\\\" is forbidden: User \\\"system:anonymous\\\" cannot get resource \\\"configmaps\\\" in API group \\\"\\\" in the namespace \\\"openshift\\\"\",\"reason\":\"Forbidden\",\"details\":{\"name\":\"motd\",\"kind\":\"configmaps\"},\"code\":403}\r\nI0505 13:09:23.752162   98512 helpers.go:234] Connection error: Head https://oauth-openshift.apps.okdqaf.example.com: EOF\r\nF0505 13:09:23.752207   98512 helpers.go:115] Unable to connect to the server: EOF\r\n```\r\n\r\nPlease suggest and let me know if any further information needed\r\n\r\nThanks\r\n ",
      "comments": [
        ">OKD Version: openshift-install-linux-4.5.0-0.okd-2020-10-15-235428\r\n\r\nIs it reproducible in latest stable?",
        "`Is it reproducible in latest stable?`\r\n\r\nAs per the customer requirement we need to use openshift-install-linux-4.5.0-0.okd-2020-10-15-235428. I did not tested with latest version. Can you please suggest if this is expected behavior (or)  known issue in the version which I have mentioned so that I can get necessary approvals to try it out in latest stable version\r\n\r\nThank you",
        ">this is expected behavior (or) known issue in the version\r\n\r\nI don't think we have enough information on what is happening or why (and whether its fixed in latest version) - please attach must-gather?",
        "I tried to collect the logs but unable to get logs when I follow below article. \r\nhttps://docs.okd.io/latest/installing/installing-troubleshooting.html\r\n\r\n```\r\nubuntu@machine:~/okd-qaf-cluster$ sudo ./openshift-install gather bootstrap --dir=/home/ubuntu/okd-qaf-cluster/installscripts\r\nFATAL failed to get bootstrap and control plane host addresses from \"/home/ubuntu/okd-qaf-cluster/installscripts/terraform.tfstate\": failed to lookup bootstrap: resource not found \r\n```\r\n\r\n```\r\nubuntu@machine:~/okd-qaf-cluster$ oc adm node-logs --role=master -u kubelet\r\nerror: Unauthorized\r\n```\r\nCan you suggest any other command/reference to fetch necessary logs\r\n\r\nThanks\r\n",
        "`gather bootstrap` won't work - bootstrap node is already destroyed and cluster was installed.\r\n\r\nYou need [must-gather](https://docs.okd.io/latest/support/gathering-cluster-data.html). If you can ssh on master you can use `export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig` there (hopefully that one is fresh)",
        "Thank you for your response. I had set up a new OKD cluster from jumpbox (where cluster setup initiated) with 3 master nodes and 3 worker nodes (SPOT instances). Post successful installation, stopped all master nodes and terminates worker nodes (SPOT instances)\r\n\r\nAfter 24 hours, started all master nodes and not able to connect to cluster.\r\n\r\nLogged into all master nodes and tried to export `/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig` but unfortunelty there is `node-kubeconfigs/localhost.kubeconfig` no such file or directory\r\n\r\nWe also copied `~/.kube/config` file from Jumpbox and pasted in master node and exported that config file to KUBECONFIG. Then I tried to run `oc adm must-gather` but we see error like below\r\n\r\n```\r\n$ oc adm must-gather\r\n[must-gather ] OUT Unauthorized\r\n[must-gather ] OUT\r\n[must-gather ] OUT Using must-gather plugin-in image: quay.io/openshift/origin-must-gather:latest\r\nerror: You must be logged in to the server (Unauthorized)\r\n```\r\n\r\nPlease suggest accordingly. \r\n\r\nThanks",
        "Also I tried below scenario too, still cluster is not accessible and here are the observations. \r\n\r\nPlease suggest\r\n\r\n**Scenario:**\r\nCreated a new cluster with 3 master nodes and 2 worker nodes (SPOT instances)\r\nStopped all 3 master nodes and 2 worker nodes are in running state\r\nStarted again all 3 master nodes after 24 hours.\r\n\r\n**Observations:**\r\nCluster is not accessible, even though all master nodes are in running state\r\nLogged in to Master nodes and observed below are not running.\r\n\r\n```\r\n$ journalctl -b -f -u fstrim.service\r\n-- Logs begin at Sun 2021-05-09 07:20:01 UTC. --\r\nMay 10 08:46:35 localhost systemd[1]: Starting Discard unused blocks on filesystems from /etc/fstab...\r\nMay 10 08:46:36 localhost fstrim[690]: fstrim: failed to parse /etc/fstab: No such file or directory\r\nMay 10 08:46:36 localhost systemd[1]: fstrim.service: Main process exited, code=exited, status=32/n/a\r\nMay 10 08:46:36 localhost systemd[1]: fstrim.service: Failed with result 'exit-code'.\r\nMay 10 08:46:36 localhost systemd[1]: Failed to start Discard unused blocks on filesystems from /etc/fstab.\r\n```\r\n\r\n```\r\n$ journalctl -b -f -u multipathd.service\r\n-- Logs begin at Sun 2021-05-09 07:20:01 UTC. --\r\nMay 10 08:46:31 localhost multipathd[444]: --------shut down-------\r\nMay 10 08:46:31 localhost multipathd[528]: May 10 08:46:31 | /etc/multipath.conf does not exist, blacklisting all devices.\r\nMay 10 08:46:31 localhost multipathd[528]: May 10 08:46:31 | You can run \"/sbin/mpathconf --enable\" to create\r\nMay 10 08:46:31 localhost multipathd[528]: May 10 08:46:31 | /etc/multipath.conf. See man mpathconf(8) for more details\r\nMay 10 08:46:31 localhost multipathd[528]: error -5 receiving packet\r\nMay 10 08:46:31 localhost systemd[1]: Stopping Device-Mapper Multipath Device Controller...\r\nMay 10 08:46:31 localhost systemd[1]: multipathd.service: Control process exited, code=exited, status=1/FAILURE\r\nMay 10 08:46:31 localhost systemd[1]: multipathd.service: Failed with result 'exit-code'.\r\nMay 10 08:46:31 localhost systemd[1]: Stopped Device-Mapper Multipath Device Controller.\r\nMay 10 08:46:34 localhost systemd[1]: Condition check resulted in Device-Mapper Multipath Device Controller being skipped.\r\n```\r\n\r\nTried below commands from Jumpbox, but not able to connect to cluster\r\n\r\n```\r\n$ oc get nodes\r\nerror: You must be logged in to the server (Unauthorized)\r\n```\r\n\r\n```\r\n$ oc login\r\nUnable to connect to the server: EOF\r\n```\r\n",
        "Issues go stale after 90d of inactivity.\n\nMark the issue as fresh by commenting `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\nExclude this issue from closing by commenting `/lifecycle frozen`.\n\nIf this issue is safe to close now please do so with `/close`.\n\n/lifecycle stale",
        "Stale issues rot after 30d of inactivity.\n\nMark the issue as fresh by commenting `/remove-lifecycle rotten`.\nRotten issues close after an additional 30d of inactivity.\nExclude this issue from closing by commenting `/lifecycle frozen`.\n\nIf this issue is safe to close now please do so with `/close`.\n\n/lifecycle rotten\n/remove-lifecycle stale",
        "Rotten issues close after 30d of inactivity.\n\nReopen the issue by commenting `/reopen`.\nMark the issue as fresh by commenting `/remove-lifecycle rotten`.\nExclude this issue from closing again by commenting `/lifecycle frozen`.\n\n/close",
        "@openshift-bot: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/openshift/okd/issues/619#issuecomment-938153522):\n\n>Rotten issues close after 30d of inactivity.\n>\n>Reopen the issue by commenting `/reopen`.\n>Mark the issue as fresh by commenting `/remove-lifecycle rotten`.\n>Exclude this issue from closing again by commenting `/lifecycle frozen`.\n>\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"
      ]
    },
    "codes": [
      "saving",
      "cluster"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/ExpediaDotCom/haystack/issues/785",
    "content": {
      "title": "[Feature Request] Log number of bytes sent and received",
      "body": "The spans include the start time and duration for each call. Developers could add a custom field to the log to include the bytes sent but that would require having every service using haystack to update their logs to a uniform field.  Instead it would be great if this could be standardized in Haystack\r\n\r\nOur team thinks that this would be an amazing feature to start analyzing \"data loss\" in our tech stack and point out spots where there was a large percentage of loss, which may be a signal that the APIs can be trimmed down or better optimized for the clients using them.\r\n\r\n\r\nNOTE: This is not counting when you can log the blob of the entire response but just the byte count which should be scalable to add to every span\r\n\r\nExample\r\n\r\n![Screen Shot 2019-10-04 at 9 42 13 AM](https://user-images.githubusercontent.com/2446877/66224644-43921300-e68b-11e9-9968-e81d9a36e6c1.png)\r\n",
      "comments": [
        "The ability to capture and visualize this information in Haystack would be enormously powerful. \r\n\r\nSome background: \r\n\r\nWe've designed the BEX-API stack to leverage Haystack by default, and combined with the GraphQL specific tracing tool Apollo Graph Manager we have complete visibility into the behavior of both the services and the client applications, and how they interact. An ongoing benefit (symptom?) of GraphQL is the ability for clients to request & receive _only_ the data they need to render an experience, and that typically results in a smaller message size than they would otherwise see from our \"legacy\" APIs. As this continues to happen, we want to be able to pinpoint significant mismatches in message size upstream to assist our efforts to streamline our entire stack. The more our teams do this, the more we increase the performance of our services while simultaneously reducing cloud costs & maintenance costs in the long term. \r\n\r\nToday this is not easy to do, and involves digging through various logs to find this data if it exists at all. Being able to capture & visualize this in a single tool makes these optimization opportunities obvious and easy to measure. "
      ]
    },
    "codes": [
      "saving"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Guimove/terraform-aws-bastion/issues/24",
    "content": {
      "title": "Route53 type CNAME?",
      "body": "Since rout53 is creating record for bastion elb. Maybe type 'A' with alias should be more efficient and cheaper?",
      "comments": [
        "Hello,\r\nThanks for you issue and you were right. :)",
        "\ud83d\udc4dThanks for the quick fix!"
      ]
    },
    "codes": [
      "domain",
      "saving"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Guimove/terraform-aws-bastion/issues/54",
    "content": {
      "title": "please consider using launch template in place of launch configuration",
      "body": "please consider using launch template in place of launch configuration. This will allow creating a bastion either with an autoscaling group behind nlb or an instance behind route53 [cost saving]. ",
      "comments": []
    },
    "codes": [
      "domain",
      "saving"
    ]
  },
  {
    "type": "issue",
    "url": "https://github.com/Guimove/terraform-aws-bastion/issues/124",
    "content": {
      "title": "[Feature request] Dynamic-latest AMI support for ARM64 instance types like t4g",
      "body": "Currently only \r\n```terraform\r\n  filter {\r\n    name   = \"architecture\"\r\n    values = [\"x86_64\"]\r\n  }\r\n```\r\n base images are supported in the dynamically loading of latest ami image featire . It would be really great if the new `t4g` instance ,which are even cheaper than `t3.nano`, would be supported as well.\r\n\r\ne.g.\r\n```terraform\r\ndata \"aws_ami\" \"amazon_linux_2_arm\" {\r\n  most_recent = true\r\n  owners      = [\"amazon\"]\r\n  name_regex  = \"^amzn2-ami-kernel-.*-hvm-.*-arm64-gp2\"\r\n\r\n  filter {\r\n    name   = \"architecture\"\r\n    values = [\"arm64\"]\r\n  }\r\n}\r\n```\r\n",
      "comments": []
    },
    "codes": [
      "saving",
      "instance"
    ]
  }
]